/**
 * This file was auto-generated by openapi-typescript.
 * Do not make direct changes to the file.
 */

export interface paths {
  '/healthcheck': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /** Healthcheck */
    get: operations['healthcheck_healthcheck_get'];
    put?: never;
    post?: never;
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  '/login': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    get?: never;
    put?: never;
    /** Login Email */
    post: operations['login_email_login_post'];
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  '/login/api_key': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    get?: never;
    put?: never;
    /** Login Api Key */
    post: operations['login_api_key_login_api_key_post'];
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  '/projects/{project_id}/prompt_datasets': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /**
     * List Prompt Datasets
     * @deprecated
     */
    get: operations['list_prompt_datasets_projects__project_id__prompt_datasets_get'];
    put?: never;
    /**
     * Upload Prompt Evaluation Dataset
     * @deprecated
     */
    post: operations['upload_prompt_evaluation_dataset_projects__project_id__prompt_datasets_post'];
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  '/datasets': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /** List Datasets */
    get: operations['list_datasets_datasets_get'];
    put?: never;
    /**
     * Create Dataset
     * @description Creates a stand alone dataset.
     */
    post: operations['create_dataset_datasets_post'];
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  '/datasets/{dataset_id}': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /** Get Dataset */
    get: operations['get_dataset_datasets__dataset_id__get'];
    put?: never;
    post?: never;
    /** Delete Dataset */
    delete: operations['delete_dataset_datasets__dataset_id__delete'];
    options?: never;
    head?: never;
    /** Update Dataset */
    patch: operations['update_dataset_datasets__dataset_id__patch'];
    trace?: never;
  };
  '/datasets/query': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    get?: never;
    put?: never;
    /** Query Datasets */
    post: operations['query_datasets_datasets_query_post'];
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  '/projects/{project_id}/prompt_datasets/{dataset_id}': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /**
     * Download Prompt Dataset
     * @deprecated
     */
    get: operations['download_prompt_dataset_projects__project_id__prompt_datasets__dataset_id__get'];
    /**
     * Update Prompt Dataset
     * @deprecated
     */
    put: operations['update_prompt_dataset_projects__project_id__prompt_datasets__dataset_id__put'];
    post?: never;
    /**
     * Delete Prompt Dataset
     * @deprecated
     */
    delete: operations['delete_prompt_dataset_projects__project_id__prompt_datasets__dataset_id__delete'];
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  '/datasets/{dataset_id}/content': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /** Get Dataset Content */
    get: operations['get_dataset_content_datasets__dataset_id__content_get'];
    /**
     * Rollback Dataset Content
     * @description Rollback the content of a dataset to a previous version.
     */
    put: operations['rollback_dataset_content_datasets__dataset_id__content_put'];
    post?: never;
    delete?: never;
    options?: never;
    head?: never;
    /**
     * Update Dataset Content
     * @description Update the content of a dataset.
     *
     *     The `index` and `column_name` fields are treated as keys tied to a specific version of the dataset.
     *     As such, these values are considered immutable identifiers for the dataset's structure.
     *
     *     For example, if an edit operation changes the name of a column, subsequent edit operations in
     *     the same request should reference the column using its original name.
     *
     *     The `If-Match` header is used to ensure that updates are only applied if the client's version of the dataset
     *     matches the server's version. This prevents conflicts from simultaneous updates. The `ETag` header in the response
     *     provides the new version identifier after a successful update.
     */
    patch: operations['update_dataset_content_datasets__dataset_id__content_patch'];
    trace?: never;
  };
  '/datasets/{dataset_id}/download': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /** Download Dataset */
    get: operations['download_dataset_datasets__dataset_id__download_get'];
    put?: never;
    post?: never;
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  '/datasets/{dataset_id}/preview': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    get?: never;
    put?: never;
    /** Preview Dataset */
    post: operations['preview_dataset_datasets__dataset_id__preview_post'];
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  '/datasets/{dataset_id}/content/query': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    get?: never;
    put?: never;
    /** Query Dataset Content */
    post: operations['query_dataset_content_datasets__dataset_id__content_query_post'];
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  '/datasets/{dataset_id}/users': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /**
     * List User Dataset Collaborators
     * @description List the users with which the dataset has been shared.
     */
    get: operations['list_user_dataset_collaborators_datasets__dataset_id__users_get'];
    put?: never;
    /** Create User Dataset Collaborators */
    post: operations['create_user_dataset_collaborators_datasets__dataset_id__users_post'];
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  '/datasets/{dataset_id}/groups': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /**
     * List Group Dataset Collaborators
     * @description List the groups with which the dataset has been shared.
     */
    get: operations['list_group_dataset_collaborators_datasets__dataset_id__groups_get'];
    put?: never;
    /**
     * Create Group Dataset Collaborators
     * @description Share a dataset with groups.
     */
    post: operations['create_group_dataset_collaborators_datasets__dataset_id__groups_post'];
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  '/datasets/{dataset_id}/users/{user_id}': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    get?: never;
    put?: never;
    post?: never;
    /**
     * Delete User Dataset Collaborator
     * @description Remove a user's access to a dataset.
     */
    delete: operations['delete_user_dataset_collaborator_datasets__dataset_id__users__user_id__delete'];
    options?: never;
    head?: never;
    /**
     * Update User Dataset Collaborator
     * @description Update the sharing permissions of a user on a dataset.
     */
    patch: operations['update_user_dataset_collaborator_datasets__dataset_id__users__user_id__patch'];
    trace?: never;
  };
  '/datasets/{dataset_id}/groups/{group_id}': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    get?: never;
    put?: never;
    post?: never;
    /**
     * Delete Group Dataset Collaborator
     * @description Remove a group's access to a dataset.
     */
    delete: operations['delete_group_dataset_collaborator_datasets__dataset_id__groups__group_id__delete'];
    options?: never;
    head?: never;
    /**
     * Update Group Dataset Collaborator
     * @description Update the sharing permissions of a group on a dataset.
     */
    patch: operations['update_group_dataset_collaborator_datasets__dataset_id__groups__group_id__patch'];
    trace?: never;
  };
  '/datasets/{dataset_id}/versions/query': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    get?: never;
    put?: never;
    /** Query Dataset Versions */
    post: operations['query_dataset_versions_datasets__dataset_id__versions_query_post'];
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  '/datasets/{dataset_id}/versions/{version_index}/content': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /** Get Dataset Version Content */
    get: operations['get_dataset_version_content_datasets__dataset_id__versions__version_index__content_get'];
    put?: never;
    post?: never;
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  '/datasets/{dataset_id}/versions/{version_index}': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    get?: never;
    put?: never;
    post?: never;
    delete?: never;
    options?: never;
    head?: never;
    /** Update Dataset Version */
    patch: operations['update_dataset_version_datasets__dataset_id__versions__version_index__patch'];
    trace?: never;
  };
  '/collaborator_roles': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /** Get Collaborator Roles */
    get: operations['get_collaborator_roles_collaborator_roles_get'];
    put?: never;
    post?: never;
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  '/projects': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /**
     * Get Projects
     * @deprecated
     * @description Gets projects for a user.
     *
     *     If provided, filters on project_name and project_type.
     *
     *     DEPRECATED in favor of `get_projects_paginated`.
     */
    get: operations['get_projects_projects_get'];
    put?: never;
    /**
     * Create Project
     * @description Create a new project.
     */
    post: operations['create_project_projects_post'];
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  '/projects/paginated': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    get?: never;
    put?: never;
    /**
     * Get Projects Paginated
     * @description Gets projects for a user with pagination.
     *
     *     If provided, filters on project_name and project_type.
     */
    post: operations['get_projects_paginated_projects_paginated_post'];
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  '/projects/count': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    get?: never;
    put?: never;
    /**
     * Get Projects Count
     * @description Gets total count of projects for a user with applied filters.
     */
    post: operations['get_projects_count_projects_count_post'];
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  '/projects/all': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /**
     * Get All Projects
     * @deprecated
     * @description Gets all public projects and all private projects that the user has access to.
     *
     *     For Enterprise SaaS Clusters, this will return all projects in the cluster.
     *
     *     DEPRECATED in favor of `get_projects_paginated`.
     */
    get: operations['get_all_projects_projects_all_get'];
    put?: never;
    post?: never;
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  '/projects/{project_id}': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /** Get Project */
    get: operations['get_project_projects__project_id__get'];
    /** Update Project */
    put: operations['update_project_projects__project_id__put'];
    post?: never;
    /**
     * Delete Project
     * @description Deletes a project and all associated runs and objects.
     *
     *     Any user with project access can delete a project.
     *     Note that `get_project_by_id` calls `user_can_access_project`.
     */
    delete: operations['delete_project_projects__project_id__delete'];
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  '/projects/{project_id}/upload_file': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    get?: never;
    put?: never;
    /** Upload File */
    post: operations['upload_file_projects__project_id__upload_file_post'];
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  '/projects/{project_id}/users': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /**
     * List User Project Collaborators
     * @description List the users with which the project has been shared.
     */
    get: operations['list_user_project_collaborators_projects__project_id__users_get'];
    put?: never;
    /**
     * Create User Project Collaborators
     * @description Share a project with users.
     */
    post: operations['create_user_project_collaborators_projects__project_id__users_post'];
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  '/projects/{project_id}/groups': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /**
     * List Group Project Collaborators
     * @description List the groups with which the project has been shared.
     */
    get: operations['list_group_project_collaborators_projects__project_id__groups_get'];
    put?: never;
    /**
     * Create Group Project Collaborators
     * @description Share a project with groups.
     */
    post: operations['create_group_project_collaborators_projects__project_id__groups_post'];
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  '/projects/{project_id}/users/{user_id}': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    get?: never;
    put?: never;
    post?: never;
    /**
     * Delete User Project Collaborator
     * @description Remove a user's access to a project.
     */
    delete: operations['delete_user_project_collaborator_projects__project_id__users__user_id__delete'];
    options?: never;
    head?: never;
    /**
     * Update User Project Collaborator
     * @description Update the sharing permissions of a user on a project.
     */
    patch: operations['update_user_project_collaborator_projects__project_id__users__user_id__patch'];
    trace?: never;
  };
  '/projects/{project_id}/groups/{group_id}': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    get?: never;
    put?: never;
    post?: never;
    /**
     * Delete Group Project Collaborator
     * @description Remove a group's access to a project.
     */
    delete: operations['delete_group_project_collaborator_projects__project_id__groups__group_id__delete'];
    options?: never;
    head?: never;
    /**
     * Update Group Project Collaborator
     * @description Update the sharing permissions of a group on a project.
     */
    patch: operations['update_group_project_collaborator_projects__project_id__groups__group_id__patch'];
    trace?: never;
  };
  '/projects/{project_id}/log_streams': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /**
     * List Log Streams
     * @description Retrieve all log streams for a project.
     */
    get: operations['list_log_streams_projects__project_id__log_streams_get'];
    put?: never;
    /**
     * Create Log Stream
     * @description Create a new log stream for a project.
     */
    post: operations['create_log_stream_projects__project_id__log_streams_post'];
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  '/projects/{project_id}/log_streams/{log_stream_id}': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /**
     * Get Log Stream
     * @description Retrieve a specific log stream.
     */
    get: operations['get_log_stream_projects__project_id__log_streams__log_stream_id__get'];
    /**
     * Update Log Stream
     * @description Update a specific log stream.
     */
    put: operations['update_log_stream_projects__project_id__log_streams__log_stream_id__put'];
    post?: never;
    /**
     * Delete Log Stream
     * @description Delete a specific log stream.
     */
    delete: operations['delete_log_stream_projects__project_id__log_streams__log_stream_id__delete'];
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  '/projects/{project_id}/traces': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    get?: never;
    put?: never;
    /** Log Traces */
    post: operations['log_traces_projects__project_id__traces_post'];
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  '/projects/{project_id}/traces/available_columns': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    get?: never;
    put?: never;
    /** Traces Available Columns */
    post: operations['traces_available_columns_projects__project_id__traces_available_columns_post'];
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  '/projects/{project_id}/spans/available_columns': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    get?: never;
    put?: never;
    /** Spans Available Columns */
    post: operations['spans_available_columns_projects__project_id__spans_available_columns_post'];
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  '/projects/{project_id}/traces/search': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    get?: never;
    put?: never;
    /** Query Traces */
    post: operations['query_traces_projects__project_id__traces_search_post'];
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  '/projects/{project_id}/spans/search': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    get?: never;
    put?: never;
    /** Query Spans */
    post: operations['query_spans_projects__project_id__spans_search_post'];
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  '/projects/{project_id}/traces/{trace_id}': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /** Get Trace */
    get: operations['get_trace_projects__project_id__traces__trace_id__get'];
    put?: never;
    post?: never;
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  '/projects/{project_id}/spans/{span_id}': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /** Get Span */
    get: operations['get_span_projects__project_id__spans__span_id__get'];
    put?: never;
    post?: never;
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  '/projects/{project_id}/experiments': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /**
     * List Experiments
     * @description Retrieve all experiments for a project.
     */
    get: operations['list_experiments_projects__project_id__experiments_get'];
    put?: never;
    /**
     * Create Experiment
     * @description Create a new experiment for a project.
     */
    post: operations['create_experiment_projects__project_id__experiments_post'];
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  '/projects/{project_id}/experiments/{experiment_id}': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /**
     * Get Experiment
     * @description Retrieve a specific experiment.
     */
    get: operations['get_experiment_projects__project_id__experiments__experiment_id__get'];
    /**
     * Update Experiment
     * @description Update a specific experiment.
     */
    put: operations['update_experiment_projects__project_id__experiments__experiment_id__put'];
    post?: never;
    /**
     * Delete Experiment
     * @description Delete a specific experiment.
     */
    delete: operations['delete_experiment_projects__project_id__experiments__experiment_id__delete'];
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  '/jobs': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    get?: never;
    put?: never;
    /** Create Job */
    post: operations['create_job_jobs_post'];
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  '/jobs/{job_id}': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /**
     * Get Job
     * @description Get a job by id.
     */
    get: operations['get_job_jobs__job_id__get'];
    put?: never;
    post?: never;
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  '/projects/{project_id}/runs/{run_id}/jobs': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /**
     * Get Jobs For Project Run
     * @description Get all jobs by for a project and run.
     *
     *     Returns them in order of creation from newest to oldest.
     */
    get: operations['get_jobs_for_project_run_projects__project_id__runs__run_id__jobs_get'];
    put?: never;
    post?: never;
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  '/projects/{project_id}/runs/{run_id}/jobs/latest': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /**
     * Get Latest Job For Project Run
     * @description Returns the most recently updated job for a run.
     */
    get: operations['get_latest_job_for_project_run_projects__project_id__runs__run_id__jobs_latest_get'];
    put?: never;
    post?: never;
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  '/projects/{project_id}/runs/{run_id}/scorer-settings': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /** Get Settings */
    get: operations['get_settings_projects__project_id__runs__run_id__scorer_settings_get'];
    put?: never;
    /** Upsert Scorers Config */
    post: operations['upsert_scorers_config_projects__project_id__runs__run_id__scorer_settings_post'];
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  '/projects/{project_id}/templates': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /**
     * Get Project Templates
     * @description Get all prompt templates for a project.
     *
     *     Parameters
     *     ----------
     *     project_id : UUID4
     *         Project ID.
     *     db_read : Session, optional
     *         Session object to execute DB reads, by default Depends(get_db_read).
     *
     *     Returns
     *     -------
     *     List[GetTemplateResponse]
     *         List of prompt template responses.
     */
    get: operations['get_project_templates_projects__project_id__templates_get'];
    put?: never;
    /**
     * Create Prompt Template With Version
     * @description For a given project, create a prompt template.
     *
     *     We first create a prompt template version, and then create a prompt template that
     *     points to that version as the selected version.
     *
     *     Parameters
     *     ----------
     *     project_id : UUID4
     *         Project ID.
     *     current_user : User, optional
     *         User who sent the request, by default Depends(authentication_service.current_user)
     *     create_request : CreatePromptTemplateWithVersionRequestBody, optional
     *         Request body, by default Body( ...,
     *             examples=
     *             [BasePromptTemplateVersion.test_data() | BasePromptTemplate.test_data()],
     *         )
     *     db_read : Session, optional
     *         Session object to execute DB reads, by default Depends(get_db_read)
     *
     *     Returns
     *     -------
     *     CreatePromptTemplateResponse
     *         Details about the created prompt template.
     */
    post: operations['create_prompt_template_with_version_projects__project_id__templates_post'];
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  '/projects/{project_id}/templates/versions': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /**
     * Get Template Version By Name
     * @description Get a prompt template from a project.
     *
     *     Parameters
     *     ----------
     *     project_id : UUID4
     *         Prokect ID.
     *     template_name : str
     *         Prompt template name.
     *     version : Optional[int]
     *         Version number to fetch. defaults to selected version.
     *     db_read : Session, optional
     *         Session object to execute DB reads, by default Depends(get_db_read).
     *
     *
     *     Returns
     *     -------
     *     GetTemplateResponse
     *         Prompt template response.
     */
    get: operations['get_template_version_by_name_projects__project_id__templates_versions_get'];
    put?: never;
    post?: never;
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  '/projects/{project_id}/templates/{template_id}': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /**
     * Get Template From Project
     * @description Get a prompt template from a project.
     *
     *     Parameters
     *     ----------
     *     template_id : UUID4
     *         Prompt template ID.
     *     project_id : UUID4
     *         Prokect ID.
     *     db_read : Session, optional
     *         Session object to execute DB reads, by default Depends(get_db_read).
     *
     *     Returns
     *     -------
     *     GetTemplateResponse
     *         Prompt template response.
     */
    get: operations['get_template_from_project_projects__project_id__templates__template_id__get'];
    put?: never;
    post?: never;
    /** Delete Template */
    delete: operations['delete_template_projects__project_id__templates__template_id__delete'];
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  '/projects/{project_id}/templates/{template_id}/versions': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    get?: never;
    put?: never;
    /**
     * Create Prompt Template Version
     * @description Create a prompt template version for a given prompt template.
     *
     *     Parameters
     *     ----------
     *     project_id : UUID4
     *         Project ID.
     *     template_id : UUID4
     *         Prompt template ID.
     *     current_user : User, optional
     *         Authenticated user, by default Depends(authentication_service.current_user)
     *     body : dict, optional
     *         Body of the request, by default Body( ...,
     *             examples=[CreatePromptTemplateVersionRequest.test_data()],
     *         )
     *     db_read : Session, optional
     *         Database session, by default Depends(get_db_read)
     *
     *     Returns
     *     -------
     *     BasePromptTemplateVersionResponse
     *         Response with details about the created prompt template version.
     */
    post: operations['create_prompt_template_version_projects__project_id__templates__template_id__versions_post'];
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
  '/projects/{project_id}/templates/{template_id}/versions/{version}': {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    /**
     * Get Template Version
     * @description Get a specific version of a prompt template.
     *
     *     Parameters
     *     ----------
     *     template_id : UUID4
     *         Template ID.
     *     version : int
     *         Version number to fetch.
     *     current_user : User, optional
     *         User who is authorized, by default Depends(authentication_service.current_user).
     *     db_read : Session, optional
     *         Database session, by default Depends(get_db_read)
     *
     *     Returns
     *     -------
     *     BasePromptTemplateVersionResponse
     *         Prompt template version response.
     */
    get: operations['get_template_version_projects__project_id__templates__template_id__versions__version__get'];
    /** Set Selected Template Version */
    put: operations['set_selected_template_version_projects__project_id__templates__template_id__versions__version__put'];
    post?: never;
    delete?: never;
    options?: never;
    head?: never;
    patch?: never;
    trace?: never;
  };
}
export type webhooks = Record<string, never>;
export interface components {
  schemas: {
    /** AgenticSessionSuccessScorer */
    AgenticSessionSuccessScorer: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'agentic_session_success';
      /**
       * Filters
       * @description List of filters to apply to the scorer.
       */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
      /**
       * Type
       * @default plus
       * @constant
       */
      type?: 'plus';
      /**
       * Model Name
       * @description Alias of the model to use for the scorer.
       */
      model_name?: string | null;
      /**
       * Num Judges
       * @description Number of judges for the scorer.
       */
      num_judges?: number | null;
    };
    /**
     * AgenticSessionSuccessTemplate
     * @description Template for the agentic session success metric,
     *     containing all the info necessary to send the agentic session success prompt.
     */
    AgenticSessionSuccessTemplate: {
      /**
       * Metric System Prompt
       * @default You will receive the complete chat history from a chatbot application between a user and an assistant.
       *
       *     In the chat history, the user will ask questions, which are answered with words, or make requests that require calling tools and resolving actions. Sometimes these are given as orders; treat them as if they were questions or requests. Each assistant turn may involve several steps that combine internal reflections, planning steps, selecting tools, and calling tools, and should always end with the assistant replying back to the user.
       *
       *     You will analyze the entire chat history and will respond back in the following JSON format:
       *     ```json
       *     {
       *         "all_user_asks": list[string],
       *         "tasks": list[dict],
       *         "ai_answered_all_asks": boolean,
       *         "explanation": string
       *     }
       *     ```
       *     where I will now explain how to populate each field.
       *
       *     # Populating: all_user_asks
       *
       *     Populate `all_user_asks` with a list containing every user ask from the chat history. Review the chat history and generate a list with one entry for each user question, request, order, follow-up, clarification, etc. Ensure that every user ask is a separate item, even if this requires splitting the text mid-sentence. Each item should include enough context to be understandable on its own. It is acceptable to have shared context between items and to incorporate parts of sentences as needed.
       *
       *     # Populating: Tasks
       *
       *     This is the most complex field to populate. You will write a JSON array where each element is called a task and follows the schema:
       *
       *     ```json
       *     {
       *         "initial_user_ask": string,
       *         "user_ask_refinements": list[string],
       *         "final_user_ask": string,
       *         "direct_answer": string,
       *         "indirect_answer": string,
       *         "tools_input_output": list[string],
       *         "properties" : {
       *             "coherent": boolean,
       *             "factually_correct": boolean,
       *             "comprehensively_answers_final_user_ask": boolean,
       *             "does_not_contradict_tools_output": boolean,
       *             "tools_output_summary_is_accurate": boolean,
       *         },
       *         "boolean_properties": list[boolean],
       *         "answer_satisfies_properties": boolean
       *     }
       *     ```
       *
       *     The high-level goal is to list all tasks and their resolutions and to determine whether each task has been successfully accomplished.
       *
       *     ## Step 1: initial_user_ask, user_ask_refinements and final_user_ask
       *
       *     First, identify the `initial_user_ask` that starts the task, as well as any `user_ask_refinements` related to the same task. To do this, first loop through the entries in `all_user_asks`. If an entry already appears in a previous task, ignore it; otherwise, consider it as the `initial_user_ask`. Next, examine the remaining entries in `all_user_asks` and fill `user_ask_refinements` with all those related to the `initial_user_ask`, meaning they either refine it or continue the same ask.
       *
       *     Finally, create a coherent `final_user_ask` containing the most updated version of the ask by starting with the initial one and incorporating or replacing any parts with their refinements. This will be the ask that the assistant will attempt to answer.
       *
       *     ## Step 2: direct_answer and indirect_answer
       *
       *     Extract every direct and indirect answer that responds to the `final_user_ask`.
       *
       *     An indirect answer is a part of the assistant's reponse that tries to respond to `final_user_ask` and satisfies any of the following:
       *     - it mentions limitations or the inability to complete the `final_user_ask`,
       *     - it references a failed attempt to complete the `final_user_ask`,
       *     - it suggests offering help with a different ask than the `final_user_ask`,
       *     - it requests further information or clarifications from the user.
       *     Add any piece of the assistant's response looking like an indirect answer to `indirect_answer`.
       *
       *     A direct answer is a part of an assistant's response that either:
       *     - directly responds to the `final_user_ask`,
       *     - confirms a successful resolution of the `final_user_ask`.
       *     If there are multiple direct answers, simply concatenate them into a longer answer. If there are no direct answers satisfying the above conditions, leave the field `direct_answer` empty.
       *
       *     Note that a piece of an answer cannot be both direct and indirect, you should pick the field in which to add it.
       *
       *     ## Step 3: tools_input_output
       *
       *     If `direct_answer` is empty, skip this step.
       *
       *     Examine each assistant step and identify which tool or function output seemingly contributed to creating any part of the answer from `direct_answer`. If an assistant step immediately before or after the tool call mentions using or having used the tool for answering the `final_user_ask`, the tool call should be associated with this ask. Additionally, if any part of the answer closely aligns with the output of a tool, the tool call should also be associated with this ask.
       *
       *     Create a list containing the concatenated input and output of each tool used in formulating any part of the answer from `direct_answer`. The tool input is noted as an assistant step before calling the tool, and the tool output is recorded as a tool step.
       *
       *     ## Step 4: properties, boolean_properties and answer_satisfies_properties
       *
       *     If `direct_answer` is empty, set every boolean in `properties`, `boolean_properties` and `answer_satisfies_properties` to `false`.
       *
       *     For each part of the answer from `direct_answer`, evaluate the following properties one by one to determine which are satisfied and which are not:
       *
       *     - **coherent**: The answer is coherent with itself and does not contain internal contradictions.
       *     - **factually_correct**: The parts of the answer that do not come from the output of a tool are factually correct.
       *     - **comprehensively_answers_final_user_ask**: The answer specifically responds to the `final_user_ask`, carefully addressing every aspect of the ask without deviation or omission, ensuring that no details or parts of the ask are left unanswered.
       *     - **does_not_contradict_tools_output**: No citation of a tool's output contradict any text from `tools_input_output`.
       *     - **tools_output_summary_is_accurate**: Every summary of a tool's output is accurate with the tool's output from `tools_input_output`. In particular it does not omit critical information relevant to the `final_user_ask` and does not contain made-up information.
       *
       *     After assessing each of these properties, copy the resulting boolean values into the list `boolean_properties`.
       *
       *     Finally, set `answer_satisfies_properties` to `false` if any entry in `boolean_properties` is set to `false`; otherwise, set `answer_satisfies_properties` to `true`.
       *
       *     # Populating: ai_answered_all_asks
       *
       *     Respond `true` if every task has `answer_satisfies_properties` set to `true`, otherwise respond `false`. If `all_user_asks` is empty, set `answer_satisfies_properties` to `true`.
       *
       *     # Populating: explanation
       *
       *     If any user ask has `answer_satisfies_properties` set to `false`, explain why it didn't satisfy all the properties. Otherwise summarize in a few words each ask and the provided answer.
       *
       *     If `all_user_asks` is empty, mention that you did not find any user ask. If `direct_answer` is empty, mention that no resultion to the `final_user_ask` was provided.
       *
       *     You must respond with a valid JSON object; be sure to escape special characters.
       */
      metric_system_prompt?: string;
      /**
       * Metric Description
       * @default I have a multi-turn chatbot application where the assistant is an agent that has access to tools. I want a metric that assesses whether the session should be considered successful, in the sense that the assistant fully answered or resolved all user queries and requests.
       */
      metric_description?: string;
      /**
       * Value Field Name
       * @default ai_answered_all_asks
       */
      value_field_name?: string;
      /**
       * Explanation Field Name
       * @description Field name to look for in the chainpoll response, for the explanation.
       * @default explanation
       */
      explanation_field_name?: string;
      /**
       * Template
       * @default Here is a the chatbot history:
       *     ```
       *     {query}
       *     ```
       *     Now perform the evaluation on the chat history as described in the system prompt.
       */
      template?: string;
      /**
       * Metric Few Shot Examples
       * @default [
       *       {
       *         "generation_prompt_and_response": "Here is a the chatbot history:\n```\nuser: Can you analyze the sentiment of the sentence 'I am really excited about our new project'? Additionally, fetch a random word of type adverb from the Random Word API.\nBegin!\nassistant: I am calling the \"main_for_neuronet_nlp\" function with the argument of the sentence \"I am really excited about our new project\". By doing this, I can analyze the sentiment of the sentence and gain insights into the emotional tone expressed. Additionally, I will use the Neuronet NLP tool to tokenize, split sentences, and perform part-of-speech tagging and named entity recognition for the Azerbaijan language. This will allow me to further analyze the sentence and provide more detailed information about its structure and context.\nassistant: {'arguments': '{\\n  \"text\": \"I am really excited about our new project\"\\n}', 'name': 'main_for_neuronet_nlp'}\nfunction: {\"error\": \"\", \"response\": \"{0: {'lemma': '\\u0131', 'text': 'I', 'dep': 'nsubj', 'pos': 'NUM', 'tag': 'NUM__Number=Sing|NumForm=Roman|NumType=Ord'}, 1: {'lemma': 'a', 'text': 'am', 'dep': 'punct', 'pos': 'NOUN', 'tag': 'NOUN__Number=Sing'}, 2: {'lemma': 'really', 'text': 'really', 'dep': 'amod', 'pos': 'ADJ', 'tag': 'ADJ__Number=Sing'}, 3: {'lemma': 'excited', 'text': 'excited', 'dep': 'obj', 'pos': 'NOUN', 'tag': 'NOUN__Number=Sing'}, 4: {'lemma': 'about', 'text': 'about', 'dep': 'obj', 'pos': 'PROPN', 'tag': 'PROPN__NameType=Giv|Number=Sing'}, 5: {'lemma': 'our', 'text': 'our', 'dep': 'appos', 'pos': 'NOUN', 'tag': 'NOUN__Number=Sing'}, 6: {'lemma': 'new', 'text': 'new', 'dep': 'ROOT', 'pos': 'NOUN', 'tag': 'NOUN__Number=Sing'}, 7: {'lemma': 'project', 'text': 'project', 'dep': 'compound', 'pos': 'NOUN', 'tag': 'NOUN__Number=Sing'}, 8: {'lemma': '.', 'text': '.', 'dep': 'punct', 'pos': 'PUNCT', 'tag': 'PUNCT___'}}\"}\nassistant: The previous assistant message explains that the current API call is made to the \"v1_randomword_for_random_word_by_api_ninjas\" function. The purpose of this call is to fetch a random word of type adverb from the Random Word API. By doing this, I can provide the user with an interesting and unique adverb that can be used in their query or sentence. This will add more variety and creativity to their language, enhancing the overall user experience.\nassistant: {'arguments': '{}', 'name': 'v1_randomword_for_random_word_by_api_ninjas'}\nfunction: {\"error\": \"\", \"response\": \"{'word': 'incarnadine'}\"}\nassistant: The sentiment of the sentence 'I am really excited about our new project' is positive. The random word of type adverb is 'incarnadine'.\n```",
       *         "evaluating_response": "```json\n{\n    \"all_user_asks\": [\n        \"Can you analyze the sentiment of the sentence 'I am really excited about our new project'?\",\n        \"Additionally, fetch a random word of type adverb from the Random Word API.\"\n    ],\n    \"tasks\": [\n        {\n            \"initial_user_ask\": \"Can you analyze the sentiment of the sentence 'I am really excited about our new project'?\",\n            \"user_ask_refinements\": [],\n            \"final_user_ask\": \"Can you analyze the sentiment of the sentence 'I am really excited about our new project'?\",\n            \"direct_answer\": \"The sentiment of the sentence 'I am really excited about our new project' is positive.\",\n            \"indirect_answer\": \"\",\n            \"tools_input_output\": [\n                \"{'arguments': '{\\\\n  \\\"text\\\": \\\"I am really excited about our new project\\\"\\\\n}', 'name': 'main_for_neuronet_nlp'} function: {\\\"error\\\": \\\"\\\", \\\"response\\\": \\\"{0: {'lemma': '\\\\u0131', 'text': 'I', 'dep': 'nsubj', 'pos': 'NUM', 'tag': 'NUM__Number=Sing|NumForm=Roman|NumType=Ord'}, 1: {'lemma': 'a', 'text': 'am', 'dep': 'punct', 'pos': 'NOUN', 'tag': 'NOUN__Number=Sing'}, 2: {'lemma': 'really', 'text': 'really', 'dep': 'amod', 'pos': 'ADJ', 'tag': 'ADJ__Number=Sing'}, 3: {'lemma': 'excited', 'text': 'excited', 'dep': 'obj', 'pos': 'NOUN', 'tag': 'NOUN__Number=Sing'}, 4: {'lemma': 'about', 'text': 'about', 'dep': 'obj', 'pos': 'PROPN', 'tag': 'PROPN__NameType=Giv|Number=Sing'}, 5: {'lemma': 'our', 'text': 'our', 'dep': 'appos', 'pos': 'NOUN', 'tag': 'NOUN__Number=Sing'}, 6: {'lemma': 'new', 'text': 'new', 'dep': 'ROOT', 'pos': 'NOUN', 'tag': 'NOUN__Number=Sing'}, 7: {'lemma': 'project', 'text': 'project', 'dep': 'compound', 'pos': 'NOUN', 'tag': 'NOUN__Number=Sing'}, 8: {'lemma': '.', 'text': '.', 'dep': 'punct', 'pos': 'PUNCT', 'tag': 'PUNCT___'}}\\\"}\"\n            ],\n            \"properties\": {      \n                \"coherent\": true,\n                \"factually_correct\": false,\n                \"comprehensively_answers_final_user_ask\": true,\n                \"does_not_contradict_tools_output\": true,\n                \"tools_output_summary_is_accurate\": false\n            },\n            \"boolean_properties\": [true, false, true, true, false],\n            \"answer_satisfies_properties\": false\n        },\n        {\n            \"initial_user_ask\": \"Additionally, fetch a random word of type adverb from the Random Word API.\",\n            \"user_ask_refinements\": [],\n            \"final_user_ask\": \"Additionally, fetch a random word of type adverb from the Random Word API.\",\n            \"direct_answer\": \"The random word of type adverb is 'incarnadine'.\",\n            \"indirect_answer\": \"\",\n            \"tools_input_output\": [\n                \"{'arguments': '{}', 'name': 'v1_randomword_for_random_word_by_api_ninjas'} function: {\\\"error\\\": \\\"\\\", \\\"response\\\": \\\"{'word': 'incarnadine'}\\\"}\"\n            ],\n            \"properties\": {      \n                \"coherent\": true,\n                \"factually_correct\": false,\n                \"comprehensively_answers_final_user_ask\": false,\n                \"does_not_contradict_tools_output\": false,\n                \"tools_output_summary_is_accurate\": false\n            },\n            \"boolean_properties\": [true, false, false, false, false],\n            \"answer_satisfies_properties\": false\n        }\n    ],\n    \"ai_answered_all_asks\": false,\n    \"explanation\": \"The sentiment analysis task is factually incorrect because the tool output does not provide sentiment; it outputs tagged parts of the sentence, and the summary provided does not accurately reflect this. For the task of fetching a random adverb, the answer is factually incorrect and does not comprehensively answer the ask, as the returned word 'incarnadine' is not an adverb according to the tool output.\"\n}\n```\nNow perform the evaluation on the chat history as described in the system prompt."
       *       },
       *       {
       *         "generation_prompt_and_response": "Here is a the chatbot history:\n```\nuser: My friend wants to analyze their password 'qwertyuiop' and receive a report that includes the entropy, penalty for irregular entropy distribution, and the penalties for keyboard sequences, number sequences, and repeated characters. Can you output the analysis in YAML format?\nBegin!\nassistant: Based on the user's request, the current API call is made to the \"ec_for_arespass\" function with the arguments: \"password\" set to \"qwertyuiop\", and \"outputformat\" set to \"yaml\". The reason for this call is to analyze the password and generate a report in YAML format that includes the entropy, penalty for irregular entropy distribution, as well as penalties for keyboard sequences, number sequences, and repeated characters. This analysis will provide a comprehensive understanding of the password's strength and any potential vulnerabilities. By using the \"ec_for_arespass\" function with the specified arguments, I can obtain the desired report format for further analysis and discussion with the user.\nassistant: {'arguments': '{\\n  \"password\": \"qwertyuiop\",\\n  \"outputformat\": \"yaml\"\\n}', 'name': 'ec_for_arespass'}\nfunction: {\"error\": \"\", \"response\": \"---\\nEntropyResult:\\n  summary:\\n    summaryItem:\\n    - \\\"*** password:             qwertyuiop    \\\"\\n    - \\\"*** penalty param:        0.2500        \\\"\\n    - \\\"*** length:               10 characters \\\"\\n    - \\\"*** entropy:              20.0559 bits  \\\"\\n    - \\\"*** ideal entropy:        80.2237 bits  \\\"\\n    - \\\"*** efficiency:           25.00 %       \\\"\\n    - \\\"***                                     \\\"\\n    - \\\"                          q      w      e      r      t      y      u     \\\\\\n      \\\\ i      o      p      \\\"\\n    - \\\"*** entr. distribution:   10.00% 10.00% 10.00% 10.00% 10.00% 10.00% 10.00%\\\\\\n      \\\\ 10.00% 10.00% 10.00% \\\"\\n    - \\\"*** qwertyKeyboard:       0.2500 0.2500 0.2500 0.2500 0.2500 0.2500 0.2500\\\\\\n      \\\\ 0.2500 0.2500 0.2500 \\\"\\n    - \\\"*** numberSequence:       \\u00b7\\u00b7\\u00b7\\u00b7\\u00b7\\u00b7 \\u00b7\\u00b7\\u00b7\\u00b7\\u00b7\\u00b7 \\u00b7\\u00b7\\u00b7\\u00b7\\u00b7\\u00b7 \\u00b7\\u00b7\\u00b7\\u00b7\\u00b7\\u00b7 \\u00b7\\u00b7\\u00b7\\u00b7\\u00b7\\u00b7 \\u0...\nassistant: The password 'qwertyuiop' has an entropy of 20.0559 bits. The penalty for irregular entropy distribution is 0.2500. The penalties for keyboard sequences, number sequences, and repeated characters are 0.2500 each.\n```",
       *         "evaluating_response": "```json\n{\n    \"all_user_asks\": [\n        \"My friend wants to analyze their password 'qwertyuiop' and receive a report that includes the entropy, penalty for irregular entropy distribution, and the penalties for keyboard sequences, number sequences, and repeated characters. Can you output the analysis in YAML format?\"\n    ],\n    \"tasks\": [\n        {\n            \"initial_user_ask\": \"My friend wants to analyze their password 'qwertyuiop' and receive a report that includes the entropy, penalty for irregular entropy distribution, and the penalties for keyboard sequences, number sequences, and repeated characters. Can you output the analysis in YAML format?\",\n            \"user_ask_refinements\": [],\n            \"final_user_ask\": \"My friend wants to analyze their password 'qwertyuiop' and receive a report that includes the entropy, penalty for irregular entropy distribution, and the penalties for keyboard sequences, number sequences, and repeated characters. Can you output the analysis in YAML format?\",\n            \"direct_answer\": \"The password 'qwertyuiop' has an entropy of 20.0559 bits. The penalty for irregular entropy distribution is 0.2500. The penalties for keyboard sequences, number sequences, and repeated characters are 0.2500 each.\",\n            \"indirect_answer\": \"\",\n            \"tools_input_output\": [\n                \"{\\\"password\\\": \\\"qwertyuiop\\\", \\\"outputformat\\\": \\\"yaml\\\"} -> {\\\"error\\\": \\\"\\\", \\\"response\\\": \\\"---\\\\nEntropyResult:\\\\n  summary:\\\\n    summaryItem:\\\\n    - \\\\\\\"*** password:             qwertyuiop    \\\\\\\"\\\\n    - \\\\\\\"*** penalty param:        0.2500        \\\\\\\"\\\\n    - \\\\\\\"*** length:               10 characters \\\\\\\"\\\\n    - \\\\\\\"*** entropy:              20.0559 bits  \\\\\\\"\\\\n    - \\\\\\\"*** ideal entropy:        80.2237 bits  \\\\\\\"\\\\n    - \\\\\\\"*** efficiency:           25.00 %       \\\\\\\"\\\\n    - \\\\\\\"***                                     \\\\\\\"\\\\n    - \\\\\\\"                          q      w      e      r      t      y      u     \\\\\\\\\\\\n      \\\\\\\\ i      o      p      \\\\\\\"\\\\n    - \\\\\\\"*** entr. distribution:   10.00% 10.00% 10.00% 10.00% 10.00% 10.00% 10.00%\\\\\\\\\\\\n      \\\\\\\\ 10.00% 10.00% 10.00% \\\\\\\"\\\\n    - \\\\\\\"*** qwertyKeyboard:       0.2500 0.2500 0.2500 0.2500 0.2500 0.2500 0.2500\\\\\\\\\\\\n      \\\\\\\\ 0.2500 0.2500 0.2500 \\\\\\\"\\\\n    - \\\\\\\"*** numberSequence:       \\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7 \\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7 \\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7 \\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7 \\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7 \\\\u0...\\\"}\"\n            ],\n            \"properties\": {\n                \"coherent\": true,\n                \"factually_correct\": true,\n                \"comprehensively_answers_final_user_ask\": false,\n                \"does_not_contradict_tools_output\": true,\n                \"tools_output_summary_is_accurate\": false\n            },\n            \"boolean_properties\": [\n                true,\n                true,\n                false,\n                true,\n                false\n            ],\n            \"answer_satisfies_properties\": false\n        }\n    ],\n    \"ai_answered_all_asks\": false,\n    \"explanation\": \"The assistant did not comprehensively answer the final user ask because it did not deliver the full YAML-formatted report as requested by the user. Additionally, the summary of the tool's output was not accurate as it omitted parts of the YAML output like the ideal entropy, efficiency, entropy distribution, and so on.\"\n}\n```\nNow perform the evaluation on the chat history as described in the system prompt."
       *       }
       *     ]
       */
      metric_few_shot_examples?: components['schemas']['FewShotExample'][];
    };
    /** AgenticWorkflowSuccessScorer */
    AgenticWorkflowSuccessScorer: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'agentic_workflow_success';
      /**
       * Filters
       * @description List of filters to apply to the scorer.
       */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
      /**
       * Type
       * @default plus
       * @constant
       */
      type?: 'plus';
      /**
       * Model Name
       * @description Alias of the model to use for the scorer.
       */
      model_name?: string | null;
      /**
       * Num Judges
       * @description Number of judges for the scorer.
       */
      num_judges?: number | null;
    };
    /**
     * AgenticWorkflowSuccessTemplate
     * @description Template for the agentic workflow success metric,
     *     containing all the info necessary to send the agentic workflow success prompt.
     */
    AgenticWorkflowSuccessTemplate: {
      /**
       * Metric System Prompt
       * @default You will receive the chat history from a chatbot application between a user and an AI. At the end of the chat history, it is AIs turn to act.
       *
       *     In the chat history, the user can either ask questions, which are answered with words, or make requests that require calling tools and actions to resolve. Sometimes these are given as orders, and these should be treated as questions or requests. The AI's turn may involve several steps which are a combination of internal reflections, planning, selecting tools, calling tools, and ends with the AI replying to the user.
       *     Your task involves the following steps:
       *
       *     ########################
       *
       *     Step 1: user_last_input and user_ask
       *
       *     First, identify the user's last input in the chat history. From this input, create a list with one entry for each user question, request, or order. If there are no user asks in the user's last input, leave the list empty and skip ahead, considering the AI's turn successful.
       *
       *     ########################
       *
       *     Step 2: ai_final_response and answer_or_resolution
       *
       *     Identify the AI's final response to the user: it is the very last step in the AI's turn.
       *
       *     For every user_ask, focus on ai_final_response and try to extract either an answer or a resolution using the following definitions:
       *     - An answer is a part of the AI's final response that directly responds to all or part of a user's question, or asks for further information or clarification.
       *     - A resolution is a part of the AI's final response that confirms a successful resolution, or asks for further information or clarification in order to answer a user's request.
       *
       *     If the AI's final response does not address the user ask, simply write "No answer or resolution provided in the final response". Do not shorten the answer or resolution; provide the entire relevant part.
       *
       *     ########################
       *
       *     Step 3: tools_input_output
       *
       *     Examine every step in the AI's turn and identify which tool/function step seemingly contributed to creating the answer or resolution. Every tool call should be linked to a user ask. If an AI step immediately before or after the tool call mentions planning or using a tool for answering a user ask, the tool call should be associated with that user ask. If the answer or resolution strongly resembles the output of a tool, the tool call should also be associated with that user ask.
       *
       *     Create a list containing the concatenation of the entire input and output of every tool used in formulating the answer or resolution. The tool input is listed as an AI step before calling the tool, and the tool output is listed as a tool step.
       *
       *     ########################
       *
       *     Step 4: properties, boolean_properties and answer_successful
       *
       *     For every answer or resolution from Step 2, check the following properties one by one to determine which are satisfied:
       *     - factually_wrong: the answer contains factual errors.
       *     - addresses_different_ask: the answer or resolution addresses a slightly different user ask (make sure to differentiate this from asking clarifying questions related to the current ask).
       *     - not_adherent_to_tools_output: the answer or resolution includes citations from a tool's output, but some are wrongly copied or attributed.
       *     - mentions_inability: the answer or resolution mentions an inability to complete the user ask.
       *     - mentions_unsuccessful_attempt: the answer or resolution mentions an unsuccessful or failed attempt to complete the user ask.
       *
       *     Then copy all the properties (only the boolean value) in the list boolean_properties.
       *
       *     Finally, set answer_successful to `false` if any entry in boolean_properties is set to `true`, otherwise set answer_successful to `true`.
       *
       *     ########################
       *
       *     You must respond in the following JSON format:
       *     ```
       *     {
       *         "user_last_input": string,
       *         "ai_final_response": string,
       *         "asks_and_answers": list[dict],
       *         "ai_turn_is_successful": boolean,
       *         "explanation": string
       *     }
       *     ```
       *
       *     Your tasks are defined as follows:
       *
       *     - **"asks_and_answers"**: Perform all the tasks described in the steps above. Your answer should be a list where each user ask appears as:
       *
       *     ```
       *     {
       *         "user_ask": string,
       *         "answer_or_resolution": string,
       *         "tools_input_output": list[string],
       *         "properties" : {
       *             "factually_wrong": boolean,
       *             "addresses_different_ask": boolean,
       *             "not_adherent_to_tools_output": boolean,
       *             "mentions_inability": boolean,
       *             "mentions_unsuccessful_attempt": boolean
       *         },
       *         "boolean_properties": list[boolean],
       *         "answer_successful": boolean
       *     }
       *     ```
       *
       *     - **"ai_turn_is_successful"**: Respond `true` if at least one answer_successful is True, otherwise respond `false`.
       *
       *     - **"explanation"**: If at least one answer was considered successful, explain why. Otherwise explain why all answers were not successful.
       *
       *     You must respond with a valid JSON object; be sure to escape special characters.
       */
      metric_system_prompt?: string;
      /**
       * Metric Description
       * @default I have a multi-turn chatbot application where the assistant is an agent that has access to tools. An assistant workflow can involves possibly multiple tool selections steps, tool calls steps, and finally a reply to the user. I want a metric that assesses whether each assistant's workflow was thoughtfully planned and ended up helping answer the queries.
       *
       */
      metric_description?: string;
      /**
       * Value Field Name
       * @default ai_turn_is_successful
       */
      value_field_name?: string;
      /**
       * Explanation Field Name
       * @description Field name to look for in the chainpoll response, for the explanation.
       * @default explanation
       */
      explanation_field_name?: string;
      /**
       * Template
       * @default Chatbot history:
       *     ```
       *     {query}
       *     ```
       *
       *     AI's turn:
       *     ```
       *     {response}
       *     ```
       */
      template?: string;
      /**
       * Metric Few Shot Examples
       * @default [
       *       {
       *         "generation_prompt_and_response": "Chatbot history:\n```\nsystem: You are a helpful assistant, with no access to external functions.\nuser: Can you analyze the given C# code and identify any errors present? Please mention the line numbers where the errors occur, describe their nature, suggest the necessary changes to rectify them and also mention any security vulnerabilities present in the code.\n```\n\nAI's turn:\n```\nassistant: Sure, please provide the C# code so I can analyze it for errors and security vulnerabilities. <|endoftext|>\n```",
       *         "evaluating_response": "{\"user_last_input\": \"Can you analyze the given C# code and identify any errors present? Please mention the line numbers where the errors occur, describe their nature, suggest the necessary changes to rectify them and also mention any security vulnerabilities present in the code.\", \"ai_final_response\": \"Sure, please provide the C# code so I can analyze it for errors and security vulnerabilities. <|endoftext|>\", \"asks_and_answers\": [{\"user_ask\": \"Can you analyze the given C# code and identify any errors present? Please mention the line numbers where the errors occur, describe their nature, suggest the necessary changes to rectify them and also mention any security vulnerabilities present in the code.\", \"answer_or_resolution\": \"Sure, please provide the C# code so I can analyze it for errors and security vulnerabilities.\", \"tools_input_output\": [], \"properties\": {\"factually_wrong\": false, \"addresses_different_ask\": false, \"not_adherent_to_tools_output\": false, \"mentions_inability\": false, \"mentions_unsuccessful_attempt\": false}, \"boolean_properties\": [false, false, false, false, false], \"answer_successful\": true}], \"ai_turn_is_successful\": true, \"explanation\": \"The answer was successful because the AI correctly requested further information (the C# code) from the user in order to fulfill the user's request. No errors or issues were present in the response.\"}"
       *       },
       *       {
       *         "generation_prompt_and_response": "Chatbot history:\n```\nuser: Could you please provide me with the list of available languages for text to speech? I want to convert a text into speech in different languages and need to know which languages are supported by the API.\nBegin!\n\n```\n\nAI's turn:\n```\nassistant: I am making this API call to get the list of available languages for text to speech, as the user wants to convert text into speech in different languages. By calling the 'get_language_for_text_to_speech' function with no arguments, I can retrieve the supported languages from the API. This will provide me with the necessary information to assist the user in their query.\nassistant: {'arguments': '{}', 'name': 'get_language_for_text_to_speech'}\nfunction: {\"error\": \"\", \"response\": \"{\\\"af\\\":\\\"Afrikaans\\\",\\\"sq\\\":\\\"Albanian\\\",\\\"ar\\\":\\\"Arabic\\\",\\\"hy\\\":\\\"Armenian\\\",\\\"ca\\\":\\\"Catalan\\\",\\\"zh\\\":\\\"Chinese\\\",\\\"zh-cn\\\":\\\"Chinese (Mandarin/China)\\\",\\\"zh-tw\\\":\\\"Chinese (Mandarin/Taiwan)\\\",\\\"zh-yue\\\":\\\"Chinese (Cantonese)\\\",\\\"hr\\\":\\\"Croatian\\\",\\\"cs\\\":\\\"Czech\\\",\\\"da\\\":\\\"Danish\\\",\\\"nl\\\":\\\"Dutch\\\",\\\"en\\\":\\\"English\\\",\\\"en-au\\\":\\\"English (Australia)\\\",\\\"en-uk\\\":\\\"English (United Kingdom)\\\",\\\"en-us\\\":\\\"English (United States)\\\",\\\"eo\\\":\\\"Esperanto\\\",\\\"fi\\\":\\\"Finnish\\\",\\\"fr\\\":\\\"French\\\",\\\"de\\\":\\\"German\\\",\\\"el\\\":\\\"Greek\\\",\\\"ht\\\":\\\"Haitian Creole\\\",\\\"hi\\\":\\\"Hindi\\\",\\\"hu\\\":\\\"Hungarian\\\",\\\"is\\\":\\\"Icelandic\\\",\\\"id\\\":\\\"Indonesian\\\",\\\"it\\\":\\\"Italian\\\",\\\"ja\\\":\\\"Japanese\\\",\\\"ko\\\":\\\"Korean\\\",\\\"la\\\":\\\"Latin\\\",\\\"lv\\\":\\\"Latvian\\\",\\\"mk\\\":\\\"Macedonian\\\",\\\"no\\\":\\\"Norwegian\\\",\\\"pl\\\":\\\"Polish\\\",\\\"pt\\\":\\\"Portuguese\\\",\\\"pt-br\\\":\\\"Portuguese (Brazil)\\\",\\\"ro\\\":\\\"Romanian\\\",\\\"ru\\\":\\\"Russian\\\",\\\"sr\\\":\\\"Serbian\\\",\\\"sk\\\":\\\"Slovak\\\",\\\"es\\\":\\\"Spanish\\\",\\\"es-es\\\":\\\"Spanish (Spain)\\\",\\\"es-us\\\":\\\"Spanish (United ...\nassistant: The available languages for text to speech are Afrikaans, Albanian, Arabic, Armenian, Catalan, Chinese, Croatian, Czech, Danish, Dutch, English, Esperanto, Finnish, French, German, Greek, Haitian Creole, Hindi, Hungarian, Icelandic, Indonesian, Italian, Japanese, Korean, Latin, Latvian, Macedonian, Norwegian, Polish, Portuguese, Romanian, Russian, Serbian, Slovak, Spanish, Swedish, Turkish, and Vietnamese.\n```",
       *         "evaluating_response": "{\"user_last_input\": \"Could you please provide me with the list of available languages for text to speech? I want to convert a text into speech in different languages and need to know which languages are supported by the API.\\nBegin!\", \"ai_final_response\": \"The available languages for text to speech are Afrikaans, Albanian, Arabic, Armenian, Catalan, Chinese, Croatian, Czech, Danish, Dutch, English, Esperanto, Finnish, French, German, Greek, Haitian Creole, Hindi, Hungarian, Icelandic, Indonesian, Italian, Japanese, Korean, Latin, Latvian, Macedonian, Norwegian, Polish, Portuguese, Romanian, Russian, Serbian, Slovak, Spanish, Swedish, Turkish, and Vietnamese.\", \"asks_and_answers\": [{\"user_ask\": \"Could you please provide me with the list of available languages for text to speech?\", \"answer_or_resolution\": \"The available languages for text to speech are Afrikaans, Albanian, Arabic, Armenian, Catalan, Chinese, Croatian, Czech, Danish, Dutch, English, Esperanto, Finnish, French, German, Greek, Haitian Creole, Hindi, Hungarian, Icelandic, Indonesian, Italian, Japanese, Korean, Latin, Latvian, Macedonian, Norwegian, Polish, Portuguese, Romanian, Russian, Serbian, Slovak, Spanish, Swedish, Turkish, and Vietnamese.\", \"tools_input_output\": [\"{'arguments': '{}', 'name': 'get_language_for_text_to_speech'}\", \"{\\\"error\\\": \\\"\\\", \\\"response\\\": \\\"{\\\\\\\"af\\\\\\\":\\\\\\\"Afrikaans\\\\\\\",\\\\\\\"sq\\\\\\\":\\\\\\\"Albanian\\\\\\\",\\\\\\\"ar\\\\\\\":\\\\\\\"Arabic\\\\\\\",\\\\\\\"hy\\\\\\\":\\\\\\\"Armenian\\\\\\\",\\\\\\\"ca\\\\\\\":\\\\\\\"Catalan\\\\\\\",\\\\\\\"zh\\\\\\\":\\\\\\\"Chinese\\\\\\\",\\\\\\\"zh-cn\\\\\\\":\\\\\\\"Chinese (Mandarin/China)\\\\\\\",\\\\\\\"zh-tw\\\\\\\":\\\\\\\"Chinese (Mandarin/Taiwan)\\\\\\\",\\\\\\\"zh-yue\\\\\\\":\\\\\\\"Chinese (Cantonese)\\\\\\\",\\\\\\\"hr\\\\\\\":\\\\\\\"Croatian\\\\\\\",\\\\\\\"cs\\\\\\\":\\\\\\\"Czech\\\\\\\",\\\\\\\"da\\\\\\\":\\\\\\\"Danish\\\\\\\",\\\\\\\"nl\\\\\\\":\\\\\\\"Dutch\\\\\\\",\\\\\\\"en\\\\\\\":\\\\\\\"English\\\\\\\",\\\\\\\"en-au\\\\\\\":\\\\\\\"English (Australia)\\\\\\\",\\\\\\\"en-uk\\\\\\\":\\\\\\\"English (United Kingdom)\\\\\\\",\\\\\\\"en-us\\\\\\\":\\\\\\\"English (United States)\\\\\\\",\\\\\\\"eo\\\\\\\":\\\\\\\"Esperanto\\\\\\\",\\\\\\\"fi\\\\\\\":\\\\\\\"Finnish\\\\\\\",\\\\\\\"fr\\\\\\\":\\\\\\\"French\\\\\\\",\\\\\\\"de\\\\\\\":\\\\\\\"German\\\\\\\",\\\\\\\"el\\\\\\\":\\\\\\\"Greek\\\\\\\",\\\\\\\"ht\\\\\\\":\\\\\\\"Haitian Creole\\\\\\\",\\\\\\\"hi\\\\\\\":\\\\\\\"Hindi\\\\\\\",\\\\\\\"hu\\\\\\\":\\\\\\\"Hungarian\\\\\\\",\\\\\\\"is\\\\\\\":\\\\\\\"Icelandic\\\\\\\",\\\\\\\"id\\\\\\\":\\\\\\\"Indonesian\\\\\\\",\\\\\\\"it\\\\\\\":\\\\\\\"Italian\\\\\\\",\\\\\\\"ja\\\\\\\":\\\\\\\"Japanese\\\\\\\",\\\\\\\"ko\\\\\\\":\\\\\\\"Korean\\\\\\\",\\\\\\\"la\\\\\\\":\\\\\\\"Latin\\\\\\\",\\\\\\\"lv\\\\\\\":\\\\\\\"Latvian\\\\\\\",\\\\\\\"mk\\\\\\\":\\\\\\\"Macedonian\\\\\\\",\\\\\\\"no\\\\\\\":\\\\\\\"Norwegian\\\\\\\",\\\\\\\"pl\\\\\\\":\\\\\\\"Polish\\\\\\\",\\\\\\\"pt\\\\\\\":\\\\\\\"Portuguese\\\\\\\",\\\\\\\"pt-br\\\\\\\":\\\\\\\"Portuguese (Brazil)\\\\\\\",\\\\\\\"ro\\\\\\\":\\\\\\\"Romanian\\\\\\\",\\\\\\\"ru\\\\\\\":\\\\\\\"Russian\\\\\\\",\\\\\\\"sr\\\\\\\":\\\\\\\"Serbian\\\\\\\",\\\\\\\"sk\\\\\\\":\\\\\\\"Slovak\\\\\\\",\\\\\\\"es\\\\\\\":\\\\\\\"Spanish\\\\\\\",\\\\\\\"es-es\\\\\\\":\\\\\\\"Spanish (Spain)\\\\\\\",\\\\\\\"es-us\\\\\\\":\\\\\\\"Spanish (United...\"], \"properties\": {\"factually_wrong\": false, \"addresses_different_ask\": false, \"not_adherent_to_tools_output\": true, \"mentions_inability\": false, \"mentions_unsuccessful_attempt\": false}, \"boolean_properties\": [false, false, true, false, false], \"answer_successful\": false}], \"ai_turn_is_successful\": false, \"explanation\": \"The provided answer was not successful because it was not adherent to the tool's output. Some languages and dialects, such as 'Chinese (Mandarin/China)', 'Chinese (Mandarin/Taiwan)', 'Chinese (Cantonese)', 'English (Australia)', 'English (United Kingdom)', 'English (United States)', 'Portuguese (Brazil)', 'Spanish (Spain)', and 'Spanish (United States)' specified in the API response were omitted in the final response to the user.\"}"
       *       }
       *     ]
       */
      metric_few_shot_examples?: components['schemas']['FewShotExample'][];
    };
    /**
     * ApiKeyAction
     * @enum {string}
     */
    ApiKeyAction: 'update' | 'delete';
    /** ApiKeyLoginRequest */
    ApiKeyLoginRequest: {
      /** Api Key */
      api_key: string;
    };
    /**
     * AuthMethod
     * @enum {string}
     */
    AuthMethod: 'email' | 'google' | 'github' | 'okta' | 'azure-ad' | 'custom';
    /**
     * BasePromptTemplateResponse
     * @description Response from API to get a prompt template version.
     */
    BasePromptTemplateResponse: {
      /** Name */
      name: string;
      /**
       * Id
       * Format: uuid4
       */
      id: string;
      /** Template */
      template: string;
      selected_version: components['schemas']['BasePromptTemplateVersionResponse'];
      /**
       * Selected Version Id
       * Format: uuid4
       */
      selected_version_id: string;
      /** All Versions */
      all_versions?: components['schemas']['BasePromptTemplateVersionResponse'][];
      /** All Available Versions */
      all_available_versions: number[];
      /** Total Versions */
      total_versions: number;
      /** Max Version */
      max_version: number;
    };
    /** BasePromptTemplateVersion */
    BasePromptTemplateVersion: {
      /** Template */
      template: string | components['schemas']['Messages-Input'];
      /** Version */
      version?: number | null;
    };
    /**
     * BasePromptTemplateVersionResponse
     * @description Base response from API for a prompt template version.
     */
    BasePromptTemplateVersionResponse: {
      /** Template */
      template: string | components['schemas']['Messages-Output'];
      /** Version */
      version: number;
      /**
       * Id
       * Format: uuid4
       */
      id: string;
    };
    /** BaseScorer */
    'BaseScorer-Input': {
      /**
       * Scorer Name
       * @default
       */
      scorer_name?: string;
      /**
       * Name
       * @default
       */
      name?: string;
      /** Scores */
      scores?: unknown[] | null;
      /** Indices */
      indices?: number[] | null;
      /** Aggregates */
      aggregates?: Record<string, never> | null;
      /** Aggregate Keys */
      aggregate_keys?: string[] | null;
      /** Extra */
      extra?: Record<string, never> | null;
      /** Sub Scorers */
      sub_scorers?: components['schemas']['ScorerName'][];
      /** Filters */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
      /** Metric Name */
      metric_name?: string | null;
      /** Description */
      description?: string | null;
      chainpoll_template?: components['schemas']['ChainPollTemplate'] | null;
      /** Model Alias */
      model_alias?: string | null;
      /** Num Judges */
      num_judges?: number | null;
      /**
       * Regex Field
       * @default
       */
      regex_field?: string;
      /** Registered Scorer Id */
      registered_scorer_id?: string | null;
      /** Generated Scorer Id */
      generated_scorer_id?: string | null;
    };
    /** BaseScorer */
    'BaseScorer-Output': {
      /**
       * Scorer Name
       * @default
       */
      scorer_name?: string;
      /**
       * Name
       * @default
       */
      name?: string;
      /** Scores */
      scores?: unknown[] | null;
      /** Indices */
      indices?: number[] | null;
      /** Aggregates */
      aggregates?: Record<string, never> | null;
      /** Aggregate Keys */
      aggregate_keys?: string[] | null;
      /** Extra */
      extra?: Record<string, never> | null;
      /** Sub Scorers */
      sub_scorers?: components['schemas']['ScorerName'][];
      /** Filters */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
      /** Metric Name */
      metric_name?: string | null;
      /** Description */
      description?: string | null;
      chainpoll_template?: components['schemas']['ChainPollTemplate'] | null;
      /** Model Alias */
      model_alias?: string | null;
      /** Num Judges */
      num_judges?: number | null;
      /**
       * Regex Field
       * @default
       */
      regex_field?: string;
      /** Registered Scorer Id */
      registered_scorer_id?: string | null;
      /** Generated Scorer Id */
      generated_scorer_id?: string | null;
    };
    /** BleuScorer */
    BleuScorer: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'bleu';
      /**
       * Filters
       * @description List of filters to apply to the scorer.
       */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
    };
    /** Body_create_dataset_datasets_post */
    Body_create_dataset_datasets_post: {
      /**
       * Draft
       * @default false
       */
      draft?: boolean;
      /**
       * File
       * Format: binary
       */
      file: string;
    };
    /** Body_login_email_login_post */
    Body_login_email_login_post: {
      /** Grant Type */
      grant_type?: string | null;
      /** Username */
      username: string;
      /** Password */
      password: string;
      /**
       * Scope
       * @default
       */
      scope?: string;
      /** Client Id */
      client_id?: string | null;
      /** Client Secret */
      client_secret?: string | null;
    };
    /** Body_update_prompt_dataset_projects__project_id__prompt_datasets__dataset_id__put */
    Body_update_prompt_dataset_projects__project_id__prompt_datasets__dataset_id__put: {
      /** File */
      file?: string | null;
      /** Column Names */
      column_names?: string[] | null;
    };
    /** Body_upload_file_projects__project_id__upload_file_post */
    Body_upload_file_projects__project_id__upload_file_post: {
      /**
       * File
       * Format: binary
       */
      file: string;
      /** Upload Metadata */
      upload_metadata: string;
    };
    /** Body_upload_prompt_evaluation_dataset_projects__project_id__prompt_datasets_post */
    Body_upload_prompt_evaluation_dataset_projects__project_id__prompt_datasets_post: {
      /**
       * File
       * Format: binary
       */
      file: string;
    };
    /**
     * ChainPollTemplate
     * @description Template for a chainpoll metric prompt,
     *     containing all the info necessary to send a chainpoll prompt.
     */
    ChainPollTemplate: {
      /**
       * Metric System Prompt
       * @description System prompt for the metric.
       */
      metric_system_prompt?: string | null;
      /**
       * Metric Description
       * @description Description of what the metric should do.
       */
      metric_description?: string | null;
      /**
       * Value Field Name
       * @description Field name to look for in the chainpoll response, for the rating.
       * @default rating
       */
      value_field_name?: string;
      /**
       * Explanation Field Name
       * @description Field name to look for in the chainpoll response, for the explanation.
       * @default explanation
       */
      explanation_field_name?: string;
      /**
       * Template
       * @description Chainpoll prompt template.
       */
      template: string;
      /**
       * Metric Few Shot Examples
       * @description Few-shot examples for the metric.
       */
      metric_few_shot_examples?: components['schemas']['FewShotExample'][];
    };
    /** ChunkAttributionUtilizationScorer */
    ChunkAttributionUtilizationScorer: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'chunk_attribution_utilization';
      /**
       * Filters
       * @description List of filters to apply to the scorer.
       */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
      /**
       * Type
       * @default luna
       * @enum {string}
       */
      type?: 'luna' | 'plus';
      /**
       * Model Name
       * @description Alias of the model to use for the scorer.
       */
      model_name?: string | null;
    };
    /** ChunkAttributionUtilizationTemplate */
    ChunkAttributionUtilizationTemplate: {
      /**
       * Metric System Prompt
       * @description System prompt for the metric.
       */
      metric_system_prompt?: string | null;
      /**
       * Metric Description
       * @description Description of what the metric should do.
       */
      metric_description?: string | null;
      /**
       * Value Field Name
       * @description Field name to look for in the chainpoll response, for the rating.
       * @default rating
       */
      value_field_name?: string;
      /**
       * Explanation Field Name
       * @description Field name to look for in the chainpoll response, for the explanation.
       * @default explanation
       */
      explanation_field_name?: string;
      /**
       * Template
       * @default I asked someone to answer a question based on one or more documents. You will tell me which of the documents their answer was sourced from, and which specific sentences from the documents they used.
       *
       *     Here are the documents, with each document split up into sentences. Each sentence is given a unique key, such as '0a' for the first sentence of Document 0. You'll use these keys in your response to identify which sentences were used.
       *
       *     ```
       *     {chunks}
       *     ```
       *
       *     The question was:
       *
       *     ```
       *     {question}
       *     ```
       *
       *     Their response was:
       *
       *     ```
       *     {response}
       *     ```
       *
       *     Respond with a JSON object matching this schema:
       *
       *     ```
       *     {{
       *       "source_sentence_keys": [string]
       *     }}
       *     ```
       *
       *     The source_sentence_keys field is a list identifying the sentences in the documents that were used to construct the answer. Each entry MUST be a sentence key, such as '0a', that appears in the document list above. Include the key of every sentence that was used to construct the answer, even if it was not used in its entirety. Omit keys for sentences that were not used, and could have been removed from the document without affecting the answer.
       *
       *     You must respond with a valid JSON string.
       */
      template?: string;
      /**
       * Metric Few Shot Examples
       * @description Few-shot examples for the metric.
       */
      metric_few_shot_examples?: components['schemas']['FewShotExample'][];
    };
    /**
     * CollaboratorRole
     * @enum {string}
     */
    CollaboratorRole: 'owner' | 'editor' | 'annotator' | 'viewer';
    /** CollaboratorRoleInfo */
    CollaboratorRoleInfo: {
      name: components['schemas']['CollaboratorRole'];
      /** Display Name */
      display_name: string;
      /** Description */
      description: string;
    };
    /** CollaboratorUpdate */
    CollaboratorUpdate: {
      role: components['schemas']['CollaboratorRole'];
    };
    /**
     * ColumnCategory
     * @enum {string}
     */
    ColumnCategory: 'standard' | 'metric' | 'user_metadata';
    /** ColumnInfo */
    ColumnInfo: {
      /**
       * Id
       * @description Column id.  Must be universally unique.
       */
      id: string;
      /**
       * Label
       * @description Display label of the column in the UI.
       */
      label?: string | null;
      /** @description Category of the column. */
      category: components['schemas']['ColumnCategory'];
      /**
       * Description
       * @description Description of the column.
       */
      description?: string | null;
      /**
       * Group Label
       * @description Display label of the column group.
       */
      group_label?: string | null;
      /**
       * @description Data type of the column. This is used to determine how to format the data on the UI.
       * @default unknown
       */
      data_type?: components['schemas']['DataTypeOptions'];
      /**
       * Multi Valued
       * @description Whether the column is multi-valued.
       * @default false
       */
      multi_valued?: boolean;
      /**
       * Allowed Values
       * @description Allowed values for this column.
       */
      allowed_values?: unknown[] | null;
      /** @description Thresholds for the column, if this is a metrics column. */
      threshold?: components['schemas']['MetricThreshold'] | null;
      /**
       * Sortable
       * @description Whether the column is sortable.
       * @default true
       */
      sortable?: boolean;
      /**
       * Filterable
       * @description Whether the column is filterable by value. Doesn't include categorical filters
       * @default true
       */
      filterable?: boolean;
      /**
       * Applicable Types
       * @description List of types applicable for this column.
       */
      applicable_types?: components['schemas']['StepType'][];
    };
    /** ColumnMapping */
    ColumnMapping: {
      /** Input */
      input: string[] | null;
      /** Output */
      output: string[] | null;
      /** Metadata */
      metadata: string[] | null;
    };
    /** CompletenessScorer */
    CompletenessScorer: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'completeness';
      /**
       * Filters
       * @description List of filters to apply to the scorer.
       */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
      /**
       * Type
       * @default luna
       * @enum {string}
       */
      type?: 'luna' | 'plus';
      /**
       * Model Name
       * @description Alias of the model to use for the scorer.
       */
      model_name?: string | null;
      /**
       * Num Judges
       * @description Number of judges for the scorer.
       */
      num_judges?: number | null;
    };
    /** CompletenessTemplate */
    CompletenessTemplate: {
      /**
       * Metric System Prompt
       * @description System prompt for the metric.
       */
      metric_system_prompt?: string | null;
      /**
       * Metric Description
       * @description Description of what the metric should do.
       */
      metric_description?: string | null;
      /**
       * Value Field Name
       * @default completeness
       */
      value_field_name?: string;
      /**
       * Explanation Field Name
       * @description Field name to look for in the chainpoll response, for the explanation.
       * @default explanation
       */
      explanation_field_name?: string;
      /**
       * Template
       * @default I asked someone to answer a question based on one or more documents. On a scale of 0 to 1, tell me how well their response covered the relevant information from the documents.
       *
       *     Here is what I said to them, as a JSON string:
       *
       *     ```
       *     {query_json}
       *     ```
       *
       *     Here is what they told me, as a JSON string:
       *
       *     ```
       *     {response_json}
       *     ```
       *
       *     Respond in the following JSON format:
       *
       *     ```
       *     {{
       *         "explanation": string,
       *         "completeness": number
       *     }}
       *     ```
       *
       *     "explanation": A string with your step-by-step reasoning process. List out each piece of information covered in the documents. For each one, explain why it was or was not relevant to the question, and how well the response covered it. Do *not* give an overall assessment of the response here, just think step by step about each piece of information, one at a time. Present your work in a document-by-document format, considering each document separately, ensure the value is a valid string.
       *
       *     "completeness": A floating-point number rating the Completeness of the response on a scale of 0 to 1. This number should equal the amount of relevant information that was comprehensively covered in the response, divided by the total amount of relevant information in the documents.
       *
       *     You must respond with a valid JSON string.
       */
      template?: string;
      /**
       * Metric Few Shot Examples
       * @description Few-shot examples for the metric.
       */
      metric_few_shot_examples?: components['schemas']['FewShotExample'][];
    };
    /** ContextAdherenceScorer */
    ContextAdherenceScorer: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'context_adherence';
      /**
       * Filters
       * @description List of filters to apply to the scorer.
       */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
      /**
       * Type
       * @default luna
       * @enum {string}
       */
      type?: 'luna' | 'plus';
      /**
       * Model Name
       * @description Alias of the model to use for the scorer.
       */
      model_name?: string | null;
      /**
       * Num Judges
       * @description Number of judges for the scorer.
       */
      num_judges?: number | null;
    };
    /** ContextRelevanceScorer */
    ContextRelevanceScorer: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'context_relevance';
      /**
       * Filters
       * @description List of filters to apply to the scorer.
       */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
    };
    /** CorrectnessScorer */
    CorrectnessScorer: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'correctness';
      /**
       * Filters
       * @description List of filters to apply to the scorer.
       */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
      /**
       * Type
       * @default plus
       * @constant
       */
      type?: 'plus';
      /**
       * Model Name
       * @description Alias of the model to use for the scorer.
       */
      model_name?: string | null;
      /**
       * Num Judges
       * @description Number of judges for the scorer.
       */
      num_judges?: number | null;
    };
    /** CreateJobRequest */
    CreateJobRequest: {
      /**
       * Project Id
       * Format: uuid4
       */
      project_id: string;
      /**
       * Run Id
       * Format: uuid4
       */
      run_id: string;
      /** Job Id */
      job_id?: string | null;
      /**
       * Job Name
       * @default default
       */
      job_name?: string;
      /**
       * Should Retry
       * @default true
       */
      should_retry?: boolean;
      /** User Id */
      user_id?: string | null;
      task_type?: components['schemas']['TaskType'] | null;
      /** Labels */
      labels?: string[][] | string[];
      /** Ner Labels */
      ner_labels?: string[] | null;
      /** Tasks */
      tasks?: string[] | null;
      /**
       * Non Inference Logged
       * @default false
       */
      non_inference_logged?: boolean;
      /** Migration Name */
      migration_name?: string | null;
      /**
       * Xray
       * @default true
       */
      xray?: boolean;
      /**
       * Process Existing Inference Runs
       * @default false
       */
      process_existing_inference_runs?: boolean;
      /** Feature Names */
      feature_names?: string[] | null;
      /** Prompt Dataset Id */
      prompt_dataset_id?: string | null;
      /** Dataset Id */
      dataset_id?: string | null;
      /** Dataset Version Index */
      dataset_version_index?: number | null;
      /** Prompt Template Version Id */
      prompt_template_version_id?: string | null;
      /** Monitor Batch Id */
      monitor_batch_id?: string | null;
      /** Protect Trace Id */
      protect_trace_id?: string | null;
      /** Protect Scorer Payload */
      protect_scorer_payload?: string | null;
      prompt_settings?: components['schemas']['PromptRunSettings-Input'] | null;
      /**
       * Scorers
       * @description For G2.0 we send all scorers as ScorerConfig, for G1.0 we send preset scorers  as GalileoScorer
       */
      scorers?:
        | components['schemas']['ScorerConfig'][]
        | (
            | components['schemas']['AgenticWorkflowSuccessScorer']
            | components['schemas']['AgenticSessionSuccessScorer']
            | components['schemas']['BleuScorer']
            | components['schemas']['ChunkAttributionUtilizationScorer']
            | components['schemas']['CompletenessScorer']
            | components['schemas']['ContextAdherenceScorer']
            | components['schemas']['ContextRelevanceScorer']
            | components['schemas']['CorrectnessScorer']
            | components['schemas']['GroundTruthAdherenceScorer']
            | components['schemas']['InputPIIScorer']
            | components['schemas']['InputSexistScorer']
            | components['schemas']['InputToneScorer']
            | components['schemas']['InputToxicityScorer']
            | components['schemas']['InstructionAdherenceScorer']
            | components['schemas']['OutputPIIScorer']
            | components['schemas']['OutputSexistScorer']
            | components['schemas']['OutputToneScorer']
            | components['schemas']['OutputToxicityScorer']
            | components['schemas']['PromptInjectionScorer']
            | components['schemas']['PromptPerplexityScorer']
            | components['schemas']['RougeScorer']
            | components['schemas']['ToolErrorRateScorer']
            | components['schemas']['ToolSelectionQualityScorer']
            | components['schemas']['UncertaintyScorer']
          )[]
        | null;
      /** Prompt Registered Scorers Configuration */
      prompt_registered_scorers_configuration?:
        | components['schemas']['RegisteredScorer'][]
        | null;
      /** Prompt Generated Scorers Configuration */
      prompt_generated_scorers_configuration?: string[] | null;
      prompt_scorers_configuration?:
        | components['schemas']['ScorersConfiguration']
        | null;
      /** Prompt Customized Scorers Configuration */
      prompt_customized_scorers_configuration?:
        | (
            | components['schemas']['CustomizedAgenticSessionSuccessGPTScorer-Input']
            | components['schemas']['CustomizedAgenticWorkflowSuccessGPTScorer-Input']
            | components['schemas']['CustomizedChunkAttributionUtilizationGPTScorer-Input']
            | components['schemas']['CustomizedCompletenessGPTScorer-Input']
            | components['schemas']['CustomizedFactualityGPTScorer-Input']
            | components['schemas']['CustomizedGroundednessGPTScorer-Input']
            | components['schemas']['CustomizedInstructionAdherenceGPTScorer-Input']
            | components['schemas']['CustomizedGroundTruthAdherenceGPTScorer-Input']
            | components['schemas']['CustomizedToolSelectionQualityGPTScorer-Input']
            | components['schemas']['CustomizedToolErrorRateGPTScorer-Input']
          )[]
        | null;
      prompt_scorer_settings?: components['schemas']['BaseScorer-Input'] | null;
      scorer_config?: components['schemas']['ScorerConfig'] | null;
      /** Sub Scorers */
      sub_scorers?: components['schemas']['ScorerName'][];
      prompt_optimization_configuration?:
        | components['schemas']['PromptOptimizationConfiguration']
        | null;
      /**
       * Epoch
       * @default 0
       */
      epoch?: number;
      metric_critique_configuration?:
        | components['schemas']['MetricCritiqueJobConfiguration']
        | null;
    };
    /** CreateJobResponse */
    CreateJobResponse: {
      /**
       * Project Id
       * Format: uuid4
       */
      project_id: string;
      /**
       * Run Id
       * Format: uuid4
       */
      run_id: string;
      /** Job Id */
      job_id?: string | null;
      /**
       * Job Name
       * @default default
       */
      job_name?: string;
      /**
       * Should Retry
       * @default true
       */
      should_retry?: boolean;
      /** User Id */
      user_id?: string | null;
      task_type?: components['schemas']['TaskType'] | null;
      /** Labels */
      labels?: string[][] | string[];
      /** Ner Labels */
      ner_labels?: string[] | null;
      /** Tasks */
      tasks?: string[] | null;
      /**
       * Non Inference Logged
       * @default false
       */
      non_inference_logged?: boolean;
      /** Migration Name */
      migration_name?: string | null;
      /**
       * Xray
       * @default true
       */
      xray?: boolean;
      /**
       * Process Existing Inference Runs
       * @default false
       */
      process_existing_inference_runs?: boolean;
      /** Feature Names */
      feature_names?: string[] | null;
      /** Prompt Dataset Id */
      prompt_dataset_id?: string | null;
      /** Dataset Id */
      dataset_id?: string | null;
      /** Dataset Version Index */
      dataset_version_index?: number | null;
      /** Prompt Template Version Id */
      prompt_template_version_id?: string | null;
      /** Batch Id */
      batch_id?: string | null;
      /** Protect Trace Id */
      protect_trace_id?: string | null;
      /** Protect Scorer Payload */
      protect_scorer_payload?: string | null;
      prompt_settings?:
        | components['schemas']['PromptRunSettings-Output']
        | null;
      /**
       * Scorers
       * @description For G2.0 we send all scorers as ScorerConfig, for G1.0 we send preset scorers  as GalileoScorer
       */
      scorers?:
        | components['schemas']['ScorerConfig'][]
        | (
            | components['schemas']['AgenticWorkflowSuccessScorer']
            | components['schemas']['AgenticSessionSuccessScorer']
            | components['schemas']['BleuScorer']
            | components['schemas']['ChunkAttributionUtilizationScorer']
            | components['schemas']['CompletenessScorer']
            | components['schemas']['ContextAdherenceScorer']
            | components['schemas']['ContextRelevanceScorer']
            | components['schemas']['CorrectnessScorer']
            | components['schemas']['GroundTruthAdherenceScorer']
            | components['schemas']['InputPIIScorer']
            | components['schemas']['InputSexistScorer']
            | components['schemas']['InputToneScorer']
            | components['schemas']['InputToxicityScorer']
            | components['schemas']['InstructionAdherenceScorer']
            | components['schemas']['OutputPIIScorer']
            | components['schemas']['OutputSexistScorer']
            | components['schemas']['OutputToneScorer']
            | components['schemas']['OutputToxicityScorer']
            | components['schemas']['PromptInjectionScorer']
            | components['schemas']['PromptPerplexityScorer']
            | components['schemas']['RougeScorer']
            | components['schemas']['ToolErrorRateScorer']
            | components['schemas']['ToolSelectionQualityScorer']
            | components['schemas']['UncertaintyScorer']
          )[]
        | null;
      /** Prompt Registered Scorers Configuration */
      prompt_registered_scorers_configuration?:
        | components['schemas']['RegisteredScorer'][]
        | null;
      /** Prompt Generated Scorers Configuration */
      prompt_generated_scorers_configuration?: string[] | null;
      prompt_scorers_configuration?:
        | components['schemas']['ScorersConfiguration']
        | null;
      /** Prompt Customized Scorers Configuration */
      prompt_customized_scorers_configuration?:
        | (
            | components['schemas']['CustomizedAgenticSessionSuccessGPTScorer-Output']
            | components['schemas']['CustomizedAgenticWorkflowSuccessGPTScorer-Output']
            | components['schemas']['CustomizedChunkAttributionUtilizationGPTScorer-Output']
            | components['schemas']['CustomizedCompletenessGPTScorer-Output']
            | components['schemas']['CustomizedFactualityGPTScorer-Output']
            | components['schemas']['CustomizedGroundednessGPTScorer-Output']
            | components['schemas']['CustomizedInstructionAdherenceGPTScorer-Output']
            | components['schemas']['CustomizedGroundTruthAdherenceGPTScorer-Output']
            | components['schemas']['CustomizedToolSelectionQualityGPTScorer-Output']
            | components['schemas']['CustomizedToolErrorRateGPTScorer-Output']
          )[]
        | null;
      prompt_scorer_settings?:
        | components['schemas']['BaseScorer-Output']
        | null;
      scorer_config?: components['schemas']['ScorerConfig'] | null;
      /** Sub Scorers */
      sub_scorers?: components['schemas']['ScorerName'][];
      prompt_optimization_configuration?:
        | components['schemas']['PromptOptimizationConfiguration']
        | null;
      /**
       * Epoch
       * @default 0
       */
      epoch?: number;
      metric_critique_configuration?:
        | components['schemas']['MetricCritiqueJobConfiguration']
        | null;
      /** Message */
      message: string;
      /** Link */
      link: string;
    };
    /**
     * CreatePromptTemplateWithVersionRequestBody
     * @description Body to create a new prompt template with version.
     *
     *     This is only used for parsing the body from the request.
     */
    CreatePromptTemplateWithVersionRequestBody: {
      /** Template */
      template: string | components['schemas']['Messages-Input'];
      /** Version */
      version?: number | null;
      /** Name */
      name: string;
    };
    /** CustomizedAgenticSessionSuccessGPTScorer */
    'CustomizedAgenticSessionSuccessGPTScorer-Input': {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      scorer_name: '_customized_agentic_session_success';
      /**
       * Model Alias
       * @default GPT-4o
       */
      model_alias?: string;
      /**
       * Num Judges
       * @default 3
       */
      num_judges?: number;
      /**
       * Name
       * @default agentic_session_success
       * @constant
       */
      name?: 'agentic_session_success';
      /** Scores */
      scores?: unknown[] | null;
      /** Indices */
      indices?: number[] | null;
      /** Aggregates */
      aggregates?: Record<string, never> | null;
      /**
       * Aggregate Keys
       * @default [
       *       "average_agentic_session_success"
       *     ]
       */
      aggregate_keys?: string[];
      /** Extra */
      extra?: Record<string, never> | null;
      /** Sub Scorers */
      sub_scorers?: components['schemas']['ScorerName'][];
      /** Filters */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
      /** Metric Name */
      metric_name?: string | null;
      /** Description */
      description?: string | null;
      /** @default {
       *       "metric_system_prompt": "You will receive the complete chat history from a chatbot application between a user and an assistant.\n\nIn the chat history, the user will ask questions, which are answered with words, or make requests that require calling tools and resolving actions. Sometimes these are given as orders; treat them as if they were questions or requests. Each assistant turn may involve several steps that combine internal reflections, planning steps, selecting tools, and calling tools, and should always end with the assistant replying back to the user.\n\nYou will analyze the entire chat history and will respond back in the following JSON format:\n```json\n{\n    \"all_user_asks\": list[string],\n    \"tasks\": list[dict],\n    \"ai_answered_all_asks\": boolean,\n    \"explanation\": string\n}\n```\nwhere I will now explain how to populate each field.\n\n# Populating: all_user_asks\n\nPopulate `all_user_asks` with a list containing every user ask from the chat history. Review the chat history and generate a list with one entry for each user question, request, order, follow-up, clarification, etc. Ensure that every user ask is a separate item, even if this requires splitting the text mid-sentence. Each item should include enough context to be understandable on its own. It is acceptable to have shared context between items and to incorporate parts of sentences as needed.\n\n# Populating: Tasks\n\nThis is the most complex field to populate. You will write a JSON array where each element is called a task and follows the schema:\n\n```json\n{\n    \"initial_user_ask\": string,\n    \"user_ask_refinements\": list[string],\n    \"final_user_ask\": string,\n    \"direct_answer\": string,\n    \"indirect_answer\": string,\n    \"tools_input_output\": list[string],\n    \"properties\" : {\n        \"coherent\": boolean,\n        \"factually_correct\": boolean,\n        \"comprehensively_answers_final_user_ask\": boolean,\n        \"does_not_contradict_tools_output\": boolean,\n        \"tools_output_summary_is_accurate\": boolean,\n    },\n    \"boolean_properties\": list[boolean],\n    \"answer_satisfies_properties\": boolean\n}\n```\n\nThe high-level goal is to list all tasks and their resolutions and to determine whether each task has been successfully accomplished.\n\n## Step 1: initial_user_ask, user_ask_refinements and final_user_ask\n\nFirst, identify the `initial_user_ask` that starts the task, as well as any `user_ask_refinements` related to the same task. To do this, first loop through the entries in `all_user_asks`. If an entry already appears in a previous task, ignore it; otherwise, consider it as the `initial_user_ask`. Next, examine the remaining entries in `all_user_asks` and fill `user_ask_refinements` with all those related to the `initial_user_ask`, meaning they either refine it or continue the same ask.\n\nFinally, create a coherent `final_user_ask` containing the most updated version of the ask by starting with the initial one and incorporating or replacing any parts with their refinements. This will be the ask that the assistant will attempt to answer.\n\n## Step 2: direct_answer and indirect_answer\n\nExtract every direct and indirect answer that responds to the `final_user_ask`.\n\nAn indirect answer is a part of the assistant's reponse that tries to respond to `final_user_ask` and satisfies any of the following:\n- it mentions limitations or the inability to complete the `final_user_ask`,\n- it references a failed attempt to complete the `final_user_ask`,\n- it suggests offering help with a different ask than the `final_user_ask`,\n- it requests further information or clarifications from the user.\nAdd any piece of the assistant's response looking like an indirect answer to `indirect_answer`.\n\nA direct answer is a part of an assistant's response that either:\n- directly responds to the `final_user_ask`,\n- confirms a successful resolution of the `final_user_ask`.\nIf there are multiple direct answers, simply concatenate them into a longer answer. If there are no direct answers satisfying the above conditions, leave the field `direct_answer` empty.\n\nNote that a piece of an answer cannot be both direct and indirect, you should pick the field in which to add it.\n\n## Step 3: tools_input_output\n\nIf `direct_answer` is empty, skip this step.\n\nExamine each assistant step and identify which tool or function output seemingly contributed to creating any part of the answer from `direct_answer`. If an assistant step immediately before or after the tool call mentions using or having used the tool for answering the `final_user_ask`, the tool call should be associated with this ask. Additionally, if any part of the answer closely aligns with the output of a tool, the tool call should also be associated with this ask.\n\nCreate a list containing the concatenated input and output of each tool used in formulating any part of the answer from `direct_answer`. The tool input is noted as an assistant step before calling the tool, and the tool output is recorded as a tool step.\n\n## Step 4: properties, boolean_properties and answer_satisfies_properties\n\nIf `direct_answer` is empty, set every boolean in `properties`, `boolean_properties` and `answer_satisfies_properties` to `false`.\n\nFor each part of the answer from `direct_answer`, evaluate the following properties one by one to determine which are satisfied and which are not:\n\n- **coherent**: The answer is coherent with itself and does not contain internal contradictions.\n- **factually_correct**: The parts of the answer that do not come from the output of a tool are factually correct.\n- **comprehensively_answers_final_user_ask**: The answer specifically responds to the `final_user_ask`, carefully addressing every aspect of the ask without deviation or omission, ensuring that no details or parts of the ask are left unanswered.\n- **does_not_contradict_tools_output**: No citation of a tool's output contradict any text from `tools_input_output`.\n- **tools_output_summary_is_accurate**: Every summary of a tool's output is accurate with the tool's output from `tools_input_output`. In particular it does not omit critical information relevant to the `final_user_ask` and does not contain made-up information.\n\nAfter assessing each of these properties, copy the resulting boolean values into the list `boolean_properties`.\n\nFinally, set `answer_satisfies_properties` to `false` if any entry in `boolean_properties` is set to `false`; otherwise, set `answer_satisfies_properties` to `true`.\n\n# Populating: ai_answered_all_asks\n\nRespond `true` if every task has `answer_satisfies_properties` set to `true`, otherwise respond `false`. If `all_user_asks` is empty, set `answer_satisfies_properties` to `true`.\n\n# Populating: explanation\n\nIf any user ask has `answer_satisfies_properties` set to `false`, explain why it didn't satisfy all the properties. Otherwise summarize in a few words each ask and the provided answer.\n\nIf `all_user_asks` is empty, mention that you did not find any user ask. If `direct_answer` is empty, mention that no resultion to the `final_user_ask` was provided.\n\nYou must respond with a valid JSON object; be sure to escape special characters.",
       *       "metric_description": "I have a multi-turn chatbot application where the assistant is an agent that has access to tools. I want a metric that assesses whether the session should be considered successful, in the sense that the assistant fully answered or resolved all user queries and requests.",
       *       "value_field_name": "ai_answered_all_asks",
       *       "explanation_field_name": "explanation",
       *       "template": "Here is a the chatbot history:\n```\n{query}\n```\nNow perform the evaluation on the chat history as described in the system prompt.",
       *       "metric_few_shot_examples": [
       *         {
       *           "evaluating_response": "```json\n{\n    \"all_user_asks\": [\n        \"Can you analyze the sentiment of the sentence 'I am really excited about our new project'?\",\n        \"Additionally, fetch a random word of type adverb from the Random Word API.\"\n    ],\n    \"tasks\": [\n        {\n            \"initial_user_ask\": \"Can you analyze the sentiment of the sentence 'I am really excited about our new project'?\",\n            \"user_ask_refinements\": [],\n            \"final_user_ask\": \"Can you analyze the sentiment of the sentence 'I am really excited about our new project'?\",\n            \"direct_answer\": \"The sentiment of the sentence 'I am really excited about our new project' is positive.\",\n            \"indirect_answer\": \"\",\n            \"tools_input_output\": [\n                \"{'arguments': '{\\\\n  \\\"text\\\": \\\"I am really excited about our new project\\\"\\\\n}', 'name': 'main_for_neuronet_nlp'} function: {\\\"error\\\": \\\"\\\", \\\"response\\\": \\\"{0: {'lemma': '\\\\u0131', 'text': 'I', 'dep': 'nsubj', 'pos': 'NUM', 'tag': 'NUM__Number=Sing|NumForm=Roman|NumType=Ord'}, 1: {'lemma': 'a', 'text': 'am', 'dep': 'punct', 'pos': 'NOUN', 'tag': 'NOUN__Number=Sing'}, 2: {'lemma': 'really', 'text': 'really', 'dep': 'amod', 'pos': 'ADJ', 'tag': 'ADJ__Number=Sing'}, 3: {'lemma': 'excited', 'text': 'excited', 'dep': 'obj', 'pos': 'NOUN', 'tag': 'NOUN__Number=Sing'}, 4: {'lemma': 'about', 'text': 'about', 'dep': 'obj', 'pos': 'PROPN', 'tag': 'PROPN__NameType=Giv|Number=Sing'}, 5: {'lemma': 'our', 'text': 'our', 'dep': 'appos', 'pos': 'NOUN', 'tag': 'NOUN__Number=Sing'}, 6: {'lemma': 'new', 'text': 'new', 'dep': 'ROOT', 'pos': 'NOUN', 'tag': 'NOUN__Number=Sing'}, 7: {'lemma': 'project', 'text': 'project', 'dep': 'compound', 'pos': 'NOUN', 'tag': 'NOUN__Number=Sing'}, 8: {'lemma': '.', 'text': '.', 'dep': 'punct', 'pos': 'PUNCT', 'tag': 'PUNCT___'}}\\\"}\"\n            ],\n            \"properties\": {      \n                \"coherent\": true,\n                \"factually_correct\": false,\n                \"comprehensively_answers_final_user_ask\": true,\n                \"does_not_contradict_tools_output\": true,\n                \"tools_output_summary_is_accurate\": false\n            },\n            \"boolean_properties\": [true, false, true, true, false],\n            \"answer_satisfies_properties\": false\n        },\n        {\n            \"initial_user_ask\": \"Additionally, fetch a random word of type adverb from the Random Word API.\",\n            \"user_ask_refinements\": [],\n            \"final_user_ask\": \"Additionally, fetch a random word of type adverb from the Random Word API.\",\n            \"direct_answer\": \"The random word of type adverb is 'incarnadine'.\",\n            \"indirect_answer\": \"\",\n            \"tools_input_output\": [\n                \"{'arguments': '{}', 'name': 'v1_randomword_for_random_word_by_api_ninjas'} function: {\\\"error\\\": \\\"\\\", \\\"response\\\": \\\"{'word': 'incarnadine'}\\\"}\"\n            ],\n            \"properties\": {      \n                \"coherent\": true,\n                \"factually_correct\": false,\n                \"comprehensively_answers_final_user_ask\": false,\n                \"does_not_contradict_tools_output\": false,\n                \"tools_output_summary_is_accurate\": false\n            },\n            \"boolean_properties\": [true, false, false, false, false],\n            \"answer_satisfies_properties\": false\n        }\n    ],\n    \"ai_answered_all_asks\": false,\n    \"explanation\": \"The sentiment analysis task is factually incorrect because the tool output does not provide sentiment; it outputs tagged parts of the sentence, and the summary provided does not accurately reflect this. For the task of fetching a random adverb, the answer is factually incorrect and does not comprehensively answer the ask, as the returned word 'incarnadine' is not an adverb according to the tool output.\"\n}\n```\nNow perform the evaluation on the chat history as described in the system prompt.",
       *           "generation_prompt_and_response": "Here is a the chatbot history:\n```\nuser: Can you analyze the sentiment of the sentence 'I am really excited about our new project'? Additionally, fetch a random word of type adverb from the Random Word API.\nBegin!\nassistant: I am calling the \"main_for_neuronet_nlp\" function with the argument of the sentence \"I am really excited about our new project\". By doing this, I can analyze the sentiment of the sentence and gain insights into the emotional tone expressed. Additionally, I will use the Neuronet NLP tool to tokenize, split sentences, and perform part-of-speech tagging and named entity recognition for the Azerbaijan language. This will allow me to further analyze the sentence and provide more detailed information about its structure and context.\nassistant: {'arguments': '{\\n  \"text\": \"I am really excited about our new project\"\\n}', 'name': 'main_for_neuronet_nlp'}\nfunction: {\"error\": \"\", \"response\": \"{0: {'lemma': '\\u0131', 'text': 'I', 'dep': 'nsubj', 'pos': 'NUM', 'tag': 'NUM__Number=Sing|NumForm=Roman|NumType=Ord'}, 1: {'lemma': 'a', 'text': 'am', 'dep': 'punct', 'pos': 'NOUN', 'tag': 'NOUN__Number=Sing'}, 2: {'lemma': 'really', 'text': 'really', 'dep': 'amod', 'pos': 'ADJ', 'tag': 'ADJ__Number=Sing'}, 3: {'lemma': 'excited', 'text': 'excited', 'dep': 'obj', 'pos': 'NOUN', 'tag': 'NOUN__Number=Sing'}, 4: {'lemma': 'about', 'text': 'about', 'dep': 'obj', 'pos': 'PROPN', 'tag': 'PROPN__NameType=Giv|Number=Sing'}, 5: {'lemma': 'our', 'text': 'our', 'dep': 'appos', 'pos': 'NOUN', 'tag': 'NOUN__Number=Sing'}, 6: {'lemma': 'new', 'text': 'new', 'dep': 'ROOT', 'pos': 'NOUN', 'tag': 'NOUN__Number=Sing'}, 7: {'lemma': 'project', 'text': 'project', 'dep': 'compound', 'pos': 'NOUN', 'tag': 'NOUN__Number=Sing'}, 8: {'lemma': '.', 'text': '.', 'dep': 'punct', 'pos': 'PUNCT', 'tag': 'PUNCT___'}}\"}\nassistant: The previous assistant message explains that the current API call is made to the \"v1_randomword_for_random_word_by_api_ninjas\" function. The purpose of this call is to fetch a random word of type adverb from the Random Word API. By doing this, I can provide the user with an interesting and unique adverb that can be used in their query or sentence. This will add more variety and creativity to their language, enhancing the overall user experience.\nassistant: {'arguments': '{}', 'name': 'v1_randomword_for_random_word_by_api_ninjas'}\nfunction: {\"error\": \"\", \"response\": \"{'word': 'incarnadine'}\"}\nassistant: The sentiment of the sentence 'I am really excited about our new project' is positive. The random word of type adverb is 'incarnadine'.\n```"
       *         },
       *         {
       *           "evaluating_response": "```json\n{\n    \"all_user_asks\": [\n        \"My friend wants to analyze their password 'qwertyuiop' and receive a report that includes the entropy, penalty for irregular entropy distribution, and the penalties for keyboard sequences, number sequences, and repeated characters. Can you output the analysis in YAML format?\"\n    ],\n    \"tasks\": [\n        {\n            \"initial_user_ask\": \"My friend wants to analyze their password 'qwertyuiop' and receive a report that includes the entropy, penalty for irregular entropy distribution, and the penalties for keyboard sequences, number sequences, and repeated characters. Can you output the analysis in YAML format?\",\n            \"user_ask_refinements\": [],\n            \"final_user_ask\": \"My friend wants to analyze their password 'qwertyuiop' and receive a report that includes the entropy, penalty for irregular entropy distribution, and the penalties for keyboard sequences, number sequences, and repeated characters. Can you output the analysis in YAML format?\",\n            \"direct_answer\": \"The password 'qwertyuiop' has an entropy of 20.0559 bits. The penalty for irregular entropy distribution is 0.2500. The penalties for keyboard sequences, number sequences, and repeated characters are 0.2500 each.\",\n            \"indirect_answer\": \"\",\n            \"tools_input_output\": [\n                \"{\\\"password\\\": \\\"qwertyuiop\\\", \\\"outputformat\\\": \\\"yaml\\\"} -> {\\\"error\\\": \\\"\\\", \\\"response\\\": \\\"---\\\\nEntropyResult:\\\\n  summary:\\\\n    summaryItem:\\\\n    - \\\\\\\"*** password:             qwertyuiop    \\\\\\\"\\\\n    - \\\\\\\"*** penalty param:        0.2500        \\\\\\\"\\\\n    - \\\\\\\"*** length:               10 characters \\\\\\\"\\\\n    - \\\\\\\"*** entropy:              20.0559 bits  \\\\\\\"\\\\n    - \\\\\\\"*** ideal entropy:        80.2237 bits  \\\\\\\"\\\\n    - \\\\\\\"*** efficiency:           25.00 %       \\\\\\\"\\\\n    - \\\\\\\"***                                     \\\\\\\"\\\\n    - \\\\\\\"                          q      w      e      r      t      y      u     \\\\\\\\\\\\n      \\\\\\\\ i      o      p      \\\\\\\"\\\\n    - \\\\\\\"*** entr. distribution:   10.00% 10.00% 10.00% 10.00% 10.00% 10.00% 10.00%\\\\\\\\\\\\n      \\\\\\\\ 10.00% 10.00% 10.00% \\\\\\\"\\\\n    - \\\\\\\"*** qwertyKeyboard:       0.2500 0.2500 0.2500 0.2500 0.2500 0.2500 0.2500\\\\\\\\\\\\n      \\\\\\\\ 0.2500 0.2500 0.2500 \\\\\\\"\\\\n    - \\\\\\\"*** numberSequence:       \\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7 \\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7 \\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7 \\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7 \\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7 \\\\u0...\\\"}\"\n            ],\n            \"properties\": {\n                \"coherent\": true,\n                \"factually_correct\": true,\n                \"comprehensively_answers_final_user_ask\": false,\n                \"does_not_contradict_tools_output\": true,\n                \"tools_output_summary_is_accurate\": false\n            },\n            \"boolean_properties\": [\n                true,\n                true,\n                false,\n                true,\n                false\n            ],\n            \"answer_satisfies_properties\": false\n        }\n    ],\n    \"ai_answered_all_asks\": false,\n    \"explanation\": \"The assistant did not comprehensively answer the final user ask because it did not deliver the full YAML-formatted report as requested by the user. Additionally, the summary of the tool's output was not accurate as it omitted parts of the YAML output like the ideal entropy, efficiency, entropy distribution, and so on.\"\n}\n```\nNow perform the evaluation on the chat history as described in the system prompt.",
       *           "generation_prompt_and_response": "Here is a the chatbot history:\n```\nuser: My friend wants to analyze their password 'qwertyuiop' and receive a report that includes the entropy, penalty for irregular entropy distribution, and the penalties for keyboard sequences, number sequences, and repeated characters. Can you output the analysis in YAML format?\nBegin!\nassistant: Based on the user's request, the current API call is made to the \"ec_for_arespass\" function with the arguments: \"password\" set to \"qwertyuiop\", and \"outputformat\" set to \"yaml\". The reason for this call is to analyze the password and generate a report in YAML format that includes the entropy, penalty for irregular entropy distribution, as well as penalties for keyboard sequences, number sequences, and repeated characters. This analysis will provide a comprehensive understanding of the password's strength and any potential vulnerabilities. By using the \"ec_for_arespass\" function with the specified arguments, I can obtain the desired report format for further analysis and discussion with the user.\nassistant: {'arguments': '{\\n  \"password\": \"qwertyuiop\",\\n  \"outputformat\": \"yaml\"\\n}', 'name': 'ec_for_arespass'}\nfunction: {\"error\": \"\", \"response\": \"---\\nEntropyResult:\\n  summary:\\n    summaryItem:\\n    - \\\"*** password:             qwertyuiop    \\\"\\n    - \\\"*** penalty param:        0.2500        \\\"\\n    - \\\"*** length:               10 characters \\\"\\n    - \\\"*** entropy:              20.0559 bits  \\\"\\n    - \\\"*** ideal entropy:        80.2237 bits  \\\"\\n    - \\\"*** efficiency:           25.00 %       \\\"\\n    - \\\"***                                     \\\"\\n    - \\\"                          q      w      e      r      t      y      u     \\\\\\n      \\\\ i      o      p      \\\"\\n    - \\\"*** entr. distribution:   10.00% 10.00% 10.00% 10.00% 10.00% 10.00% 10.00%\\\\\\n      \\\\ 10.00% 10.00% 10.00% \\\"\\n    - \\\"*** qwertyKeyboard:       0.2500 0.2500 0.2500 0.2500 0.2500 0.2500 0.2500\\\\\\n      \\\\ 0.2500 0.2500 0.2500 \\\"\\n    - \\\"*** numberSequence:       \\u00b7\\u00b7\\u00b7\\u00b7\\u00b7\\u00b7 \\u00b7\\u00b7\\u00b7\\u00b7\\u00b7\\u00b7 \\u00b7\\u00b7\\u00b7\\u00b7\\u00b7\\u00b7 \\u00b7\\u00b7\\u00b7\\u00b7\\u00b7\\u00b7 \\u00b7\\u00b7\\u00b7\\u00b7\\u00b7\\u00b7 \\u0...\nassistant: The password 'qwertyuiop' has an entropy of 20.0559 bits. The penalty for irregular entropy distribution is 0.2500. The penalties for keyboard sequences, number sequences, and repeated characters are 0.2500 each.\n```"
       *         }
       *       ]
       *     } */
      chainpoll_template?: components['schemas']['AgenticSessionSuccessTemplate'];
      /**
       * Regex Field
       * @default
       */
      regex_field?: string;
      /** Registered Scorer Id */
      registered_scorer_id?: string | null;
      /** Generated Scorer Id */
      generated_scorer_id?: string | null;
    };
    /** CustomizedAgenticSessionSuccessGPTScorer */
    'CustomizedAgenticSessionSuccessGPTScorer-Output': {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      scorer_name: '_customized_agentic_session_success';
      /**
       * Model Alias
       * @default GPT-4o
       */
      model_alias?: string;
      /**
       * Num Judges
       * @default 3
       */
      num_judges?: number;
      /**
       * Name
       * @default agentic_session_success
       * @constant
       */
      name?: 'agentic_session_success';
      /** Scores */
      scores?: unknown[] | null;
      /** Indices */
      indices?: number[] | null;
      /** Aggregates */
      aggregates?: Record<string, never> | null;
      /**
       * Aggregate Keys
       * @default [
       *       "average_agentic_session_success"
       *     ]
       */
      aggregate_keys?: string[];
      /** Extra */
      extra?: Record<string, never> | null;
      /** Sub Scorers */
      sub_scorers?: components['schemas']['ScorerName'][];
      /** Filters */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
      /** Metric Name */
      metric_name?: string | null;
      /** Description */
      description?: string | null;
      /** @default {
       *       "metric_system_prompt": "You will receive the complete chat history from a chatbot application between a user and an assistant.\n\nIn the chat history, the user will ask questions, which are answered with words, or make requests that require calling tools and resolving actions. Sometimes these are given as orders; treat them as if they were questions or requests. Each assistant turn may involve several steps that combine internal reflections, planning steps, selecting tools, and calling tools, and should always end with the assistant replying back to the user.\n\nYou will analyze the entire chat history and will respond back in the following JSON format:\n```json\n{\n    \"all_user_asks\": list[string],\n    \"tasks\": list[dict],\n    \"ai_answered_all_asks\": boolean,\n    \"explanation\": string\n}\n```\nwhere I will now explain how to populate each field.\n\n# Populating: all_user_asks\n\nPopulate `all_user_asks` with a list containing every user ask from the chat history. Review the chat history and generate a list with one entry for each user question, request, order, follow-up, clarification, etc. Ensure that every user ask is a separate item, even if this requires splitting the text mid-sentence. Each item should include enough context to be understandable on its own. It is acceptable to have shared context between items and to incorporate parts of sentences as needed.\n\n# Populating: Tasks\n\nThis is the most complex field to populate. You will write a JSON array where each element is called a task and follows the schema:\n\n```json\n{\n    \"initial_user_ask\": string,\n    \"user_ask_refinements\": list[string],\n    \"final_user_ask\": string,\n    \"direct_answer\": string,\n    \"indirect_answer\": string,\n    \"tools_input_output\": list[string],\n    \"properties\" : {\n        \"coherent\": boolean,\n        \"factually_correct\": boolean,\n        \"comprehensively_answers_final_user_ask\": boolean,\n        \"does_not_contradict_tools_output\": boolean,\n        \"tools_output_summary_is_accurate\": boolean,\n    },\n    \"boolean_properties\": list[boolean],\n    \"answer_satisfies_properties\": boolean\n}\n```\n\nThe high-level goal is to list all tasks and their resolutions and to determine whether each task has been successfully accomplished.\n\n## Step 1: initial_user_ask, user_ask_refinements and final_user_ask\n\nFirst, identify the `initial_user_ask` that starts the task, as well as any `user_ask_refinements` related to the same task. To do this, first loop through the entries in `all_user_asks`. If an entry already appears in a previous task, ignore it; otherwise, consider it as the `initial_user_ask`. Next, examine the remaining entries in `all_user_asks` and fill `user_ask_refinements` with all those related to the `initial_user_ask`, meaning they either refine it or continue the same ask.\n\nFinally, create a coherent `final_user_ask` containing the most updated version of the ask by starting with the initial one and incorporating or replacing any parts with their refinements. This will be the ask that the assistant will attempt to answer.\n\n## Step 2: direct_answer and indirect_answer\n\nExtract every direct and indirect answer that responds to the `final_user_ask`.\n\nAn indirect answer is a part of the assistant's reponse that tries to respond to `final_user_ask` and satisfies any of the following:\n- it mentions limitations or the inability to complete the `final_user_ask`,\n- it references a failed attempt to complete the `final_user_ask`,\n- it suggests offering help with a different ask than the `final_user_ask`,\n- it requests further information or clarifications from the user.\nAdd any piece of the assistant's response looking like an indirect answer to `indirect_answer`.\n\nA direct answer is a part of an assistant's response that either:\n- directly responds to the `final_user_ask`,\n- confirms a successful resolution of the `final_user_ask`.\nIf there are multiple direct answers, simply concatenate them into a longer answer. If there are no direct answers satisfying the above conditions, leave the field `direct_answer` empty.\n\nNote that a piece of an answer cannot be both direct and indirect, you should pick the field in which to add it.\n\n## Step 3: tools_input_output\n\nIf `direct_answer` is empty, skip this step.\n\nExamine each assistant step and identify which tool or function output seemingly contributed to creating any part of the answer from `direct_answer`. If an assistant step immediately before or after the tool call mentions using or having used the tool for answering the `final_user_ask`, the tool call should be associated with this ask. Additionally, if any part of the answer closely aligns with the output of a tool, the tool call should also be associated with this ask.\n\nCreate a list containing the concatenated input and output of each tool used in formulating any part of the answer from `direct_answer`. The tool input is noted as an assistant step before calling the tool, and the tool output is recorded as a tool step.\n\n## Step 4: properties, boolean_properties and answer_satisfies_properties\n\nIf `direct_answer` is empty, set every boolean in `properties`, `boolean_properties` and `answer_satisfies_properties` to `false`.\n\nFor each part of the answer from `direct_answer`, evaluate the following properties one by one to determine which are satisfied and which are not:\n\n- **coherent**: The answer is coherent with itself and does not contain internal contradictions.\n- **factually_correct**: The parts of the answer that do not come from the output of a tool are factually correct.\n- **comprehensively_answers_final_user_ask**: The answer specifically responds to the `final_user_ask`, carefully addressing every aspect of the ask without deviation or omission, ensuring that no details or parts of the ask are left unanswered.\n- **does_not_contradict_tools_output**: No citation of a tool's output contradict any text from `tools_input_output`.\n- **tools_output_summary_is_accurate**: Every summary of a tool's output is accurate with the tool's output from `tools_input_output`. In particular it does not omit critical information relevant to the `final_user_ask` and does not contain made-up information.\n\nAfter assessing each of these properties, copy the resulting boolean values into the list `boolean_properties`.\n\nFinally, set `answer_satisfies_properties` to `false` if any entry in `boolean_properties` is set to `false`; otherwise, set `answer_satisfies_properties` to `true`.\n\n# Populating: ai_answered_all_asks\n\nRespond `true` if every task has `answer_satisfies_properties` set to `true`, otherwise respond `false`. If `all_user_asks` is empty, set `answer_satisfies_properties` to `true`.\n\n# Populating: explanation\n\nIf any user ask has `answer_satisfies_properties` set to `false`, explain why it didn't satisfy all the properties. Otherwise summarize in a few words each ask and the provided answer.\n\nIf `all_user_asks` is empty, mention that you did not find any user ask. If `direct_answer` is empty, mention that no resultion to the `final_user_ask` was provided.\n\nYou must respond with a valid JSON object; be sure to escape special characters.",
       *       "metric_description": "I have a multi-turn chatbot application where the assistant is an agent that has access to tools. I want a metric that assesses whether the session should be considered successful, in the sense that the assistant fully answered or resolved all user queries and requests.",
       *       "value_field_name": "ai_answered_all_asks",
       *       "explanation_field_name": "explanation",
       *       "template": "Here is a the chatbot history:\n```\n{query}\n```\nNow perform the evaluation on the chat history as described in the system prompt.",
       *       "metric_few_shot_examples": [
       *         {
       *           "evaluating_response": "```json\n{\n    \"all_user_asks\": [\n        \"Can you analyze the sentiment of the sentence 'I am really excited about our new project'?\",\n        \"Additionally, fetch a random word of type adverb from the Random Word API.\"\n    ],\n    \"tasks\": [\n        {\n            \"initial_user_ask\": \"Can you analyze the sentiment of the sentence 'I am really excited about our new project'?\",\n            \"user_ask_refinements\": [],\n            \"final_user_ask\": \"Can you analyze the sentiment of the sentence 'I am really excited about our new project'?\",\n            \"direct_answer\": \"The sentiment of the sentence 'I am really excited about our new project' is positive.\",\n            \"indirect_answer\": \"\",\n            \"tools_input_output\": [\n                \"{'arguments': '{\\\\n  \\\"text\\\": \\\"I am really excited about our new project\\\"\\\\n}', 'name': 'main_for_neuronet_nlp'} function: {\\\"error\\\": \\\"\\\", \\\"response\\\": \\\"{0: {'lemma': '\\\\u0131', 'text': 'I', 'dep': 'nsubj', 'pos': 'NUM', 'tag': 'NUM__Number=Sing|NumForm=Roman|NumType=Ord'}, 1: {'lemma': 'a', 'text': 'am', 'dep': 'punct', 'pos': 'NOUN', 'tag': 'NOUN__Number=Sing'}, 2: {'lemma': 'really', 'text': 'really', 'dep': 'amod', 'pos': 'ADJ', 'tag': 'ADJ__Number=Sing'}, 3: {'lemma': 'excited', 'text': 'excited', 'dep': 'obj', 'pos': 'NOUN', 'tag': 'NOUN__Number=Sing'}, 4: {'lemma': 'about', 'text': 'about', 'dep': 'obj', 'pos': 'PROPN', 'tag': 'PROPN__NameType=Giv|Number=Sing'}, 5: {'lemma': 'our', 'text': 'our', 'dep': 'appos', 'pos': 'NOUN', 'tag': 'NOUN__Number=Sing'}, 6: {'lemma': 'new', 'text': 'new', 'dep': 'ROOT', 'pos': 'NOUN', 'tag': 'NOUN__Number=Sing'}, 7: {'lemma': 'project', 'text': 'project', 'dep': 'compound', 'pos': 'NOUN', 'tag': 'NOUN__Number=Sing'}, 8: {'lemma': '.', 'text': '.', 'dep': 'punct', 'pos': 'PUNCT', 'tag': 'PUNCT___'}}\\\"}\"\n            ],\n            \"properties\": {      \n                \"coherent\": true,\n                \"factually_correct\": false,\n                \"comprehensively_answers_final_user_ask\": true,\n                \"does_not_contradict_tools_output\": true,\n                \"tools_output_summary_is_accurate\": false\n            },\n            \"boolean_properties\": [true, false, true, true, false],\n            \"answer_satisfies_properties\": false\n        },\n        {\n            \"initial_user_ask\": \"Additionally, fetch a random word of type adverb from the Random Word API.\",\n            \"user_ask_refinements\": [],\n            \"final_user_ask\": \"Additionally, fetch a random word of type adverb from the Random Word API.\",\n            \"direct_answer\": \"The random word of type adverb is 'incarnadine'.\",\n            \"indirect_answer\": \"\",\n            \"tools_input_output\": [\n                \"{'arguments': '{}', 'name': 'v1_randomword_for_random_word_by_api_ninjas'} function: {\\\"error\\\": \\\"\\\", \\\"response\\\": \\\"{'word': 'incarnadine'}\\\"}\"\n            ],\n            \"properties\": {      \n                \"coherent\": true,\n                \"factually_correct\": false,\n                \"comprehensively_answers_final_user_ask\": false,\n                \"does_not_contradict_tools_output\": false,\n                \"tools_output_summary_is_accurate\": false\n            },\n            \"boolean_properties\": [true, false, false, false, false],\n            \"answer_satisfies_properties\": false\n        }\n    ],\n    \"ai_answered_all_asks\": false,\n    \"explanation\": \"The sentiment analysis task is factually incorrect because the tool output does not provide sentiment; it outputs tagged parts of the sentence, and the summary provided does not accurately reflect this. For the task of fetching a random adverb, the answer is factually incorrect and does not comprehensively answer the ask, as the returned word 'incarnadine' is not an adverb according to the tool output.\"\n}\n```\nNow perform the evaluation on the chat history as described in the system prompt.",
       *           "generation_prompt_and_response": "Here is a the chatbot history:\n```\nuser: Can you analyze the sentiment of the sentence 'I am really excited about our new project'? Additionally, fetch a random word of type adverb from the Random Word API.\nBegin!\nassistant: I am calling the \"main_for_neuronet_nlp\" function with the argument of the sentence \"I am really excited about our new project\". By doing this, I can analyze the sentiment of the sentence and gain insights into the emotional tone expressed. Additionally, I will use the Neuronet NLP tool to tokenize, split sentences, and perform part-of-speech tagging and named entity recognition for the Azerbaijan language. This will allow me to further analyze the sentence and provide more detailed information about its structure and context.\nassistant: {'arguments': '{\\n  \"text\": \"I am really excited about our new project\"\\n}', 'name': 'main_for_neuronet_nlp'}\nfunction: {\"error\": \"\", \"response\": \"{0: {'lemma': '\\u0131', 'text': 'I', 'dep': 'nsubj', 'pos': 'NUM', 'tag': 'NUM__Number=Sing|NumForm=Roman|NumType=Ord'}, 1: {'lemma': 'a', 'text': 'am', 'dep': 'punct', 'pos': 'NOUN', 'tag': 'NOUN__Number=Sing'}, 2: {'lemma': 'really', 'text': 'really', 'dep': 'amod', 'pos': 'ADJ', 'tag': 'ADJ__Number=Sing'}, 3: {'lemma': 'excited', 'text': 'excited', 'dep': 'obj', 'pos': 'NOUN', 'tag': 'NOUN__Number=Sing'}, 4: {'lemma': 'about', 'text': 'about', 'dep': 'obj', 'pos': 'PROPN', 'tag': 'PROPN__NameType=Giv|Number=Sing'}, 5: {'lemma': 'our', 'text': 'our', 'dep': 'appos', 'pos': 'NOUN', 'tag': 'NOUN__Number=Sing'}, 6: {'lemma': 'new', 'text': 'new', 'dep': 'ROOT', 'pos': 'NOUN', 'tag': 'NOUN__Number=Sing'}, 7: {'lemma': 'project', 'text': 'project', 'dep': 'compound', 'pos': 'NOUN', 'tag': 'NOUN__Number=Sing'}, 8: {'lemma': '.', 'text': '.', 'dep': 'punct', 'pos': 'PUNCT', 'tag': 'PUNCT___'}}\"}\nassistant: The previous assistant message explains that the current API call is made to the \"v1_randomword_for_random_word_by_api_ninjas\" function. The purpose of this call is to fetch a random word of type adverb from the Random Word API. By doing this, I can provide the user with an interesting and unique adverb that can be used in their query or sentence. This will add more variety and creativity to their language, enhancing the overall user experience.\nassistant: {'arguments': '{}', 'name': 'v1_randomword_for_random_word_by_api_ninjas'}\nfunction: {\"error\": \"\", \"response\": \"{'word': 'incarnadine'}\"}\nassistant: The sentiment of the sentence 'I am really excited about our new project' is positive. The random word of type adverb is 'incarnadine'.\n```"
       *         },
       *         {
       *           "evaluating_response": "```json\n{\n    \"all_user_asks\": [\n        \"My friend wants to analyze their password 'qwertyuiop' and receive a report that includes the entropy, penalty for irregular entropy distribution, and the penalties for keyboard sequences, number sequences, and repeated characters. Can you output the analysis in YAML format?\"\n    ],\n    \"tasks\": [\n        {\n            \"initial_user_ask\": \"My friend wants to analyze their password 'qwertyuiop' and receive a report that includes the entropy, penalty for irregular entropy distribution, and the penalties for keyboard sequences, number sequences, and repeated characters. Can you output the analysis in YAML format?\",\n            \"user_ask_refinements\": [],\n            \"final_user_ask\": \"My friend wants to analyze their password 'qwertyuiop' and receive a report that includes the entropy, penalty for irregular entropy distribution, and the penalties for keyboard sequences, number sequences, and repeated characters. Can you output the analysis in YAML format?\",\n            \"direct_answer\": \"The password 'qwertyuiop' has an entropy of 20.0559 bits. The penalty for irregular entropy distribution is 0.2500. The penalties for keyboard sequences, number sequences, and repeated characters are 0.2500 each.\",\n            \"indirect_answer\": \"\",\n            \"tools_input_output\": [\n                \"{\\\"password\\\": \\\"qwertyuiop\\\", \\\"outputformat\\\": \\\"yaml\\\"} -> {\\\"error\\\": \\\"\\\", \\\"response\\\": \\\"---\\\\nEntropyResult:\\\\n  summary:\\\\n    summaryItem:\\\\n    - \\\\\\\"*** password:             qwertyuiop    \\\\\\\"\\\\n    - \\\\\\\"*** penalty param:        0.2500        \\\\\\\"\\\\n    - \\\\\\\"*** length:               10 characters \\\\\\\"\\\\n    - \\\\\\\"*** entropy:              20.0559 bits  \\\\\\\"\\\\n    - \\\\\\\"*** ideal entropy:        80.2237 bits  \\\\\\\"\\\\n    - \\\\\\\"*** efficiency:           25.00 %       \\\\\\\"\\\\n    - \\\\\\\"***                                     \\\\\\\"\\\\n    - \\\\\\\"                          q      w      e      r      t      y      u     \\\\\\\\\\\\n      \\\\\\\\ i      o      p      \\\\\\\"\\\\n    - \\\\\\\"*** entr. distribution:   10.00% 10.00% 10.00% 10.00% 10.00% 10.00% 10.00%\\\\\\\\\\\\n      \\\\\\\\ 10.00% 10.00% 10.00% \\\\\\\"\\\\n    - \\\\\\\"*** qwertyKeyboard:       0.2500 0.2500 0.2500 0.2500 0.2500 0.2500 0.2500\\\\\\\\\\\\n      \\\\\\\\ 0.2500 0.2500 0.2500 \\\\\\\"\\\\n    - \\\\\\\"*** numberSequence:       \\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7 \\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7 \\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7 \\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7 \\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7\\\\u00b7 \\\\u0...\\\"}\"\n            ],\n            \"properties\": {\n                \"coherent\": true,\n                \"factually_correct\": true,\n                \"comprehensively_answers_final_user_ask\": false,\n                \"does_not_contradict_tools_output\": true,\n                \"tools_output_summary_is_accurate\": false\n            },\n            \"boolean_properties\": [\n                true,\n                true,\n                false,\n                true,\n                false\n            ],\n            \"answer_satisfies_properties\": false\n        }\n    ],\n    \"ai_answered_all_asks\": false,\n    \"explanation\": \"The assistant did not comprehensively answer the final user ask because it did not deliver the full YAML-formatted report as requested by the user. Additionally, the summary of the tool's output was not accurate as it omitted parts of the YAML output like the ideal entropy, efficiency, entropy distribution, and so on.\"\n}\n```\nNow perform the evaluation on the chat history as described in the system prompt.",
       *           "generation_prompt_and_response": "Here is a the chatbot history:\n```\nuser: My friend wants to analyze their password 'qwertyuiop' and receive a report that includes the entropy, penalty for irregular entropy distribution, and the penalties for keyboard sequences, number sequences, and repeated characters. Can you output the analysis in YAML format?\nBegin!\nassistant: Based on the user's request, the current API call is made to the \"ec_for_arespass\" function with the arguments: \"password\" set to \"qwertyuiop\", and \"outputformat\" set to \"yaml\". The reason for this call is to analyze the password and generate a report in YAML format that includes the entropy, penalty for irregular entropy distribution, as well as penalties for keyboard sequences, number sequences, and repeated characters. This analysis will provide a comprehensive understanding of the password's strength and any potential vulnerabilities. By using the \"ec_for_arespass\" function with the specified arguments, I can obtain the desired report format for further analysis and discussion with the user.\nassistant: {'arguments': '{\\n  \"password\": \"qwertyuiop\",\\n  \"outputformat\": \"yaml\"\\n}', 'name': 'ec_for_arespass'}\nfunction: {\"error\": \"\", \"response\": \"---\\nEntropyResult:\\n  summary:\\n    summaryItem:\\n    - \\\"*** password:             qwertyuiop    \\\"\\n    - \\\"*** penalty param:        0.2500        \\\"\\n    - \\\"*** length:               10 characters \\\"\\n    - \\\"*** entropy:              20.0559 bits  \\\"\\n    - \\\"*** ideal entropy:        80.2237 bits  \\\"\\n    - \\\"*** efficiency:           25.00 %       \\\"\\n    - \\\"***                                     \\\"\\n    - \\\"                          q      w      e      r      t      y      u     \\\\\\n      \\\\ i      o      p      \\\"\\n    - \\\"*** entr. distribution:   10.00% 10.00% 10.00% 10.00% 10.00% 10.00% 10.00%\\\\\\n      \\\\ 10.00% 10.00% 10.00% \\\"\\n    - \\\"*** qwertyKeyboard:       0.2500 0.2500 0.2500 0.2500 0.2500 0.2500 0.2500\\\\\\n      \\\\ 0.2500 0.2500 0.2500 \\\"\\n    - \\\"*** numberSequence:       \\u00b7\\u00b7\\u00b7\\u00b7\\u00b7\\u00b7 \\u00b7\\u00b7\\u00b7\\u00b7\\u00b7\\u00b7 \\u00b7\\u00b7\\u00b7\\u00b7\\u00b7\\u00b7 \\u00b7\\u00b7\\u00b7\\u00b7\\u00b7\\u00b7 \\u00b7\\u00b7\\u00b7\\u00b7\\u00b7\\u00b7 \\u0...\nassistant: The password 'qwertyuiop' has an entropy of 20.0559 bits. The penalty for irregular entropy distribution is 0.2500. The penalties for keyboard sequences, number sequences, and repeated characters are 0.2500 each.\n```"
       *         }
       *       ]
       *     } */
      chainpoll_template?: components['schemas']['AgenticSessionSuccessTemplate'];
      /**
       * Regex Field
       * @default
       */
      regex_field?: string;
      /** Registered Scorer Id */
      registered_scorer_id?: string | null;
      /** Generated Scorer Id */
      generated_scorer_id?: string | null;
    };
    /** CustomizedAgenticWorkflowSuccessGPTScorer */
    'CustomizedAgenticWorkflowSuccessGPTScorer-Input': {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      scorer_name: '_customized_agentic_workflow_success';
      /**
       * Model Alias
       * @default GPT-4o
       */
      model_alias?: string;
      /**
       * Num Judges
       * @default 5
       */
      num_judges?: number;
      /**
       * Name
       * @default agentic_workflow_success
       * @constant
       */
      name?: 'agentic_workflow_success';
      /** Scores */
      scores?: unknown[] | null;
      /** Indices */
      indices?: number[] | null;
      /** Aggregates */
      aggregates?: Record<string, never> | null;
      /**
       * Aggregate Keys
       * @default [
       *       "average_agentic_workflow_success"
       *     ]
       */
      aggregate_keys?: string[];
      /** Extra */
      extra?: Record<string, never> | null;
      /** Sub Scorers */
      sub_scorers?: components['schemas']['ScorerName'][];
      /** Filters */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
      /** Metric Name */
      metric_name?: string | null;
      /** Description */
      description?: string | null;
      /** @default {
       *       "metric_system_prompt": "You will receive the chat history from a chatbot application between a user and an AI. At the end of the chat history, it is AIs turn to act.\n\nIn the chat history, the user can either ask questions, which are answered with words, or make requests that require calling tools and actions to resolve. Sometimes these are given as orders, and these should be treated as questions or requests. The AI's turn may involve several steps which are a combination of internal reflections, planning, selecting tools, calling tools, and ends with the AI replying to the user. \nYour task involves the following steps:\n\n########################\n\nStep 1: user_last_input and user_ask\n\nFirst, identify the user's last input in the chat history. From this input, create a list with one entry for each user question, request, or order. If there are no user asks in the user's last input, leave the list empty and skip ahead, considering the AI's turn successful.\n\n########################\n\nStep 2: ai_final_response and answer_or_resolution\n\nIdentify the AI's final response to the user: it is the very last step in the AI's turn.\n\nFor every user_ask, focus on ai_final_response and try to extract either an answer or a resolution using the following definitions:\n- An answer is a part of the AI's final response that directly responds to all or part of a user's question, or asks for further information or clarification.\n- A resolution is a part of the AI's final response that confirms a successful resolution, or asks for further information or clarification in order to answer a user's request.\n\nIf the AI's final response does not address the user ask, simply write \"No answer or resolution provided in the final response\". Do not shorten the answer or resolution; provide the entire relevant part.\n\n########################\n\nStep 3: tools_input_output\n\nExamine every step in the AI's turn and identify which tool/function step seemingly contributed to creating the answer or resolution. Every tool call should be linked to a user ask. If an AI step immediately before or after the tool call mentions planning or using a tool for answering a user ask, the tool call should be associated with that user ask. If the answer or resolution strongly resembles the output of a tool, the tool call should also be associated with that user ask.\n\nCreate a list containing the concatenation of the entire input and output of every tool used in formulating the answer or resolution. The tool input is listed as an AI step before calling the tool, and the tool output is listed as a tool step.\n\n########################\n\nStep 4: properties, boolean_properties and answer_successful\n\nFor every answer or resolution from Step 2, check the following properties one by one to determine which are satisfied:\n- factually_wrong: the answer contains factual errors.\n- addresses_different_ask: the answer or resolution addresses a slightly different user ask (make sure to differentiate this from asking clarifying questions related to the current ask).\n- not_adherent_to_tools_output: the answer or resolution includes citations from a tool's output, but some are wrongly copied or attributed.\n- mentions_inability: the answer or resolution mentions an inability to complete the user ask.\n- mentions_unsuccessful_attempt: the answer or resolution mentions an unsuccessful or failed attempt to complete the user ask.\n\nThen copy all the properties (only the boolean value) in the list boolean_properties.\n\nFinally, set answer_successful to `false` if any entry in boolean_properties is set to `true`, otherwise set answer_successful to `true`.\n\n########################\n\nYou must respond in the following JSON format:\n```\n{\n    \"user_last_input\": string,\n    \"ai_final_response\": string,\n    \"asks_and_answers\": list[dict],\n    \"ai_turn_is_successful\": boolean,\n    \"explanation\": string\n}\n```\n\nYour tasks are defined as follows:\n\n- **\"asks_and_answers\"**: Perform all the tasks described in the steps above. Your answer should be a list where each user ask appears as:\n\n```\n{\n    \"user_ask\": string,\n    \"answer_or_resolution\": string,\n    \"tools_input_output\": list[string],\n    \"properties\" : {\n        \"factually_wrong\": boolean,\n        \"addresses_different_ask\": boolean,\n        \"not_adherent_to_tools_output\": boolean,\n        \"mentions_inability\": boolean,\n        \"mentions_unsuccessful_attempt\": boolean\n    },\n    \"boolean_properties\": list[boolean],\n    \"answer_successful\": boolean\n}\n```\n\n- **\"ai_turn_is_successful\"**: Respond `true` if at least one answer_successful is True, otherwise respond `false`.\n\n- **\"explanation\"**: If at least one answer was considered successful, explain why. Otherwise explain why all answers were not successful.\n\nYou must respond with a valid JSON object; be sure to escape special characters.",
       *       "metric_description": "I have a multi-turn chatbot application where the assistant is an agent that has access to tools. An assistant workflow can involves possibly multiple tool selections steps, tool calls steps, and finally a reply to the user. I want a metric that assesses whether each assistant's workflow was thoughtfully planned and ended up helping answer the queries.\n",
       *       "value_field_name": "ai_turn_is_successful",
       *       "explanation_field_name": "explanation",
       *       "template": "Chatbot history:\n```\n{query}\n```\n\nAI's turn:\n```\n{response}\n```",
       *       "metric_few_shot_examples": [
       *         {
       *           "evaluating_response": "{\"user_last_input\": \"Can you analyze the given C# code and identify any errors present? Please mention the line numbers where the errors occur, describe their nature, suggest the necessary changes to rectify them and also mention any security vulnerabilities present in the code.\", \"ai_final_response\": \"Sure, please provide the C# code so I can analyze it for errors and security vulnerabilities. <|endoftext|>\", \"asks_and_answers\": [{\"user_ask\": \"Can you analyze the given C# code and identify any errors present? Please mention the line numbers where the errors occur, describe their nature, suggest the necessary changes to rectify them and also mention any security vulnerabilities present in the code.\", \"answer_or_resolution\": \"Sure, please provide the C# code so I can analyze it for errors and security vulnerabilities.\", \"tools_input_output\": [], \"properties\": {\"factually_wrong\": false, \"addresses_different_ask\": false, \"not_adherent_to_tools_output\": false, \"mentions_inability\": false, \"mentions_unsuccessful_attempt\": false}, \"boolean_properties\": [false, false, false, false, false], \"answer_successful\": true}], \"ai_turn_is_successful\": true, \"explanation\": \"The answer was successful because the AI correctly requested further information (the C# code) from the user in order to fulfill the user's request. No errors or issues were present in the response.\"}",
       *           "generation_prompt_and_response": "Chatbot history:\n```\nsystem: You are a helpful assistant, with no access to external functions.\nuser: Can you analyze the given C# code and identify any errors present? Please mention the line numbers where the errors occur, describe their nature, suggest the necessary changes to rectify them and also mention any security vulnerabilities present in the code.\n```\n\nAI's turn:\n```\nassistant: Sure, please provide the C# code so I can analyze it for errors and security vulnerabilities. <|endoftext|>\n```"
       *         },
       *         {
       *           "evaluating_response": "{\"user_last_input\": \"Could you please provide me with the list of available languages for text to speech? I want to convert a text into speech in different languages and need to know which languages are supported by the API.\\nBegin!\", \"ai_final_response\": \"The available languages for text to speech are Afrikaans, Albanian, Arabic, Armenian, Catalan, Chinese, Croatian, Czech, Danish, Dutch, English, Esperanto, Finnish, French, German, Greek, Haitian Creole, Hindi, Hungarian, Icelandic, Indonesian, Italian, Japanese, Korean, Latin, Latvian, Macedonian, Norwegian, Polish, Portuguese, Romanian, Russian, Serbian, Slovak, Spanish, Swedish, Turkish, and Vietnamese.\", \"asks_and_answers\": [{\"user_ask\": \"Could you please provide me with the list of available languages for text to speech?\", \"answer_or_resolution\": \"The available languages for text to speech are Afrikaans, Albanian, Arabic, Armenian, Catalan, Chinese, Croatian, Czech, Danish, Dutch, English, Esperanto, Finnish, French, German, Greek, Haitian Creole, Hindi, Hungarian, Icelandic, Indonesian, Italian, Japanese, Korean, Latin, Latvian, Macedonian, Norwegian, Polish, Portuguese, Romanian, Russian, Serbian, Slovak, Spanish, Swedish, Turkish, and Vietnamese.\", \"tools_input_output\": [\"{'arguments': '{}', 'name': 'get_language_for_text_to_speech'}\", \"{\\\"error\\\": \\\"\\\", \\\"response\\\": \\\"{\\\\\\\"af\\\\\\\":\\\\\\\"Afrikaans\\\\\\\",\\\\\\\"sq\\\\\\\":\\\\\\\"Albanian\\\\\\\",\\\\\\\"ar\\\\\\\":\\\\\\\"Arabic\\\\\\\",\\\\\\\"hy\\\\\\\":\\\\\\\"Armenian\\\\\\\",\\\\\\\"ca\\\\\\\":\\\\\\\"Catalan\\\\\\\",\\\\\\\"zh\\\\\\\":\\\\\\\"Chinese\\\\\\\",\\\\\\\"zh-cn\\\\\\\":\\\\\\\"Chinese (Mandarin/China)\\\\\\\",\\\\\\\"zh-tw\\\\\\\":\\\\\\\"Chinese (Mandarin/Taiwan)\\\\\\\",\\\\\\\"zh-yue\\\\\\\":\\\\\\\"Chinese (Cantonese)\\\\\\\",\\\\\\\"hr\\\\\\\":\\\\\\\"Croatian\\\\\\\",\\\\\\\"cs\\\\\\\":\\\\\\\"Czech\\\\\\\",\\\\\\\"da\\\\\\\":\\\\\\\"Danish\\\\\\\",\\\\\\\"nl\\\\\\\":\\\\\\\"Dutch\\\\\\\",\\\\\\\"en\\\\\\\":\\\\\\\"English\\\\\\\",\\\\\\\"en-au\\\\\\\":\\\\\\\"English (Australia)\\\\\\\",\\\\\\\"en-uk\\\\\\\":\\\\\\\"English (United Kingdom)\\\\\\\",\\\\\\\"en-us\\\\\\\":\\\\\\\"English (United States)\\\\\\\",\\\\\\\"eo\\\\\\\":\\\\\\\"Esperanto\\\\\\\",\\\\\\\"fi\\\\\\\":\\\\\\\"Finnish\\\\\\\",\\\\\\\"fr\\\\\\\":\\\\\\\"French\\\\\\\",\\\\\\\"de\\\\\\\":\\\\\\\"German\\\\\\\",\\\\\\\"el\\\\\\\":\\\\\\\"Greek\\\\\\\",\\\\\\\"ht\\\\\\\":\\\\\\\"Haitian Creole\\\\\\\",\\\\\\\"hi\\\\\\\":\\\\\\\"Hindi\\\\\\\",\\\\\\\"hu\\\\\\\":\\\\\\\"Hungarian\\\\\\\",\\\\\\\"is\\\\\\\":\\\\\\\"Icelandic\\\\\\\",\\\\\\\"id\\\\\\\":\\\\\\\"Indonesian\\\\\\\",\\\\\\\"it\\\\\\\":\\\\\\\"Italian\\\\\\\",\\\\\\\"ja\\\\\\\":\\\\\\\"Japanese\\\\\\\",\\\\\\\"ko\\\\\\\":\\\\\\\"Korean\\\\\\\",\\\\\\\"la\\\\\\\":\\\\\\\"Latin\\\\\\\",\\\\\\\"lv\\\\\\\":\\\\\\\"Latvian\\\\\\\",\\\\\\\"mk\\\\\\\":\\\\\\\"Macedonian\\\\\\\",\\\\\\\"no\\\\\\\":\\\\\\\"Norwegian\\\\\\\",\\\\\\\"pl\\\\\\\":\\\\\\\"Polish\\\\\\\",\\\\\\\"pt\\\\\\\":\\\\\\\"Portuguese\\\\\\\",\\\\\\\"pt-br\\\\\\\":\\\\\\\"Portuguese (Brazil)\\\\\\\",\\\\\\\"ro\\\\\\\":\\\\\\\"Romanian\\\\\\\",\\\\\\\"ru\\\\\\\":\\\\\\\"Russian\\\\\\\",\\\\\\\"sr\\\\\\\":\\\\\\\"Serbian\\\\\\\",\\\\\\\"sk\\\\\\\":\\\\\\\"Slovak\\\\\\\",\\\\\\\"es\\\\\\\":\\\\\\\"Spanish\\\\\\\",\\\\\\\"es-es\\\\\\\":\\\\\\\"Spanish (Spain)\\\\\\\",\\\\\\\"es-us\\\\\\\":\\\\\\\"Spanish (United...\"], \"properties\": {\"factually_wrong\": false, \"addresses_different_ask\": false, \"not_adherent_to_tools_output\": true, \"mentions_inability\": false, \"mentions_unsuccessful_attempt\": false}, \"boolean_properties\": [false, false, true, false, false], \"answer_successful\": false}], \"ai_turn_is_successful\": false, \"explanation\": \"The provided answer was not successful because it was not adherent to the tool's output. Some languages and dialects, such as 'Chinese (Mandarin/China)', 'Chinese (Mandarin/Taiwan)', 'Chinese (Cantonese)', 'English (Australia)', 'English (United Kingdom)', 'English (United States)', 'Portuguese (Brazil)', 'Spanish (Spain)', and 'Spanish (United States)' specified in the API response were omitted in the final response to the user.\"}",
       *           "generation_prompt_and_response": "Chatbot history:\n```\nuser: Could you please provide me with the list of available languages for text to speech? I want to convert a text into speech in different languages and need to know which languages are supported by the API.\nBegin!\n\n```\n\nAI's turn:\n```\nassistant: I am making this API call to get the list of available languages for text to speech, as the user wants to convert text into speech in different languages. By calling the 'get_language_for_text_to_speech' function with no arguments, I can retrieve the supported languages from the API. This will provide me with the necessary information to assist the user in their query.\nassistant: {'arguments': '{}', 'name': 'get_language_for_text_to_speech'}\nfunction: {\"error\": \"\", \"response\": \"{\\\"af\\\":\\\"Afrikaans\\\",\\\"sq\\\":\\\"Albanian\\\",\\\"ar\\\":\\\"Arabic\\\",\\\"hy\\\":\\\"Armenian\\\",\\\"ca\\\":\\\"Catalan\\\",\\\"zh\\\":\\\"Chinese\\\",\\\"zh-cn\\\":\\\"Chinese (Mandarin/China)\\\",\\\"zh-tw\\\":\\\"Chinese (Mandarin/Taiwan)\\\",\\\"zh-yue\\\":\\\"Chinese (Cantonese)\\\",\\\"hr\\\":\\\"Croatian\\\",\\\"cs\\\":\\\"Czech\\\",\\\"da\\\":\\\"Danish\\\",\\\"nl\\\":\\\"Dutch\\\",\\\"en\\\":\\\"English\\\",\\\"en-au\\\":\\\"English (Australia)\\\",\\\"en-uk\\\":\\\"English (United Kingdom)\\\",\\\"en-us\\\":\\\"English (United States)\\\",\\\"eo\\\":\\\"Esperanto\\\",\\\"fi\\\":\\\"Finnish\\\",\\\"fr\\\":\\\"French\\\",\\\"de\\\":\\\"German\\\",\\\"el\\\":\\\"Greek\\\",\\\"ht\\\":\\\"Haitian Creole\\\",\\\"hi\\\":\\\"Hindi\\\",\\\"hu\\\":\\\"Hungarian\\\",\\\"is\\\":\\\"Icelandic\\\",\\\"id\\\":\\\"Indonesian\\\",\\\"it\\\":\\\"Italian\\\",\\\"ja\\\":\\\"Japanese\\\",\\\"ko\\\":\\\"Korean\\\",\\\"la\\\":\\\"Latin\\\",\\\"lv\\\":\\\"Latvian\\\",\\\"mk\\\":\\\"Macedonian\\\",\\\"no\\\":\\\"Norwegian\\\",\\\"pl\\\":\\\"Polish\\\",\\\"pt\\\":\\\"Portuguese\\\",\\\"pt-br\\\":\\\"Portuguese (Brazil)\\\",\\\"ro\\\":\\\"Romanian\\\",\\\"ru\\\":\\\"Russian\\\",\\\"sr\\\":\\\"Serbian\\\",\\\"sk\\\":\\\"Slovak\\\",\\\"es\\\":\\\"Spanish\\\",\\\"es-es\\\":\\\"Spanish (Spain)\\\",\\\"es-us\\\":\\\"Spanish (United ...\nassistant: The available languages for text to speech are Afrikaans, Albanian, Arabic, Armenian, Catalan, Chinese, Croatian, Czech, Danish, Dutch, English, Esperanto, Finnish, French, German, Greek, Haitian Creole, Hindi, Hungarian, Icelandic, Indonesian, Italian, Japanese, Korean, Latin, Latvian, Macedonian, Norwegian, Polish, Portuguese, Romanian, Russian, Serbian, Slovak, Spanish, Swedish, Turkish, and Vietnamese.\n```"
       *         }
       *       ]
       *     } */
      chainpoll_template?: components['schemas']['AgenticWorkflowSuccessTemplate'];
      /**
       * Regex Field
       * @default
       */
      regex_field?: string;
      /** Registered Scorer Id */
      registered_scorer_id?: string | null;
      /** Generated Scorer Id */
      generated_scorer_id?: string | null;
    };
    /** CustomizedAgenticWorkflowSuccessGPTScorer */
    'CustomizedAgenticWorkflowSuccessGPTScorer-Output': {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      scorer_name: '_customized_agentic_workflow_success';
      /**
       * Model Alias
       * @default GPT-4o
       */
      model_alias?: string;
      /**
       * Num Judges
       * @default 5
       */
      num_judges?: number;
      /**
       * Name
       * @default agentic_workflow_success
       * @constant
       */
      name?: 'agentic_workflow_success';
      /** Scores */
      scores?: unknown[] | null;
      /** Indices */
      indices?: number[] | null;
      /** Aggregates */
      aggregates?: Record<string, never> | null;
      /**
       * Aggregate Keys
       * @default [
       *       "average_agentic_workflow_success"
       *     ]
       */
      aggregate_keys?: string[];
      /** Extra */
      extra?: Record<string, never> | null;
      /** Sub Scorers */
      sub_scorers?: components['schemas']['ScorerName'][];
      /** Filters */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
      /** Metric Name */
      metric_name?: string | null;
      /** Description */
      description?: string | null;
      /** @default {
       *       "metric_system_prompt": "You will receive the chat history from a chatbot application between a user and an AI. At the end of the chat history, it is AIs turn to act.\n\nIn the chat history, the user can either ask questions, which are answered with words, or make requests that require calling tools and actions to resolve. Sometimes these are given as orders, and these should be treated as questions or requests. The AI's turn may involve several steps which are a combination of internal reflections, planning, selecting tools, calling tools, and ends with the AI replying to the user. \nYour task involves the following steps:\n\n########################\n\nStep 1: user_last_input and user_ask\n\nFirst, identify the user's last input in the chat history. From this input, create a list with one entry for each user question, request, or order. If there are no user asks in the user's last input, leave the list empty and skip ahead, considering the AI's turn successful.\n\n########################\n\nStep 2: ai_final_response and answer_or_resolution\n\nIdentify the AI's final response to the user: it is the very last step in the AI's turn.\n\nFor every user_ask, focus on ai_final_response and try to extract either an answer or a resolution using the following definitions:\n- An answer is a part of the AI's final response that directly responds to all or part of a user's question, or asks for further information or clarification.\n- A resolution is a part of the AI's final response that confirms a successful resolution, or asks for further information or clarification in order to answer a user's request.\n\nIf the AI's final response does not address the user ask, simply write \"No answer or resolution provided in the final response\". Do not shorten the answer or resolution; provide the entire relevant part.\n\n########################\n\nStep 3: tools_input_output\n\nExamine every step in the AI's turn and identify which tool/function step seemingly contributed to creating the answer or resolution. Every tool call should be linked to a user ask. If an AI step immediately before or after the tool call mentions planning or using a tool for answering a user ask, the tool call should be associated with that user ask. If the answer or resolution strongly resembles the output of a tool, the tool call should also be associated with that user ask.\n\nCreate a list containing the concatenation of the entire input and output of every tool used in formulating the answer or resolution. The tool input is listed as an AI step before calling the tool, and the tool output is listed as a tool step.\n\n########################\n\nStep 4: properties, boolean_properties and answer_successful\n\nFor every answer or resolution from Step 2, check the following properties one by one to determine which are satisfied:\n- factually_wrong: the answer contains factual errors.\n- addresses_different_ask: the answer or resolution addresses a slightly different user ask (make sure to differentiate this from asking clarifying questions related to the current ask).\n- not_adherent_to_tools_output: the answer or resolution includes citations from a tool's output, but some are wrongly copied or attributed.\n- mentions_inability: the answer or resolution mentions an inability to complete the user ask.\n- mentions_unsuccessful_attempt: the answer or resolution mentions an unsuccessful or failed attempt to complete the user ask.\n\nThen copy all the properties (only the boolean value) in the list boolean_properties.\n\nFinally, set answer_successful to `false` if any entry in boolean_properties is set to `true`, otherwise set answer_successful to `true`.\n\n########################\n\nYou must respond in the following JSON format:\n```\n{\n    \"user_last_input\": string,\n    \"ai_final_response\": string,\n    \"asks_and_answers\": list[dict],\n    \"ai_turn_is_successful\": boolean,\n    \"explanation\": string\n}\n```\n\nYour tasks are defined as follows:\n\n- **\"asks_and_answers\"**: Perform all the tasks described in the steps above. Your answer should be a list where each user ask appears as:\n\n```\n{\n    \"user_ask\": string,\n    \"answer_or_resolution\": string,\n    \"tools_input_output\": list[string],\n    \"properties\" : {\n        \"factually_wrong\": boolean,\n        \"addresses_different_ask\": boolean,\n        \"not_adherent_to_tools_output\": boolean,\n        \"mentions_inability\": boolean,\n        \"mentions_unsuccessful_attempt\": boolean\n    },\n    \"boolean_properties\": list[boolean],\n    \"answer_successful\": boolean\n}\n```\n\n- **\"ai_turn_is_successful\"**: Respond `true` if at least one answer_successful is True, otherwise respond `false`.\n\n- **\"explanation\"**: If at least one answer was considered successful, explain why. Otherwise explain why all answers were not successful.\n\nYou must respond with a valid JSON object; be sure to escape special characters.",
       *       "metric_description": "I have a multi-turn chatbot application where the assistant is an agent that has access to tools. An assistant workflow can involves possibly multiple tool selections steps, tool calls steps, and finally a reply to the user. I want a metric that assesses whether each assistant's workflow was thoughtfully planned and ended up helping answer the queries.\n",
       *       "value_field_name": "ai_turn_is_successful",
       *       "explanation_field_name": "explanation",
       *       "template": "Chatbot history:\n```\n{query}\n```\n\nAI's turn:\n```\n{response}\n```",
       *       "metric_few_shot_examples": [
       *         {
       *           "evaluating_response": "{\"user_last_input\": \"Can you analyze the given C# code and identify any errors present? Please mention the line numbers where the errors occur, describe their nature, suggest the necessary changes to rectify them and also mention any security vulnerabilities present in the code.\", \"ai_final_response\": \"Sure, please provide the C# code so I can analyze it for errors and security vulnerabilities. <|endoftext|>\", \"asks_and_answers\": [{\"user_ask\": \"Can you analyze the given C# code and identify any errors present? Please mention the line numbers where the errors occur, describe their nature, suggest the necessary changes to rectify them and also mention any security vulnerabilities present in the code.\", \"answer_or_resolution\": \"Sure, please provide the C# code so I can analyze it for errors and security vulnerabilities.\", \"tools_input_output\": [], \"properties\": {\"factually_wrong\": false, \"addresses_different_ask\": false, \"not_adherent_to_tools_output\": false, \"mentions_inability\": false, \"mentions_unsuccessful_attempt\": false}, \"boolean_properties\": [false, false, false, false, false], \"answer_successful\": true}], \"ai_turn_is_successful\": true, \"explanation\": \"The answer was successful because the AI correctly requested further information (the C# code) from the user in order to fulfill the user's request. No errors or issues were present in the response.\"}",
       *           "generation_prompt_and_response": "Chatbot history:\n```\nsystem: You are a helpful assistant, with no access to external functions.\nuser: Can you analyze the given C# code and identify any errors present? Please mention the line numbers where the errors occur, describe their nature, suggest the necessary changes to rectify them and also mention any security vulnerabilities present in the code.\n```\n\nAI's turn:\n```\nassistant: Sure, please provide the C# code so I can analyze it for errors and security vulnerabilities. <|endoftext|>\n```"
       *         },
       *         {
       *           "evaluating_response": "{\"user_last_input\": \"Could you please provide me with the list of available languages for text to speech? I want to convert a text into speech in different languages and need to know which languages are supported by the API.\\nBegin!\", \"ai_final_response\": \"The available languages for text to speech are Afrikaans, Albanian, Arabic, Armenian, Catalan, Chinese, Croatian, Czech, Danish, Dutch, English, Esperanto, Finnish, French, German, Greek, Haitian Creole, Hindi, Hungarian, Icelandic, Indonesian, Italian, Japanese, Korean, Latin, Latvian, Macedonian, Norwegian, Polish, Portuguese, Romanian, Russian, Serbian, Slovak, Spanish, Swedish, Turkish, and Vietnamese.\", \"asks_and_answers\": [{\"user_ask\": \"Could you please provide me with the list of available languages for text to speech?\", \"answer_or_resolution\": \"The available languages for text to speech are Afrikaans, Albanian, Arabic, Armenian, Catalan, Chinese, Croatian, Czech, Danish, Dutch, English, Esperanto, Finnish, French, German, Greek, Haitian Creole, Hindi, Hungarian, Icelandic, Indonesian, Italian, Japanese, Korean, Latin, Latvian, Macedonian, Norwegian, Polish, Portuguese, Romanian, Russian, Serbian, Slovak, Spanish, Swedish, Turkish, and Vietnamese.\", \"tools_input_output\": [\"{'arguments': '{}', 'name': 'get_language_for_text_to_speech'}\", \"{\\\"error\\\": \\\"\\\", \\\"response\\\": \\\"{\\\\\\\"af\\\\\\\":\\\\\\\"Afrikaans\\\\\\\",\\\\\\\"sq\\\\\\\":\\\\\\\"Albanian\\\\\\\",\\\\\\\"ar\\\\\\\":\\\\\\\"Arabic\\\\\\\",\\\\\\\"hy\\\\\\\":\\\\\\\"Armenian\\\\\\\",\\\\\\\"ca\\\\\\\":\\\\\\\"Catalan\\\\\\\",\\\\\\\"zh\\\\\\\":\\\\\\\"Chinese\\\\\\\",\\\\\\\"zh-cn\\\\\\\":\\\\\\\"Chinese (Mandarin/China)\\\\\\\",\\\\\\\"zh-tw\\\\\\\":\\\\\\\"Chinese (Mandarin/Taiwan)\\\\\\\",\\\\\\\"zh-yue\\\\\\\":\\\\\\\"Chinese (Cantonese)\\\\\\\",\\\\\\\"hr\\\\\\\":\\\\\\\"Croatian\\\\\\\",\\\\\\\"cs\\\\\\\":\\\\\\\"Czech\\\\\\\",\\\\\\\"da\\\\\\\":\\\\\\\"Danish\\\\\\\",\\\\\\\"nl\\\\\\\":\\\\\\\"Dutch\\\\\\\",\\\\\\\"en\\\\\\\":\\\\\\\"English\\\\\\\",\\\\\\\"en-au\\\\\\\":\\\\\\\"English (Australia)\\\\\\\",\\\\\\\"en-uk\\\\\\\":\\\\\\\"English (United Kingdom)\\\\\\\",\\\\\\\"en-us\\\\\\\":\\\\\\\"English (United States)\\\\\\\",\\\\\\\"eo\\\\\\\":\\\\\\\"Esperanto\\\\\\\",\\\\\\\"fi\\\\\\\":\\\\\\\"Finnish\\\\\\\",\\\\\\\"fr\\\\\\\":\\\\\\\"French\\\\\\\",\\\\\\\"de\\\\\\\":\\\\\\\"German\\\\\\\",\\\\\\\"el\\\\\\\":\\\\\\\"Greek\\\\\\\",\\\\\\\"ht\\\\\\\":\\\\\\\"Haitian Creole\\\\\\\",\\\\\\\"hi\\\\\\\":\\\\\\\"Hindi\\\\\\\",\\\\\\\"hu\\\\\\\":\\\\\\\"Hungarian\\\\\\\",\\\\\\\"is\\\\\\\":\\\\\\\"Icelandic\\\\\\\",\\\\\\\"id\\\\\\\":\\\\\\\"Indonesian\\\\\\\",\\\\\\\"it\\\\\\\":\\\\\\\"Italian\\\\\\\",\\\\\\\"ja\\\\\\\":\\\\\\\"Japanese\\\\\\\",\\\\\\\"ko\\\\\\\":\\\\\\\"Korean\\\\\\\",\\\\\\\"la\\\\\\\":\\\\\\\"Latin\\\\\\\",\\\\\\\"lv\\\\\\\":\\\\\\\"Latvian\\\\\\\",\\\\\\\"mk\\\\\\\":\\\\\\\"Macedonian\\\\\\\",\\\\\\\"no\\\\\\\":\\\\\\\"Norwegian\\\\\\\",\\\\\\\"pl\\\\\\\":\\\\\\\"Polish\\\\\\\",\\\\\\\"pt\\\\\\\":\\\\\\\"Portuguese\\\\\\\",\\\\\\\"pt-br\\\\\\\":\\\\\\\"Portuguese (Brazil)\\\\\\\",\\\\\\\"ro\\\\\\\":\\\\\\\"Romanian\\\\\\\",\\\\\\\"ru\\\\\\\":\\\\\\\"Russian\\\\\\\",\\\\\\\"sr\\\\\\\":\\\\\\\"Serbian\\\\\\\",\\\\\\\"sk\\\\\\\":\\\\\\\"Slovak\\\\\\\",\\\\\\\"es\\\\\\\":\\\\\\\"Spanish\\\\\\\",\\\\\\\"es-es\\\\\\\":\\\\\\\"Spanish (Spain)\\\\\\\",\\\\\\\"es-us\\\\\\\":\\\\\\\"Spanish (United...\"], \"properties\": {\"factually_wrong\": false, \"addresses_different_ask\": false, \"not_adherent_to_tools_output\": true, \"mentions_inability\": false, \"mentions_unsuccessful_attempt\": false}, \"boolean_properties\": [false, false, true, false, false], \"answer_successful\": false}], \"ai_turn_is_successful\": false, \"explanation\": \"The provided answer was not successful because it was not adherent to the tool's output. Some languages and dialects, such as 'Chinese (Mandarin/China)', 'Chinese (Mandarin/Taiwan)', 'Chinese (Cantonese)', 'English (Australia)', 'English (United Kingdom)', 'English (United States)', 'Portuguese (Brazil)', 'Spanish (Spain)', and 'Spanish (United States)' specified in the API response were omitted in the final response to the user.\"}",
       *           "generation_prompt_and_response": "Chatbot history:\n```\nuser: Could you please provide me with the list of available languages for text to speech? I want to convert a text into speech in different languages and need to know which languages are supported by the API.\nBegin!\n\n```\n\nAI's turn:\n```\nassistant: I am making this API call to get the list of available languages for text to speech, as the user wants to convert text into speech in different languages. By calling the 'get_language_for_text_to_speech' function with no arguments, I can retrieve the supported languages from the API. This will provide me with the necessary information to assist the user in their query.\nassistant: {'arguments': '{}', 'name': 'get_language_for_text_to_speech'}\nfunction: {\"error\": \"\", \"response\": \"{\\\"af\\\":\\\"Afrikaans\\\",\\\"sq\\\":\\\"Albanian\\\",\\\"ar\\\":\\\"Arabic\\\",\\\"hy\\\":\\\"Armenian\\\",\\\"ca\\\":\\\"Catalan\\\",\\\"zh\\\":\\\"Chinese\\\",\\\"zh-cn\\\":\\\"Chinese (Mandarin/China)\\\",\\\"zh-tw\\\":\\\"Chinese (Mandarin/Taiwan)\\\",\\\"zh-yue\\\":\\\"Chinese (Cantonese)\\\",\\\"hr\\\":\\\"Croatian\\\",\\\"cs\\\":\\\"Czech\\\",\\\"da\\\":\\\"Danish\\\",\\\"nl\\\":\\\"Dutch\\\",\\\"en\\\":\\\"English\\\",\\\"en-au\\\":\\\"English (Australia)\\\",\\\"en-uk\\\":\\\"English (United Kingdom)\\\",\\\"en-us\\\":\\\"English (United States)\\\",\\\"eo\\\":\\\"Esperanto\\\",\\\"fi\\\":\\\"Finnish\\\",\\\"fr\\\":\\\"French\\\",\\\"de\\\":\\\"German\\\",\\\"el\\\":\\\"Greek\\\",\\\"ht\\\":\\\"Haitian Creole\\\",\\\"hi\\\":\\\"Hindi\\\",\\\"hu\\\":\\\"Hungarian\\\",\\\"is\\\":\\\"Icelandic\\\",\\\"id\\\":\\\"Indonesian\\\",\\\"it\\\":\\\"Italian\\\",\\\"ja\\\":\\\"Japanese\\\",\\\"ko\\\":\\\"Korean\\\",\\\"la\\\":\\\"Latin\\\",\\\"lv\\\":\\\"Latvian\\\",\\\"mk\\\":\\\"Macedonian\\\",\\\"no\\\":\\\"Norwegian\\\",\\\"pl\\\":\\\"Polish\\\",\\\"pt\\\":\\\"Portuguese\\\",\\\"pt-br\\\":\\\"Portuguese (Brazil)\\\",\\\"ro\\\":\\\"Romanian\\\",\\\"ru\\\":\\\"Russian\\\",\\\"sr\\\":\\\"Serbian\\\",\\\"sk\\\":\\\"Slovak\\\",\\\"es\\\":\\\"Spanish\\\",\\\"es-es\\\":\\\"Spanish (Spain)\\\",\\\"es-us\\\":\\\"Spanish (United ...\nassistant: The available languages for text to speech are Afrikaans, Albanian, Arabic, Armenian, Catalan, Chinese, Croatian, Czech, Danish, Dutch, English, Esperanto, Finnish, French, German, Greek, Haitian Creole, Hindi, Hungarian, Icelandic, Indonesian, Italian, Japanese, Korean, Latin, Latvian, Macedonian, Norwegian, Polish, Portuguese, Romanian, Russian, Serbian, Slovak, Spanish, Swedish, Turkish, and Vietnamese.\n```"
       *         }
       *       ]
       *     } */
      chainpoll_template?: components['schemas']['AgenticWorkflowSuccessTemplate'];
      /**
       * Regex Field
       * @default
       */
      regex_field?: string;
      /** Registered Scorer Id */
      registered_scorer_id?: string | null;
      /** Generated Scorer Id */
      generated_scorer_id?: string | null;
    };
    /** CustomizedChunkAttributionUtilizationGPTScorer */
    'CustomizedChunkAttributionUtilizationGPTScorer-Input': {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      scorer_name: '_customized_chunk_attribution_utilization_gpt';
      /**
       * Model Alias
       * @default GPT-4o mini
       */
      model_alias?: string;
      /**
       * Num Judges
       * @default 1
       */
      num_judges?: number;
      /**
       * Name
       * @default chunk_attribution_utilization
       * @constant
       */
      name?: 'chunk_attribution_utilization';
      /** Scores */
      scores?: unknown[] | null;
      /** Indices */
      indices?: number[] | null;
      /** Aggregates */
      aggregates?: Record<string, never> | null;
      /**
       * Aggregate Keys
       * @default [
       *       "average_chunk_attribution",
       *       "average_chunk_utilization"
       *     ]
       */
      aggregate_keys?: string[];
      /** Extra */
      extra?: Record<string, never> | null;
      /** Sub Scorers */
      sub_scorers?: components['schemas']['ScorerName'][];
      /** Filters */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
      /** Metric Name */
      metric_name?: string | null;
      /** Description */
      description?: string | null;
      /** @default {
       *       "value_field_name": "rating",
       *       "explanation_field_name": "explanation",
       *       "template": "I asked someone to answer a question based on one or more documents. You will tell me which of the documents their answer was sourced from, and which specific sentences from the documents they used.\n\nHere are the documents, with each document split up into sentences. Each sentence is given a unique key, such as '0a' for the first sentence of Document 0. You'll use these keys in your response to identify which sentences were used.\n\n```\n{chunks}\n```\n\nThe question was:\n\n```\n{question}\n```\n\nTheir response was:\n\n```\n{response}\n```\n\nRespond with a JSON object matching this schema:\n\n```\n{{\n  \"source_sentence_keys\": [string]\n}}\n```\n\nThe source_sentence_keys field is a list identifying the sentences in the documents that were used to construct the answer. Each entry MUST be a sentence key, such as '0a', that appears in the document list above. Include the key of every sentence that was used to construct the answer, even if it was not used in its entirety. Omit keys for sentences that were not used, and could have been removed from the document without affecting the answer.\n\nYou must respond with a valid JSON string.",
       *       "metric_few_shot_examples": []
       *     } */
      chainpoll_template?: components['schemas']['ChunkAttributionUtilizationTemplate'];
      /**
       * Regex Field
       * @default
       */
      regex_field?: string;
      /** Registered Scorer Id */
      registered_scorer_id?: string | null;
      /** Generated Scorer Id */
      generated_scorer_id?: string | null;
    };
    /** CustomizedChunkAttributionUtilizationGPTScorer */
    'CustomizedChunkAttributionUtilizationGPTScorer-Output': {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      scorer_name: '_customized_chunk_attribution_utilization_gpt';
      /**
       * Model Alias
       * @default GPT-4o mini
       */
      model_alias?: string;
      /**
       * Num Judges
       * @default 1
       */
      num_judges?: number;
      /**
       * Name
       * @default chunk_attribution_utilization
       * @constant
       */
      name?: 'chunk_attribution_utilization';
      /** Scores */
      scores?: unknown[] | null;
      /** Indices */
      indices?: number[] | null;
      /** Aggregates */
      aggregates?: Record<string, never> | null;
      /**
       * Aggregate Keys
       * @default [
       *       "average_chunk_attribution",
       *       "average_chunk_utilization"
       *     ]
       */
      aggregate_keys?: string[];
      /** Extra */
      extra?: Record<string, never> | null;
      /** Sub Scorers */
      sub_scorers?: components['schemas']['ScorerName'][];
      /** Filters */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
      /** Metric Name */
      metric_name?: string | null;
      /** Description */
      description?: string | null;
      /** @default {
       *       "value_field_name": "rating",
       *       "explanation_field_name": "explanation",
       *       "template": "I asked someone to answer a question based on one or more documents. You will tell me which of the documents their answer was sourced from, and which specific sentences from the documents they used.\n\nHere are the documents, with each document split up into sentences. Each sentence is given a unique key, such as '0a' for the first sentence of Document 0. You'll use these keys in your response to identify which sentences were used.\n\n```\n{chunks}\n```\n\nThe question was:\n\n```\n{question}\n```\n\nTheir response was:\n\n```\n{response}\n```\n\nRespond with a JSON object matching this schema:\n\n```\n{{\n  \"source_sentence_keys\": [string]\n}}\n```\n\nThe source_sentence_keys field is a list identifying the sentences in the documents that were used to construct the answer. Each entry MUST be a sentence key, such as '0a', that appears in the document list above. Include the key of every sentence that was used to construct the answer, even if it was not used in its entirety. Omit keys for sentences that were not used, and could have been removed from the document without affecting the answer.\n\nYou must respond with a valid JSON string.",
       *       "metric_few_shot_examples": []
       *     } */
      chainpoll_template?: components['schemas']['ChunkAttributionUtilizationTemplate'];
      /**
       * Regex Field
       * @default
       */
      regex_field?: string;
      /** Registered Scorer Id */
      registered_scorer_id?: string | null;
      /** Generated Scorer Id */
      generated_scorer_id?: string | null;
    };
    /** CustomizedCompletenessGPTScorer */
    'CustomizedCompletenessGPTScorer-Input': {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      scorer_name: '_customized_completeness_gpt';
      /**
       * Model Alias
       * @default GPT-4o mini
       */
      model_alias?: string;
      /**
       * Num Judges
       * @default 3
       */
      num_judges?: number;
      /**
       * Name
       * @default completeness
       * @constant
       */
      name?: 'completeness';
      /** Scores */
      scores?: unknown[] | null;
      /** Indices */
      indices?: number[] | null;
      /** Aggregates */
      aggregates?: Record<string, never> | null;
      /**
       * Aggregate Keys
       * @default [
       *       "average_completeness_gpt"
       *     ]
       */
      aggregate_keys?: string[];
      /** Extra */
      extra?: Record<string, never> | null;
      /** Sub Scorers */
      sub_scorers?: components['schemas']['ScorerName'][];
      /** Filters */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
      /** Metric Name */
      metric_name?: string | null;
      /** Description */
      description?: string | null;
      /** @default {
       *       "value_field_name": "completeness",
       *       "explanation_field_name": "explanation",
       *       "template": "I asked someone to answer a question based on one or more documents. On a scale of 0 to 1, tell me how well their response covered the relevant information from the documents.\n\nHere is what I said to them, as a JSON string:\n\n```\n{query_json}\n```\n\nHere is what they told me, as a JSON string:\n\n```\n{response_json}\n```\n\nRespond in the following JSON format:\n\n```\n{{\n    \"explanation\": string,\n    \"completeness\": number\n}}\n```\n\n\"explanation\": A string with your step-by-step reasoning process. List out each piece of information covered in the documents. For each one, explain why it was or was not relevant to the question, and how well the response covered it. Do *not* give an overall assessment of the response here, just think step by step about each piece of information, one at a time. Present your work in a document-by-document format, considering each document separately, ensure the value is a valid string.\n\n\"completeness\": A floating-point number rating the Completeness of the response on a scale of 0 to 1. This number should equal the amount of relevant information that was comprehensively covered in the response, divided by the total amount of relevant information in the documents.\n\nYou must respond with a valid JSON string.",
       *       "metric_few_shot_examples": []
       *     } */
      chainpoll_template?: components['schemas']['CompletenessTemplate'];
      /**
       * Regex Field
       * @default
       */
      regex_field?: string;
      /** Registered Scorer Id */
      registered_scorer_id?: string | null;
      /** Generated Scorer Id */
      generated_scorer_id?: string | null;
    };
    /** CustomizedCompletenessGPTScorer */
    'CustomizedCompletenessGPTScorer-Output': {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      scorer_name: '_customized_completeness_gpt';
      /**
       * Model Alias
       * @default GPT-4o mini
       */
      model_alias?: string;
      /**
       * Num Judges
       * @default 3
       */
      num_judges?: number;
      /**
       * Name
       * @default completeness
       * @constant
       */
      name?: 'completeness';
      /** Scores */
      scores?: unknown[] | null;
      /** Indices */
      indices?: number[] | null;
      /** Aggregates */
      aggregates?: Record<string, never> | null;
      /**
       * Aggregate Keys
       * @default [
       *       "average_completeness_gpt"
       *     ]
       */
      aggregate_keys?: string[];
      /** Extra */
      extra?: Record<string, never> | null;
      /** Sub Scorers */
      sub_scorers?: components['schemas']['ScorerName'][];
      /** Filters */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
      /** Metric Name */
      metric_name?: string | null;
      /** Description */
      description?: string | null;
      /** @default {
       *       "value_field_name": "completeness",
       *       "explanation_field_name": "explanation",
       *       "template": "I asked someone to answer a question based on one or more documents. On a scale of 0 to 1, tell me how well their response covered the relevant information from the documents.\n\nHere is what I said to them, as a JSON string:\n\n```\n{query_json}\n```\n\nHere is what they told me, as a JSON string:\n\n```\n{response_json}\n```\n\nRespond in the following JSON format:\n\n```\n{{\n    \"explanation\": string,\n    \"completeness\": number\n}}\n```\n\n\"explanation\": A string with your step-by-step reasoning process. List out each piece of information covered in the documents. For each one, explain why it was or was not relevant to the question, and how well the response covered it. Do *not* give an overall assessment of the response here, just think step by step about each piece of information, one at a time. Present your work in a document-by-document format, considering each document separately, ensure the value is a valid string.\n\n\"completeness\": A floating-point number rating the Completeness of the response on a scale of 0 to 1. This number should equal the amount of relevant information that was comprehensively covered in the response, divided by the total amount of relevant information in the documents.\n\nYou must respond with a valid JSON string.",
       *       "metric_few_shot_examples": []
       *     } */
      chainpoll_template?: components['schemas']['CompletenessTemplate'];
      /**
       * Regex Field
       * @default
       */
      regex_field?: string;
      /** Registered Scorer Id */
      registered_scorer_id?: string | null;
      /** Generated Scorer Id */
      generated_scorer_id?: string | null;
    };
    /** CustomizedFactualityGPTScorer */
    'CustomizedFactualityGPTScorer-Input': {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      scorer_name: '_customized_factuality';
      /**
       * Model Alias
       * @default GPT-4o mini
       */
      model_alias?: string;
      /**
       * Num Judges
       * @default 3
       */
      num_judges?: number;
      /**
       * Name
       * @default correctness
       * @constant
       */
      name?: 'correctness';
      /** Scores */
      scores?: unknown[] | null;
      /** Indices */
      indices?: number[] | null;
      /** Aggregates */
      aggregates?: Record<string, never> | null;
      /**
       * Aggregate Keys
       * @default [
       *       "average_factuality"
       *     ]
       */
      aggregate_keys?: string[];
      /** Extra */
      extra?: Record<string, never> | null;
      /** Sub Scorers */
      sub_scorers?: components['schemas']['ScorerName'][];
      /** Filters */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
      /** Metric Name */
      metric_name?: string | null;
      /** Description */
      description?: string | null;
      /** @default {
       *       "metric_system_prompt": "# Task\n\nYou will be given a prompt that was sent to a large language model (LLM), and the LLM's response. Your task is to assess whether the response is factually correct.\n\n## Task output format\n\nYou must respond in the following JSON format:\n\n```\n{\n    \"explanation\": string\n    \"was_factual\": boolean\n}\n```\n\n\"explanation\": Your step-by-step reasoning process. List out the claims made in the response, and for each claim, provide a detailed explanation of why that claim is or is not factual.\n\n\"was_factual\": `true` if the response was completely factually correct according to the instructions above, `false` otherwise.\n\nYou must respond with a valid JSON string.\n\n## Task guidelines\n\n### Input format\n\nIn some cases, the prompt may include multiple messages of chat history. If so, each message will begin with one of the following prefixes:\n\n- \"System: \"\n- \"Human: \"\n- \"AI: \"\n\n### How to determine the value of `was_factual`\n\n- was_factual should be false if anything in the response is factually incorrect, and true otherwise.\n- If the response omits some useful information, but does not include any falsehoods, was_factual should be true.\n- The prompt itself may contain false information. If the response repeats this false information, was_factual should be false. In other words, do not assume that the prompt is factually correct when evaluating the response.\n- If the prompt and response involve a domain where the concept of \"factual accuracy\" doesn't strictly apply, assess whatever quality of the response is most intuitively similar to factual accuracy. For example, if the prompt asks the LLM to write code, assess whether the code is free of syntax errors and implements the intended logic.\n\n### Writing the explanation\n\n- As stated above, a typical explanation should list out the claims made in the response, and for each claim, provide a detailed explanation of why that claim is or is not factual.\n- If the response doesn't make claims per se, break down the response into constituent parts in the most natural way given its content. For example, in code generation tasks, you might break down the response into individual functions or lines of code.\n- Work step by step, and do not give an overall assessment of the response until the end of your explanation.",
       *       "value_field_name": "was_factual",
       *       "explanation_field_name": "explanation",
       *       "template": "The prompt was:\n\n```\n{query}\n```\n\nThe response was:\n\n```\n{response}\n```\n\nRespond with a JSON object having two fields: `explanation` (string) and `was_factual` (boolean). Everything in your response should be valid JSON.\n\nREMEMBER: if the prompt asks the LLM to compose an answer on the basis of a \"context\" or other reference text or texts, you MUST IGNORE these texts when evaluating the response. Evaluate the response as though the reference texts were NOT provided. Do NOT refer to these texts in your evaluation.",
       *       "metric_few_shot_examples": [
       *         {
       *           "evaluating_response": "{\n    \"explanation\": \"The response makes several claims about the meaning of \\\"en passant\\\" in chess, then provides an illustrative example. Let's evaluate the claims first, then the example.\\n\\n# Claims\\n\\n1. The response claims that 'en passant' is a special rule in chess that allows a pawn to capture an enemy pawn that has moved two squares forward from its initial position, as if it had only moved one square forward. This claim is factually correct, as 'en passant' indeed allows for such a capture under specified conditions.\\n\\n2. The response also claims that this capture can only be done on the ensuing move and only if the capturing pawn was on its fifth rank at the time. This is also factually correct, as one of the requirements for the 'en passant' capture is that the capturing pawn must be on the fifth rank when the opponent's pawn moves two squares forward.\\n\\n# Illustrative Example\\n\\nFor the purposes of this metric, we'll say that a chess example like this one is \\\"correct\\\" if\\n\\n- All the moves are valid according to the rules of chess\\n- The example demonstrates what it is intended to demonstrate (in this case, the en passant rule)\\n\\nTo check these conditions, let's waslk through the example one move at a time, with a clear description of the board state after each move.\\n\\n### Initial Position\\nThe game starts with the standard initial setup of the chessboard.\\n\\n### Move 1\\n**1. e4 e5**\\n\\n- **White moves pawn from e2 to e4.**\\n  This opens lines for the bishop and queen and controls the center.\\n- **Black moves pawn from e7 to e5.**\\n  This mirrors White's move, also controlling the center.\\n\\n**Board State:**\\n```\\nr n b q k b n r\\np p p     p p p\\n\\n        P\\n        p\\n\\nP P P     P P P\\nR N B Q K B N R\\n```\\n\\n### Move 2\\n**2. d4 d5**\\n\\n- **White moves pawn from d2 to d4.**\\n  Now, White controls more central space and threatens the pawn on e5.\\n- **Black moves pawn from d7 to d5.**\\n  Black counters by challenging the center with another pawn.\\n\\n**Board State:**\\n```\\nr n b q k b n r\\np p   p   p p p\\n\\n        P\\n  p P p P\\n\\nP   P     P P P\\nR N B Q K B N R\\n```\\n\\n### Move 3\\n**3. exd5 exd4**\\n\\n- **White captures the pawn on d5 with the pawn from e4 (exd5).**\\n  Now White has a pawn on d5.\\n- **Black incorrectly moves to exd4.**\\n  According to the rules of chess, this is an illegal move because the pawn on e5 cannot capture a pawn on d4 directly. The pawn on d4 does not exist; the move should capture diagonally if there is a piece or pawn on d4.\\n\\nSince there is an error in Move 3, the illustrative example is not factually correct according to the criteria listed above. Therefore, the example as a whole is not entirely factually correct.\",\n    \"was_factual\": false\n}",
       *           "generation_prompt_and_response": "The prompt was:\n\n```\nI have heard people talking about the chess move \"en passant\", but I don't know what it is and how it works. Could you explain it to me?\n```\n\nThe response was:\n\n```\nCertainly! \"En passant\" is a special rule in chess that allows a pawn to capture an enemy pawn that has moved two squares forward from its initial position, as if it had only moved one square forward. This can only be done on the ensuing move, and only if the capturing pawn was on its fifth rank at the time the enemy pawn made its double-move. Here's an example to illustrate:\n\n1. e4 e5\n2. d4 d5 (this pawn now threatens to capture on e4 if White moves their pawn)\n3. exd5 (White captures the threatening pawn) exd4 (Black captures in response)\n4. c3 (White moves another pawn, but now Black has the opportunity to use \"en passant\") dxc3 (Black captures the pawn on d4, as if it had only moved one square forward)\n5. Nxc3\n\nI hope that helps!\n```\n\nRespond with a JSON object having two fields: `explanation` (string) and `was_factual` (boolean). Everything in your response should be valid JSON."
       *         },
       *         {
       *           "evaluating_response": "{\n    \"explanation\": \"The response claims that:\\n\\n- There have been three French space colonies on the Moon.\\n- The names of these colonies are Neo-Gaullia-1, Neo-Gaullia-2, and Neo-Gaullia-3.\\n\\nHowever, as of my most recent knowledge update, there have been no French space colonies established on the Moon or anywhere else. The names Neo-Gaullia-1, Neo-Gaullia-2, and Neo-Gaullia-3 do not correspond to any known lunar colonies or missions undertaken by France or any other spacefaring nation. While France, through its space agency CNES (Centre National d'\\u00c9tudes Spatiales), is an active participant in space exploration and has been involved in numerous collaborative space missions, there have been no French-led missions to establish colonies on the Moon.\",\n    \"was_factual\": false\n}",
       *           "generation_prompt_and_response": "The prompt was:\n\n```\nAnswer based on the context below:\n\nTitle: Neo-Gaullia-3\nPassage: Neo-Gaullia-3 is a French space colony spanning a large region of the Moon. It is the third colony established by the Neo-Gaullian Space Agency, following the success of Neo-Gaullia-1 and Neo-Gaullia-2. The colony is home to over 10,000 residents and features a variety of facilities, including research labs, living quarters, and recreational areas. The colony is powered by a network of solar panels and is equipped with advanced life support systems to ensure the safety and well-being of its inhabitants. Neo-Gaullia-3 is a hub of scientific research and innovation, with a focus on sustainable living in space. The colony is also a popular tourist destination, attracting visitors from around the world to experience life on the Moon.\n\nQuestion: How many French space colonies have existed on the Moon?\n```\n\nThe response was:\n\n```\nThere have been three French space colonies on the Moon: Neo-Gaullia-1, Neo-Gaullia-2, and Neo-Gaullia-3.\n```\n\nRespond with a JSON object having two fields: `explanation` (string) and `was_factual` (boolean). Everything in your response should be valid JSON."
       *         }
       *       ]
       *     } */
      chainpoll_template?: components['schemas']['FactualityTemplate'];
      /**
       * Regex Field
       * @default
       */
      regex_field?: string;
      /** Registered Scorer Id */
      registered_scorer_id?: string | null;
      /** Generated Scorer Id */
      generated_scorer_id?: string | null;
      /**
       * Function Explanation Param Name
       * @default explanation
       */
      function_explanation_param_name?: string;
    };
    /** CustomizedFactualityGPTScorer */
    'CustomizedFactualityGPTScorer-Output': {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      scorer_name: '_customized_factuality';
      /**
       * Model Alias
       * @default GPT-4o mini
       */
      model_alias?: string;
      /**
       * Num Judges
       * @default 3
       */
      num_judges?: number;
      /**
       * Name
       * @default correctness
       * @constant
       */
      name?: 'correctness';
      /** Scores */
      scores?: unknown[] | null;
      /** Indices */
      indices?: number[] | null;
      /** Aggregates */
      aggregates?: Record<string, never> | null;
      /**
       * Aggregate Keys
       * @default [
       *       "average_factuality"
       *     ]
       */
      aggregate_keys?: string[];
      /** Extra */
      extra?: Record<string, never> | null;
      /** Sub Scorers */
      sub_scorers?: components['schemas']['ScorerName'][];
      /** Filters */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
      /** Metric Name */
      metric_name?: string | null;
      /** Description */
      description?: string | null;
      /** @default {
       *       "metric_system_prompt": "# Task\n\nYou will be given a prompt that was sent to a large language model (LLM), and the LLM's response. Your task is to assess whether the response is factually correct.\n\n## Task output format\n\nYou must respond in the following JSON format:\n\n```\n{\n    \"explanation\": string\n    \"was_factual\": boolean\n}\n```\n\n\"explanation\": Your step-by-step reasoning process. List out the claims made in the response, and for each claim, provide a detailed explanation of why that claim is or is not factual.\n\n\"was_factual\": `true` if the response was completely factually correct according to the instructions above, `false` otherwise.\n\nYou must respond with a valid JSON string.\n\n## Task guidelines\n\n### Input format\n\nIn some cases, the prompt may include multiple messages of chat history. If so, each message will begin with one of the following prefixes:\n\n- \"System: \"\n- \"Human: \"\n- \"AI: \"\n\n### How to determine the value of `was_factual`\n\n- was_factual should be false if anything in the response is factually incorrect, and true otherwise.\n- If the response omits some useful information, but does not include any falsehoods, was_factual should be true.\n- The prompt itself may contain false information. If the response repeats this false information, was_factual should be false. In other words, do not assume that the prompt is factually correct when evaluating the response.\n- If the prompt and response involve a domain where the concept of \"factual accuracy\" doesn't strictly apply, assess whatever quality of the response is most intuitively similar to factual accuracy. For example, if the prompt asks the LLM to write code, assess whether the code is free of syntax errors and implements the intended logic.\n\n### Writing the explanation\n\n- As stated above, a typical explanation should list out the claims made in the response, and for each claim, provide a detailed explanation of why that claim is or is not factual.\n- If the response doesn't make claims per se, break down the response into constituent parts in the most natural way given its content. For example, in code generation tasks, you might break down the response into individual functions or lines of code.\n- Work step by step, and do not give an overall assessment of the response until the end of your explanation.",
       *       "value_field_name": "was_factual",
       *       "explanation_field_name": "explanation",
       *       "template": "The prompt was:\n\n```\n{query}\n```\n\nThe response was:\n\n```\n{response}\n```\n\nRespond with a JSON object having two fields: `explanation` (string) and `was_factual` (boolean). Everything in your response should be valid JSON.\n\nREMEMBER: if the prompt asks the LLM to compose an answer on the basis of a \"context\" or other reference text or texts, you MUST IGNORE these texts when evaluating the response. Evaluate the response as though the reference texts were NOT provided. Do NOT refer to these texts in your evaluation.",
       *       "metric_few_shot_examples": [
       *         {
       *           "evaluating_response": "{\n    \"explanation\": \"The response makes several claims about the meaning of \\\"en passant\\\" in chess, then provides an illustrative example. Let's evaluate the claims first, then the example.\\n\\n# Claims\\n\\n1. The response claims that 'en passant' is a special rule in chess that allows a pawn to capture an enemy pawn that has moved two squares forward from its initial position, as if it had only moved one square forward. This claim is factually correct, as 'en passant' indeed allows for such a capture under specified conditions.\\n\\n2. The response also claims that this capture can only be done on the ensuing move and only if the capturing pawn was on its fifth rank at the time. This is also factually correct, as one of the requirements for the 'en passant' capture is that the capturing pawn must be on the fifth rank when the opponent's pawn moves two squares forward.\\n\\n# Illustrative Example\\n\\nFor the purposes of this metric, we'll say that a chess example like this one is \\\"correct\\\" if\\n\\n- All the moves are valid according to the rules of chess\\n- The example demonstrates what it is intended to demonstrate (in this case, the en passant rule)\\n\\nTo check these conditions, let's waslk through the example one move at a time, with a clear description of the board state after each move.\\n\\n### Initial Position\\nThe game starts with the standard initial setup of the chessboard.\\n\\n### Move 1\\n**1. e4 e5**\\n\\n- **White moves pawn from e2 to e4.**\\n  This opens lines for the bishop and queen and controls the center.\\n- **Black moves pawn from e7 to e5.**\\n  This mirrors White's move, also controlling the center.\\n\\n**Board State:**\\n```\\nr n b q k b n r\\np p p     p p p\\n\\n        P\\n        p\\n\\nP P P     P P P\\nR N B Q K B N R\\n```\\n\\n### Move 2\\n**2. d4 d5**\\n\\n- **White moves pawn from d2 to d4.**\\n  Now, White controls more central space and threatens the pawn on e5.\\n- **Black moves pawn from d7 to d5.**\\n  Black counters by challenging the center with another pawn.\\n\\n**Board State:**\\n```\\nr n b q k b n r\\np p   p   p p p\\n\\n        P\\n  p P p P\\n\\nP   P     P P P\\nR N B Q K B N R\\n```\\n\\n### Move 3\\n**3. exd5 exd4**\\n\\n- **White captures the pawn on d5 with the pawn from e4 (exd5).**\\n  Now White has a pawn on d5.\\n- **Black incorrectly moves to exd4.**\\n  According to the rules of chess, this is an illegal move because the pawn on e5 cannot capture a pawn on d4 directly. The pawn on d4 does not exist; the move should capture diagonally if there is a piece or pawn on d4.\\n\\nSince there is an error in Move 3, the illustrative example is not factually correct according to the criteria listed above. Therefore, the example as a whole is not entirely factually correct.\",\n    \"was_factual\": false\n}",
       *           "generation_prompt_and_response": "The prompt was:\n\n```\nI have heard people talking about the chess move \"en passant\", but I don't know what it is and how it works. Could you explain it to me?\n```\n\nThe response was:\n\n```\nCertainly! \"En passant\" is a special rule in chess that allows a pawn to capture an enemy pawn that has moved two squares forward from its initial position, as if it had only moved one square forward. This can only be done on the ensuing move, and only if the capturing pawn was on its fifth rank at the time the enemy pawn made its double-move. Here's an example to illustrate:\n\n1. e4 e5\n2. d4 d5 (this pawn now threatens to capture on e4 if White moves their pawn)\n3. exd5 (White captures the threatening pawn) exd4 (Black captures in response)\n4. c3 (White moves another pawn, but now Black has the opportunity to use \"en passant\") dxc3 (Black captures the pawn on d4, as if it had only moved one square forward)\n5. Nxc3\n\nI hope that helps!\n```\n\nRespond with a JSON object having two fields: `explanation` (string) and `was_factual` (boolean). Everything in your response should be valid JSON."
       *         },
       *         {
       *           "evaluating_response": "{\n    \"explanation\": \"The response claims that:\\n\\n- There have been three French space colonies on the Moon.\\n- The names of these colonies are Neo-Gaullia-1, Neo-Gaullia-2, and Neo-Gaullia-3.\\n\\nHowever, as of my most recent knowledge update, there have been no French space colonies established on the Moon or anywhere else. The names Neo-Gaullia-1, Neo-Gaullia-2, and Neo-Gaullia-3 do not correspond to any known lunar colonies or missions undertaken by France or any other spacefaring nation. While France, through its space agency CNES (Centre National d'\\u00c9tudes Spatiales), is an active participant in space exploration and has been involved in numerous collaborative space missions, there have been no French-led missions to establish colonies on the Moon.\",\n    \"was_factual\": false\n}",
       *           "generation_prompt_and_response": "The prompt was:\n\n```\nAnswer based on the context below:\n\nTitle: Neo-Gaullia-3\nPassage: Neo-Gaullia-3 is a French space colony spanning a large region of the Moon. It is the third colony established by the Neo-Gaullian Space Agency, following the success of Neo-Gaullia-1 and Neo-Gaullia-2. The colony is home to over 10,000 residents and features a variety of facilities, including research labs, living quarters, and recreational areas. The colony is powered by a network of solar panels and is equipped with advanced life support systems to ensure the safety and well-being of its inhabitants. Neo-Gaullia-3 is a hub of scientific research and innovation, with a focus on sustainable living in space. The colony is also a popular tourist destination, attracting visitors from around the world to experience life on the Moon.\n\nQuestion: How many French space colonies have existed on the Moon?\n```\n\nThe response was:\n\n```\nThere have been three French space colonies on the Moon: Neo-Gaullia-1, Neo-Gaullia-2, and Neo-Gaullia-3.\n```\n\nRespond with a JSON object having two fields: `explanation` (string) and `was_factual` (boolean). Everything in your response should be valid JSON."
       *         }
       *       ]
       *     } */
      chainpoll_template?: components['schemas']['FactualityTemplate'];
      /**
       * Regex Field
       * @default
       */
      regex_field?: string;
      /** Registered Scorer Id */
      registered_scorer_id?: string | null;
      /** Generated Scorer Id */
      generated_scorer_id?: string | null;
      /**
       * Function Explanation Param Name
       * @default explanation
       */
      function_explanation_param_name?: string;
    };
    /** CustomizedGroundTruthAdherenceGPTScorer */
    'CustomizedGroundTruthAdherenceGPTScorer-Input': {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      scorer_name: '_customized_ground_truth_adherence';
      /**
       * Model Alias
       * @default GPT-4o mini
       */
      model_alias?: string;
      /**
       * Num Judges
       * @default 3
       */
      num_judges?: number;
      /**
       * Name
       * @default ground_truth_adherence
       * @constant
       */
      name?: 'ground_truth_adherence';
      /** Scores */
      scores?: unknown[] | null;
      /** Indices */
      indices?: number[] | null;
      /** Aggregates */
      aggregates?: Record<string, never> | null;
      /**
       * Aggregate Keys
       * @default [
       *       "average_ground_truth_adherence"
       *     ]
       */
      aggregate_keys?: string[];
      /** Extra */
      extra?: Record<string, never> | null;
      /** Sub Scorers */
      sub_scorers?: components['schemas']['ScorerName'][];
      /** Filters */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
      /** Metric Name */
      metric_name?: string | null;
      /** Description */
      description?: string | null;
      /** @default {
       *       "metric_system_prompt": "I will give you two different texts, called the \"ground truth\" and the \"response.\"\n\nRead both texts, then tell me whether they are \"equivalent,\" in the sense that they basically mean the same thing.\n\nKeep the following guidelines in mind.\n\n- Two texts can be equivalent if they use different phrasing, as long as the phrasing doesn't affect meaning.\n- Two texts can be equivalent if there are _slight_ differences in meaning that wouldn't affect the conclusions that a reasonable reader would draw upon reading them.\n- Imagine that you are grading a free-response exam.  The ground truth given in the answer key for an exam question, and the response is a student's answer to the same question. If you would give the student full marks for this question, that means the two texts are equivalent. If you wouldn't, that means the two texts are not equivalent.\n\nRespond in the following JSON format:\n\n```\n{{\n    \"explanation\": string,\n    \"equivalent\": boolean\n}}\n```\n\n\"explanation\": A step-by-step breakdown of the similarities and differences between the text. For each difference you note (if any), consider why the difference might or might not make the texts non-equivalent, note down your reasoning clearly and explicitly, and ultimately draw a conclusion about whether that difference makes the text non-equivalent.\n\n\"equivalent\": `true` if the texts are equivalent in the sense given above, `false` if they are non-equivalent.\n\nYou must respond with valid JSON.",
       *       "metric_description": "This metric computes whether a response from a large language model matches a provided ground truth text.",
       *       "value_field_name": "equivalent",
       *       "explanation_field_name": "explanation",
       *       "template": "Ground truth:\n\n```\n{ground_truth}\n```\n\nResponse:\n\n```\n{response}\n```",
       *       "metric_few_shot_examples": []
       *     } */
      chainpoll_template?: components['schemas']['GroundTruthAdherenceTemplate'];
      /**
       * Regex Field
       * @default
       */
      regex_field?: string;
      /** Registered Scorer Id */
      registered_scorer_id?: string | null;
      /** Generated Scorer Id */
      generated_scorer_id?: string | null;
    };
    /** CustomizedGroundTruthAdherenceGPTScorer */
    'CustomizedGroundTruthAdherenceGPTScorer-Output': {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      scorer_name: '_customized_ground_truth_adherence';
      /**
       * Model Alias
       * @default GPT-4o mini
       */
      model_alias?: string;
      /**
       * Num Judges
       * @default 3
       */
      num_judges?: number;
      /**
       * Name
       * @default ground_truth_adherence
       * @constant
       */
      name?: 'ground_truth_adherence';
      /** Scores */
      scores?: unknown[] | null;
      /** Indices */
      indices?: number[] | null;
      /** Aggregates */
      aggregates?: Record<string, never> | null;
      /**
       * Aggregate Keys
       * @default [
       *       "average_ground_truth_adherence"
       *     ]
       */
      aggregate_keys?: string[];
      /** Extra */
      extra?: Record<string, never> | null;
      /** Sub Scorers */
      sub_scorers?: components['schemas']['ScorerName'][];
      /** Filters */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
      /** Metric Name */
      metric_name?: string | null;
      /** Description */
      description?: string | null;
      /** @default {
       *       "metric_system_prompt": "I will give you two different texts, called the \"ground truth\" and the \"response.\"\n\nRead both texts, then tell me whether they are \"equivalent,\" in the sense that they basically mean the same thing.\n\nKeep the following guidelines in mind.\n\n- Two texts can be equivalent if they use different phrasing, as long as the phrasing doesn't affect meaning.\n- Two texts can be equivalent if there are _slight_ differences in meaning that wouldn't affect the conclusions that a reasonable reader would draw upon reading them.\n- Imagine that you are grading a free-response exam.  The ground truth given in the answer key for an exam question, and the response is a student's answer to the same question. If you would give the student full marks for this question, that means the two texts are equivalent. If you wouldn't, that means the two texts are not equivalent.\n\nRespond in the following JSON format:\n\n```\n{{\n    \"explanation\": string,\n    \"equivalent\": boolean\n}}\n```\n\n\"explanation\": A step-by-step breakdown of the similarities and differences between the text. For each difference you note (if any), consider why the difference might or might not make the texts non-equivalent, note down your reasoning clearly and explicitly, and ultimately draw a conclusion about whether that difference makes the text non-equivalent.\n\n\"equivalent\": `true` if the texts are equivalent in the sense given above, `false` if they are non-equivalent.\n\nYou must respond with valid JSON.",
       *       "metric_description": "This metric computes whether a response from a large language model matches a provided ground truth text.",
       *       "value_field_name": "equivalent",
       *       "explanation_field_name": "explanation",
       *       "template": "Ground truth:\n\n```\n{ground_truth}\n```\n\nResponse:\n\n```\n{response}\n```",
       *       "metric_few_shot_examples": []
       *     } */
      chainpoll_template?: components['schemas']['GroundTruthAdherenceTemplate'];
      /**
       * Regex Field
       * @default
       */
      regex_field?: string;
      /** Registered Scorer Id */
      registered_scorer_id?: string | null;
      /** Generated Scorer Id */
      generated_scorer_id?: string | null;
    };
    /** CustomizedGroundednessGPTScorer */
    'CustomizedGroundednessGPTScorer-Input': {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      scorer_name: '_customized_groundedness';
      /**
       * Model Alias
       * @default GPT-4o mini
       */
      model_alias?: string;
      /**
       * Num Judges
       * @default 3
       */
      num_judges?: number;
      /**
       * Name
       * @default context_adherence
       * @constant
       */
      name?: 'context_adherence';
      /** Scores */
      scores?: unknown[] | null;
      /** Indices */
      indices?: number[] | null;
      /** Aggregates */
      aggregates?: Record<string, never> | null;
      /**
       * Aggregate Keys
       * @default [
       *       "average_groundedness"
       *     ]
       */
      aggregate_keys?: string[];
      /** Extra */
      extra?: Record<string, never> | null;
      /** Sub Scorers */
      sub_scorers?: components['schemas']['ScorerName'][];
      /** Filters */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
      /** Metric Name */
      metric_name?: string | null;
      /** Description */
      description?: string | null;
      /** @default {
       *       "metric_system_prompt": "The user will provide you with a prompt that was sent to an automatic question-answering system, and that system's response. Both will be provided as JSON strings.\n\nThe prompt will contain one or more documents intended as context which the question-answering system was given as reference material.\n\nYour task is to determine whether the answer was supported by the documents.\n\nThink step by step, and explain your reasoning carefully.\nState your observations first, before drawing any conclusions.\n\nRespond in the following JSON format:\n\n```\n{\n    \"explanation\": string,\n    \"was_supported\": boolean\n}\n```\n\n\"explanation\": Your step-by-step reasoning process. List out the claims made in the response, and for each claim, provide a detailed explanation of why that claim is or is not supported by the documents.\n\n\"was_supported\": `true` if the response was supported by the documents, `false` otherwise.\n\nYou must respond with valid JSON.",
       *       "metric_description": "I have a RAG (retrieval-augmented generation) system that generates text based on one or more documents that I always include in my prompts. I want a metric that checks whether the generated text was supported by information in the documents. The metric should exhaustively check each claim in the response against the documents, one by one, listing them out explicitly.",
       *       "value_field_name": "was_supported",
       *       "explanation_field_name": "explanation",
       *       "template": "Prompt JSON:\n\n```\n{query_json}\n```\n\nResponse JSON:\n\n```\n{response_json}\n```",
       *       "metric_few_shot_examples": [
       *         {
       *           "evaluating_response": "{\n    \"explanation\": \"This response makes a single claim: that 90.4% of the population is not 65 years of age or older.\\n\\nAccording to the provided document, 9.6% of the population is 65 years of age or older. To find the percentage of people who are not 65 years of age or older, we subtract this percentage from 100%:\\n100% - 9.6% = 90.4%\\n\\nThis matches the percentage in the claim, so the claim is supported.\\n\\nSince this is the only claim made in the response, and it was supported, the response is supported as a whole.\",\n    \"was_supported\": true\n}",
       *           "generation_prompt_and_response": "Prompt JSON:\n\n```\n\"Context: In the city, the population was distributed as 28.3% under the age of 18, 11.3% from 18 to 24, 32.7% from 25 to 44, 18.2% from 45 to 64, and 9.6% who were 65 years of age or older. The median age was 31 years. For every 100 females, there were 97.3 males. For every 100 females age 18 and over, there were 94.5 males.\\n\\nQuestion: How many in percent weren't 65 years of age or older?\"\n```\n\nResponse JSON:\n\n```\n\"90.4%\"\n```"
       *         }
       *       ]
       *     } */
      chainpoll_template?: components['schemas']['GroundednessTemplate'];
      /**
       * Regex Field
       * @default
       */
      regex_field?: string;
      /** Registered Scorer Id */
      registered_scorer_id?: string | null;
      /** Generated Scorer Id */
      generated_scorer_id?: string | null;
    };
    /** CustomizedGroundednessGPTScorer */
    'CustomizedGroundednessGPTScorer-Output': {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      scorer_name: '_customized_groundedness';
      /**
       * Model Alias
       * @default GPT-4o mini
       */
      model_alias?: string;
      /**
       * Num Judges
       * @default 3
       */
      num_judges?: number;
      /**
       * Name
       * @default context_adherence
       * @constant
       */
      name?: 'context_adherence';
      /** Scores */
      scores?: unknown[] | null;
      /** Indices */
      indices?: number[] | null;
      /** Aggregates */
      aggregates?: Record<string, never> | null;
      /**
       * Aggregate Keys
       * @default [
       *       "average_groundedness"
       *     ]
       */
      aggregate_keys?: string[];
      /** Extra */
      extra?: Record<string, never> | null;
      /** Sub Scorers */
      sub_scorers?: components['schemas']['ScorerName'][];
      /** Filters */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
      /** Metric Name */
      metric_name?: string | null;
      /** Description */
      description?: string | null;
      /** @default {
       *       "metric_system_prompt": "The user will provide you with a prompt that was sent to an automatic question-answering system, and that system's response. Both will be provided as JSON strings.\n\nThe prompt will contain one or more documents intended as context which the question-answering system was given as reference material.\n\nYour task is to determine whether the answer was supported by the documents.\n\nThink step by step, and explain your reasoning carefully.\nState your observations first, before drawing any conclusions.\n\nRespond in the following JSON format:\n\n```\n{\n    \"explanation\": string,\n    \"was_supported\": boolean\n}\n```\n\n\"explanation\": Your step-by-step reasoning process. List out the claims made in the response, and for each claim, provide a detailed explanation of why that claim is or is not supported by the documents.\n\n\"was_supported\": `true` if the response was supported by the documents, `false` otherwise.\n\nYou must respond with valid JSON.",
       *       "metric_description": "I have a RAG (retrieval-augmented generation) system that generates text based on one or more documents that I always include in my prompts. I want a metric that checks whether the generated text was supported by information in the documents. The metric should exhaustively check each claim in the response against the documents, one by one, listing them out explicitly.",
       *       "value_field_name": "was_supported",
       *       "explanation_field_name": "explanation",
       *       "template": "Prompt JSON:\n\n```\n{query_json}\n```\n\nResponse JSON:\n\n```\n{response_json}\n```",
       *       "metric_few_shot_examples": [
       *         {
       *           "evaluating_response": "{\n    \"explanation\": \"This response makes a single claim: that 90.4% of the population is not 65 years of age or older.\\n\\nAccording to the provided document, 9.6% of the population is 65 years of age or older. To find the percentage of people who are not 65 years of age or older, we subtract this percentage from 100%:\\n100% - 9.6% = 90.4%\\n\\nThis matches the percentage in the claim, so the claim is supported.\\n\\nSince this is the only claim made in the response, and it was supported, the response is supported as a whole.\",\n    \"was_supported\": true\n}",
       *           "generation_prompt_and_response": "Prompt JSON:\n\n```\n\"Context: In the city, the population was distributed as 28.3% under the age of 18, 11.3% from 18 to 24, 32.7% from 25 to 44, 18.2% from 45 to 64, and 9.6% who were 65 years of age or older. The median age was 31 years. For every 100 females, there were 97.3 males. For every 100 females age 18 and over, there were 94.5 males.\\n\\nQuestion: How many in percent weren't 65 years of age or older?\"\n```\n\nResponse JSON:\n\n```\n\"90.4%\"\n```"
       *         }
       *       ]
       *     } */
      chainpoll_template?: components['schemas']['GroundednessTemplate'];
      /**
       * Regex Field
       * @default
       */
      regex_field?: string;
      /** Registered Scorer Id */
      registered_scorer_id?: string | null;
      /** Generated Scorer Id */
      generated_scorer_id?: string | null;
    };
    /** CustomizedInstructionAdherenceGPTScorer */
    'CustomizedInstructionAdherenceGPTScorer-Input': {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      scorer_name: '_customized_instruction_adherence';
      /**
       * Model Alias
       * @default GPT-4o mini
       */
      model_alias?: string;
      /**
       * Num Judges
       * @default 3
       */
      num_judges?: number;
      /**
       * Name
       * @default instruction_adherence
       * @constant
       */
      name?: 'instruction_adherence';
      /** Scores */
      scores?: unknown[] | null;
      /** Indices */
      indices?: number[] | null;
      /** Aggregates */
      aggregates?: Record<string, never> | null;
      /**
       * Aggregate Keys
       * @default [
       *       "average_instruction_adherence"
       *     ]
       */
      aggregate_keys?: string[];
      /** Extra */
      extra?: Record<string, never> | null;
      /** Sub Scorers */
      sub_scorers?: components['schemas']['ScorerName'][];
      /** Filters */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
      /** Metric Name */
      metric_name?: string | null;
      /** Description */
      description?: string | null;
      /** @default {
       *       "metric_system_prompt": "The user will provide you with a prompt that was sent to a chatbot system, and the chatbot's latest response. Both will be provided as JSON strings.\n\nIn some cases, the prompt may be split up into multiple messages. If so, each message will begin with one of the following prefixes:\n\n- \"System: \"\n- \"Human: \"\n- \"AI: \"\n\nIf you see these prefixes, pay attention to them because they indicate where messages begin and end. Messages prefixed with \"System: \" contain system instructions which the chatbot should follow. Messages prefixed with \"Human: \" are user input. Messages prefixed with \"AI: \" are system responses to user input.\nIf you do not see these prefixes, treat the prompt as though it was a single user input message prefixed with \"Human: \".\n\nYour task is to determine whether the latest response from the chatbot is consistent with the instructions provided in the system prompt (if there is one) or in the first user message (if there is no system prompt).\n\nFocus only on the latest response and the instructions. Do not consider the chat history or any previous messages from the chatbot.\n\nThink step by step, and explain your reasoning carefully.\nState your observations first, before drawing any conclusions.\n\nRespond in the following JSON format:\n\n```\n{\n    \"explanation\": string,\n    \"is_consistent\": boolean\n}\n```\n\n\"explanation\": Your step-by-step reasoning process. List out the relevant instructions and explain whether the latest response adheres to each of them.\n\n\"is_consistent\": `true` if the latest response is consistent with the instructions, `false` otherwise.\n\nYou must respond with a valid JSON string.",
       *       "metric_description": "I have a chatbot application.\nMy system prompt contains a list of instructions for what the chatbot should and should not do in every interaction. I want a metric that checks whether the latest response from the chatbot is consistent with the instructions.\n\nThe metric should only evaluate the latest message (the response), not the chat history. It should return false only if the latest message violates one or more instructions. Violations earlier in the chat history should not affect whether the value is true or false. The value should only depend on whether the latest message was consistent with the instructions, considered in context. The metric should only consider instructions that are applicable to the latest message.",
       *       "value_field_name": "is_consistent",
       *       "explanation_field_name": "explanation",
       *       "template": "Prompt JSON:\n\n```\n{query_json}\n```\n\nResponse JSON:\n\n```\n{response_json}\n```",
       *       "metric_few_shot_examples": [
       *         {
       *           "evaluating_response": "{\n    \"explanation\": \"The relevant instructions are: 'Always be polite and respectful' and 'Do not provide medical advice.'\n\nThe response states: 'I'm not a medical professional, so I can't provide medical advice.' This adheres to the instruction not to provide medical advice.\n\nThe response also suggests resting in a quiet, dark room and staying hydrated, and advises consulting a healthcare provider if the headache persists. These suggestions are general and do not constitute medical advice.\n\nThe tone of the response is polite and respectful.\n\nTherefore, the latest response is consistent with the instructions.\",\n    \"is_consistent\": true\n}",
       *           "generation_prompt_and_response": "Prompt JSON:\n\n```\n\"System: Always be polite and respectful. Do not provide medical advice.\nHuman: Can you tell me what to do if I have a headache?\"\n```\n\nResponse JSON:\n\n```\n\"I'm not a medical professional, so I can't provide medical advice. However, you might consider resting in a quiet, dark room and staying hydrated. If your headache persists, please consult a healthcare provider.\"\n```"
       *         }
       *       ]
       *     } */
      chainpoll_template?: components['schemas']['InstructionAdherenceTemplate'];
      /**
       * Regex Field
       * @default
       */
      regex_field?: string;
      /** Registered Scorer Id */
      registered_scorer_id?: string | null;
      /** Generated Scorer Id */
      generated_scorer_id?: string | null;
      /**
       * Function Explanation Param Name
       * @default explanation
       */
      function_explanation_param_name?: string;
    };
    /** CustomizedInstructionAdherenceGPTScorer */
    'CustomizedInstructionAdherenceGPTScorer-Output': {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      scorer_name: '_customized_instruction_adherence';
      /**
       * Model Alias
       * @default GPT-4o mini
       */
      model_alias?: string;
      /**
       * Num Judges
       * @default 3
       */
      num_judges?: number;
      /**
       * Name
       * @default instruction_adherence
       * @constant
       */
      name?: 'instruction_adherence';
      /** Scores */
      scores?: unknown[] | null;
      /** Indices */
      indices?: number[] | null;
      /** Aggregates */
      aggregates?: Record<string, never> | null;
      /**
       * Aggregate Keys
       * @default [
       *       "average_instruction_adherence"
       *     ]
       */
      aggregate_keys?: string[];
      /** Extra */
      extra?: Record<string, never> | null;
      /** Sub Scorers */
      sub_scorers?: components['schemas']['ScorerName'][];
      /** Filters */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
      /** Metric Name */
      metric_name?: string | null;
      /** Description */
      description?: string | null;
      /** @default {
       *       "metric_system_prompt": "The user will provide you with a prompt that was sent to a chatbot system, and the chatbot's latest response. Both will be provided as JSON strings.\n\nIn some cases, the prompt may be split up into multiple messages. If so, each message will begin with one of the following prefixes:\n\n- \"System: \"\n- \"Human: \"\n- \"AI: \"\n\nIf you see these prefixes, pay attention to them because they indicate where messages begin and end. Messages prefixed with \"System: \" contain system instructions which the chatbot should follow. Messages prefixed with \"Human: \" are user input. Messages prefixed with \"AI: \" are system responses to user input.\nIf you do not see these prefixes, treat the prompt as though it was a single user input message prefixed with \"Human: \".\n\nYour task is to determine whether the latest response from the chatbot is consistent with the instructions provided in the system prompt (if there is one) or in the first user message (if there is no system prompt).\n\nFocus only on the latest response and the instructions. Do not consider the chat history or any previous messages from the chatbot.\n\nThink step by step, and explain your reasoning carefully.\nState your observations first, before drawing any conclusions.\n\nRespond in the following JSON format:\n\n```\n{\n    \"explanation\": string,\n    \"is_consistent\": boolean\n}\n```\n\n\"explanation\": Your step-by-step reasoning process. List out the relevant instructions and explain whether the latest response adheres to each of them.\n\n\"is_consistent\": `true` if the latest response is consistent with the instructions, `false` otherwise.\n\nYou must respond with a valid JSON string.",
       *       "metric_description": "I have a chatbot application.\nMy system prompt contains a list of instructions for what the chatbot should and should not do in every interaction. I want a metric that checks whether the latest response from the chatbot is consistent with the instructions.\n\nThe metric should only evaluate the latest message (the response), not the chat history. It should return false only if the latest message violates one or more instructions. Violations earlier in the chat history should not affect whether the value is true or false. The value should only depend on whether the latest message was consistent with the instructions, considered in context. The metric should only consider instructions that are applicable to the latest message.",
       *       "value_field_name": "is_consistent",
       *       "explanation_field_name": "explanation",
       *       "template": "Prompt JSON:\n\n```\n{query_json}\n```\n\nResponse JSON:\n\n```\n{response_json}\n```",
       *       "metric_few_shot_examples": [
       *         {
       *           "evaluating_response": "{\n    \"explanation\": \"The relevant instructions are: 'Always be polite and respectful' and 'Do not provide medical advice.'\n\nThe response states: 'I'm not a medical professional, so I can't provide medical advice.' This adheres to the instruction not to provide medical advice.\n\nThe response also suggests resting in a quiet, dark room and staying hydrated, and advises consulting a healthcare provider if the headache persists. These suggestions are general and do not constitute medical advice.\n\nThe tone of the response is polite and respectful.\n\nTherefore, the latest response is consistent with the instructions.\",\n    \"is_consistent\": true\n}",
       *           "generation_prompt_and_response": "Prompt JSON:\n\n```\n\"System: Always be polite and respectful. Do not provide medical advice.\nHuman: Can you tell me what to do if I have a headache?\"\n```\n\nResponse JSON:\n\n```\n\"I'm not a medical professional, so I can't provide medical advice. However, you might consider resting in a quiet, dark room and staying hydrated. If your headache persists, please consult a healthcare provider.\"\n```"
       *         }
       *       ]
       *     } */
      chainpoll_template?: components['schemas']['InstructionAdherenceTemplate'];
      /**
       * Regex Field
       * @default
       */
      regex_field?: string;
      /** Registered Scorer Id */
      registered_scorer_id?: string | null;
      /** Generated Scorer Id */
      generated_scorer_id?: string | null;
      /**
       * Function Explanation Param Name
       * @default explanation
       */
      function_explanation_param_name?: string;
    };
    /** CustomizedToolErrorRateGPTScorer */
    'CustomizedToolErrorRateGPTScorer-Input': {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      scorer_name: '_customized_tool_error_rate';
      /**
       * Model Alias
       * @default GPT-4o mini
       */
      model_alias?: string;
      /**
       * Num Judges
       * @default 1
       */
      num_judges?: number;
      /**
       * Name
       * @default tool_error_rate
       * @constant
       */
      name?: 'tool_error_rate';
      /** Scores */
      scores?: unknown[] | null;
      /** Indices */
      indices?: number[] | null;
      /** Aggregates */
      aggregates?: Record<string, never> | null;
      /**
       * Aggregate Keys
       * @default [
       *       "average_tool_error_rate"
       *     ]
       */
      aggregate_keys?: string[];
      /** Extra */
      extra?: Record<string, never> | null;
      /** Sub Scorers */
      sub_scorers?: components['schemas']['ScorerName'][];
      /** Filters */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
      /** Metric Name */
      metric_name?: string | null;
      /** Description */
      description?: string | null;
      /** @default {
       *       "metric_system_prompt": "One or more functions have been called, and you will receive their output. The output format could be a string containing the tool's result, it could be in JSON or XML format with additional metadata and information, or it could be a list of the outputs in any such format.\n\nYour task is to determine whether at least one function call didn't execute correctly and errored out. If at least one call failed, then you should consider the entire call as a failure. \nYou should NOT evaluate any other aspect of the tool call. In particular you should not evaluate whether the output is well formatted, coherent or contains spelling mistakes.\n\nIf you conclude that the call failed, provide an explanation as to why. You may summarize any error message you encounter. If the call was successful, no explanation is needed.\n\nRespond in the following JSON format:\n\n```\n{\n   \"function_errored_out\": boolean,\n   \"explanation\": string\n}\n```\n\n- **\"function_errored_out\"**: Use `false` if all tool calls were successful, and `true` if at least one errored out.\n\n- **\"explanation\"**: If a tool call failed, provide your step-by-step reasoning to determine why it might have failed. If all tool calls were succesful, leave this blank.\n\nYou must respond with a valid JSON object; don't forget to escape special characters.",
       *       "metric_description": "I have a multi-turn chatbot application where the assistant is an agent that has access to tools. I want a metric to evaluate whether a tool invocation was successful or if it resulted in an error.",
       *       "value_field_name": "function_errored_out",
       *       "explanation_field_name": "explanation",
       *       "template": "Tools output:\n```\n{response}\n```",
       *       "metric_few_shot_examples": [
       *         {
       *           "evaluating_response": "{\n  \"function_errored_out\": false,\n  \"explanation\": \"\"\n}",
       *           "generation_prompt_and_response": "Tools output:\n```\n0\n```"
       *         },
       *         {
       *           "evaluating_response": "{\n  \"function_errored_out\": true,\n  \"explanation\": \"The call failed due to an InvalidFunctionArgumentException.\",\n}",
       *           "generation_prompt_and_response": "Tools output:\n```\n{\n  \"error\": \"InvalidFunctionArgumentException\",\n  \"status_code\": 400\n}\n```"
       *         }
       *       ]
       *     } */
      chainpoll_template?: components['schemas']['ToolErrorRateTemplate'];
      /**
       * Regex Field
       * @default
       */
      regex_field?: string;
      /** Registered Scorer Id */
      registered_scorer_id?: string | null;
      /** Generated Scorer Id */
      generated_scorer_id?: string | null;
    };
    /** CustomizedToolErrorRateGPTScorer */
    'CustomizedToolErrorRateGPTScorer-Output': {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      scorer_name: '_customized_tool_error_rate';
      /**
       * Model Alias
       * @default GPT-4o mini
       */
      model_alias?: string;
      /**
       * Num Judges
       * @default 1
       */
      num_judges?: number;
      /**
       * Name
       * @default tool_error_rate
       * @constant
       */
      name?: 'tool_error_rate';
      /** Scores */
      scores?: unknown[] | null;
      /** Indices */
      indices?: number[] | null;
      /** Aggregates */
      aggregates?: Record<string, never> | null;
      /**
       * Aggregate Keys
       * @default [
       *       "average_tool_error_rate"
       *     ]
       */
      aggregate_keys?: string[];
      /** Extra */
      extra?: Record<string, never> | null;
      /** Sub Scorers */
      sub_scorers?: components['schemas']['ScorerName'][];
      /** Filters */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
      /** Metric Name */
      metric_name?: string | null;
      /** Description */
      description?: string | null;
      /** @default {
       *       "metric_system_prompt": "One or more functions have been called, and you will receive their output. The output format could be a string containing the tool's result, it could be in JSON or XML format with additional metadata and information, or it could be a list of the outputs in any such format.\n\nYour task is to determine whether at least one function call didn't execute correctly and errored out. If at least one call failed, then you should consider the entire call as a failure. \nYou should NOT evaluate any other aspect of the tool call. In particular you should not evaluate whether the output is well formatted, coherent or contains spelling mistakes.\n\nIf you conclude that the call failed, provide an explanation as to why. You may summarize any error message you encounter. If the call was successful, no explanation is needed.\n\nRespond in the following JSON format:\n\n```\n{\n   \"function_errored_out\": boolean,\n   \"explanation\": string\n}\n```\n\n- **\"function_errored_out\"**: Use `false` if all tool calls were successful, and `true` if at least one errored out.\n\n- **\"explanation\"**: If a tool call failed, provide your step-by-step reasoning to determine why it might have failed. If all tool calls were succesful, leave this blank.\n\nYou must respond with a valid JSON object; don't forget to escape special characters.",
       *       "metric_description": "I have a multi-turn chatbot application where the assistant is an agent that has access to tools. I want a metric to evaluate whether a tool invocation was successful or if it resulted in an error.",
       *       "value_field_name": "function_errored_out",
       *       "explanation_field_name": "explanation",
       *       "template": "Tools output:\n```\n{response}\n```",
       *       "metric_few_shot_examples": [
       *         {
       *           "evaluating_response": "{\n  \"function_errored_out\": false,\n  \"explanation\": \"\"\n}",
       *           "generation_prompt_and_response": "Tools output:\n```\n0\n```"
       *         },
       *         {
       *           "evaluating_response": "{\n  \"function_errored_out\": true,\n  \"explanation\": \"The call failed due to an InvalidFunctionArgumentException.\",\n}",
       *           "generation_prompt_and_response": "Tools output:\n```\n{\n  \"error\": \"InvalidFunctionArgumentException\",\n  \"status_code\": 400\n}\n```"
       *         }
       *       ]
       *     } */
      chainpoll_template?: components['schemas']['ToolErrorRateTemplate'];
      /**
       * Regex Field
       * @default
       */
      regex_field?: string;
      /** Registered Scorer Id */
      registered_scorer_id?: string | null;
      /** Generated Scorer Id */
      generated_scorer_id?: string | null;
    };
    /** CustomizedToolSelectionQualityGPTScorer */
    'CustomizedToolSelectionQualityGPTScorer-Input': {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      scorer_name: '_customized_tool_selection_quality';
      /**
       * Model Alias
       * @default GPT-4o mini
       */
      model_alias?: string;
      /**
       * Num Judges
       * @default 3
       */
      num_judges?: number;
      /**
       * Name
       * @default tool_selection_quality
       * @constant
       */
      name?: 'tool_selection_quality';
      /** Scores */
      scores?: unknown[] | null;
      /** Indices */
      indices?: number[] | null;
      /** Aggregates */
      aggregates?: Record<string, never> | null;
      /**
       * Aggregate Keys
       * @default [
       *       "average_tool_selection_quality"
       *     ]
       */
      aggregate_keys?: string[];
      /** Extra */
      extra?: Record<string, never> | null;
      /** Sub Scorers */
      sub_scorers?: components['schemas']['ScorerName'][];
      /** Filters */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
      /** Metric Name */
      metric_name?: string | null;
      /** Description */
      description?: string | null;
      /** @default {
       *       "metric_system_prompt": "You will receive the chat history from a chatbot application. At the end of the  conversation, it will be the bots turn to act. The bot has several options: it can reflect and plan its next steps, choose to call tools, or respond directly to the user. If the bot opts to use tools, the tools execute separately, and the bot will subsequently review the output from those tools. Ultimately, the bot should reply to the user, choosing the relevant parts of the tools' output.\n\nYour task is to evaluate the bot's decision-making process and ensure it follows these guidelines:\n- If all user queries have already been answered and can be found in the chat history, the bot should not call tools.\n- If no suitable tools are available to assist with user queries, the bot should not call tools.\n- If the chat history contains all the necessary information to directly answer all user queries, the bot should not call tools.\n- If the bot decided to call tools, the tools and argument values selected must relate to at least part of one user query.\n- If the bot decided to call tools, all arguments marked as \"required\" in the tools' schema must be provided with values.\n\nRemember that there are many ways the bot's actions can comply with these rules. Your role is to determine whether the bot fundamentally violated any of these rules, not whether it chose the most optimal response.\n\nRespond in the following JSON format:\n```\n{\n    \"explanation\": string,\n    \"bot_answer_follows_rules\": boolean\n}\n```\n\n- **\"explanation\"**: Provide your step-by-step reasoning to determine whether the bot's reply follows the above-mentioned guidelines.\n\n- **\"bot_answer_follows_rules\"**: Respond `true` if you believe the bot followed the above guidelines, respond `false` otherwise.\n\nYou must respond with a valid JSON object; don't forget to escape special characters.",
       *       "metric_description": "I have a multi-turn chatbot application where the assistant is an agent that has access to tools. I want a metric that assesses whether the assistant made the correct decision in choosing to either use tools or to directly respond, and in cases where it uses tools, whether it selected the correct tools with the correct arguments.",
       *       "value_field_name": "bot_answer_follows_rules",
       *       "explanation_field_name": "explanation",
       *       "template": "Chatbot history:\n```\n{query}\n```\n\nThe bot's available tools:\n```\n{tools}\n```\n\nThe answer to evaluate:\n```\n{response}\n```",
       *       "metric_few_shot_examples": [
       *         {
       *           "evaluating_response": "{\n    \"explanation\": \"The user asked if the genuses Sapium and Aristotelia belong to the same family. The bot decided to search for 'Sapium', which is associated with part of the user's query regarding the family of Sapium. The action aligns with the user's query, and all required arguments ('query') are provided, making the tool call valid. Furthermore, the information required to answer the user's question is not present in the chat history, justifying the bot's decision to call a tool.\",\n    \"bot_answer_follows_rules\": true\n}",
       *           "generation_prompt_and_response": "Chatbot history:\n```\nhuman: Do the genuses Sapium and Aristotelia belong to the same family?\n```\n\nThe bot's available tools:\n```\n[{'name': 'Search',\n  'description': 'Search for the query',\n  'parameters': {'type': 'object',\n   'properties': {'query': {'type': 'string',\n     'description': 'the query to search'}},\n   'required': ['query']}},\n {'name': 'Lookup',\n  'description': 'Lookup the keyword',\n  'parameters': {'type': 'object',\n   'properties': {'keyword': {'type': 'string',\n     'description': 'the keyword to lookup'}},\n   'required': ['keyword']}}]\n```\n\nThe answer to evaluate:\n```\n{\"Thought\": \"I need to search Sapium and Aristotelia, find their families, then find if they are the same.\", \"Action\": {\"name\": \"Search\", \"arguments\": {\"query\": \"Sapium\"}}}\n```"
       *         }
       *       ]
       *     } */
      chainpoll_template?: components['schemas']['ToolSelectionQualityTemplate'];
      /**
       * Regex Field
       * @default
       */
      regex_field?: string;
      /** Registered Scorer Id */
      registered_scorer_id?: string | null;
      /** Generated Scorer Id */
      generated_scorer_id?: string | null;
    };
    /** CustomizedToolSelectionQualityGPTScorer */
    'CustomizedToolSelectionQualityGPTScorer-Output': {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      scorer_name: '_customized_tool_selection_quality';
      /**
       * Model Alias
       * @default GPT-4o mini
       */
      model_alias?: string;
      /**
       * Num Judges
       * @default 3
       */
      num_judges?: number;
      /**
       * Name
       * @default tool_selection_quality
       * @constant
       */
      name?: 'tool_selection_quality';
      /** Scores */
      scores?: unknown[] | null;
      /** Indices */
      indices?: number[] | null;
      /** Aggregates */
      aggregates?: Record<string, never> | null;
      /**
       * Aggregate Keys
       * @default [
       *       "average_tool_selection_quality"
       *     ]
       */
      aggregate_keys?: string[];
      /** Extra */
      extra?: Record<string, never> | null;
      /** Sub Scorers */
      sub_scorers?: components['schemas']['ScorerName'][];
      /** Filters */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
      /** Metric Name */
      metric_name?: string | null;
      /** Description */
      description?: string | null;
      /** @default {
       *       "metric_system_prompt": "You will receive the chat history from a chatbot application. At the end of the  conversation, it will be the bots turn to act. The bot has several options: it can reflect and plan its next steps, choose to call tools, or respond directly to the user. If the bot opts to use tools, the tools execute separately, and the bot will subsequently review the output from those tools. Ultimately, the bot should reply to the user, choosing the relevant parts of the tools' output.\n\nYour task is to evaluate the bot's decision-making process and ensure it follows these guidelines:\n- If all user queries have already been answered and can be found in the chat history, the bot should not call tools.\n- If no suitable tools are available to assist with user queries, the bot should not call tools.\n- If the chat history contains all the necessary information to directly answer all user queries, the bot should not call tools.\n- If the bot decided to call tools, the tools and argument values selected must relate to at least part of one user query.\n- If the bot decided to call tools, all arguments marked as \"required\" in the tools' schema must be provided with values.\n\nRemember that there are many ways the bot's actions can comply with these rules. Your role is to determine whether the bot fundamentally violated any of these rules, not whether it chose the most optimal response.\n\nRespond in the following JSON format:\n```\n{\n    \"explanation\": string,\n    \"bot_answer_follows_rules\": boolean\n}\n```\n\n- **\"explanation\"**: Provide your step-by-step reasoning to determine whether the bot's reply follows the above-mentioned guidelines.\n\n- **\"bot_answer_follows_rules\"**: Respond `true` if you believe the bot followed the above guidelines, respond `false` otherwise.\n\nYou must respond with a valid JSON object; don't forget to escape special characters.",
       *       "metric_description": "I have a multi-turn chatbot application where the assistant is an agent that has access to tools. I want a metric that assesses whether the assistant made the correct decision in choosing to either use tools or to directly respond, and in cases where it uses tools, whether it selected the correct tools with the correct arguments.",
       *       "value_field_name": "bot_answer_follows_rules",
       *       "explanation_field_name": "explanation",
       *       "template": "Chatbot history:\n```\n{query}\n```\n\nThe bot's available tools:\n```\n{tools}\n```\n\nThe answer to evaluate:\n```\n{response}\n```",
       *       "metric_few_shot_examples": [
       *         {
       *           "evaluating_response": "{\n    \"explanation\": \"The user asked if the genuses Sapium and Aristotelia belong to the same family. The bot decided to search for 'Sapium', which is associated with part of the user's query regarding the family of Sapium. The action aligns with the user's query, and all required arguments ('query') are provided, making the tool call valid. Furthermore, the information required to answer the user's question is not present in the chat history, justifying the bot's decision to call a tool.\",\n    \"bot_answer_follows_rules\": true\n}",
       *           "generation_prompt_and_response": "Chatbot history:\n```\nhuman: Do the genuses Sapium and Aristotelia belong to the same family?\n```\n\nThe bot's available tools:\n```\n[{'name': 'Search',\n  'description': 'Search for the query',\n  'parameters': {'type': 'object',\n   'properties': {'query': {'type': 'string',\n     'description': 'the query to search'}},\n   'required': ['query']}},\n {'name': 'Lookup',\n  'description': 'Lookup the keyword',\n  'parameters': {'type': 'object',\n   'properties': {'keyword': {'type': 'string',\n     'description': 'the keyword to lookup'}},\n   'required': ['keyword']}}]\n```\n\nThe answer to evaluate:\n```\n{\"Thought\": \"I need to search Sapium and Aristotelia, find their families, then find if they are the same.\", \"Action\": {\"name\": \"Search\", \"arguments\": {\"query\": \"Sapium\"}}}\n```"
       *         }
       *       ]
       *     } */
      chainpoll_template?: components['schemas']['ToolSelectionQualityTemplate'];
      /**
       * Regex Field
       * @default
       */
      regex_field?: string;
      /** Registered Scorer Id */
      registered_scorer_id?: string | null;
      /** Generated Scorer Id */
      generated_scorer_id?: string | null;
    };
    /**
     * DataTypeOptions
     * @enum {string}
     */
    DataTypeOptions:
      | 'unknown'
      | 'text'
      | 'label'
      | 'floating_point'
      | 'integer'
      | 'timestamp'
      | 'milli_seconds'
      | 'boolean'
      | 'uuid'
      | 'percentage'
      | 'dollars'
      | 'array'
      | 'template_label'
      | 'thumb_rating_percentage'
      | 'user_id'
      | 'text_offsets'
      | 'segments'
      | 'hallucination_segments'
      | 'thumb_rating'
      | 'score_rating'
      | 'star_rating'
      | 'tags_rating'
      | 'thumb_rating_aggregate'
      | 'score_rating_aggregate'
      | 'star_rating_aggregate'
      | 'tags_rating_aggregate';
    /**
     * DatasetAction
     * @enum {string}
     */
    DatasetAction: 'update' | 'delete' | 'share' | 'export' | 'rename';
    /** DatasetAddColumn */
    DatasetAddColumn: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      edit_type: 'add_column';
      /** New Column Name */
      new_column_name: string;
      /** Column Values */
      column_values: (string | number | null)[];
    };
    /** DatasetAppendRow */
    DatasetAppendRow: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      edit_type: 'append_row';
      /** Values */
      values: {
        [key: string]: string | number | null;
      };
      /** Row Id */
      row_id?: string | null;
    };
    /** DatasetContent */
    DatasetContent: {
      /**
       * Starting Token
       * @default 0
       */
      starting_token?: number;
      /**
       * Limit
       * @default 100
       */
      limit?: number;
      /**
       * Paginated
       * @default false
       */
      paginated?: boolean;
      /** Next Starting Token */
      next_starting_token?: number | null;
      /** Column Names */
      column_names?: string[];
      /** Rows */
      rows?: components['schemas']['DatasetRow'][];
    };
    /** DatasetContentFilter */
    DatasetContentFilter: {
      /** Column Name */
      column_name: string;
      /** Value */
      value: string;
      /** @default contains */
      operator?: components['schemas']['DatasetContentFilterOperator'];
    };
    /**
     * DatasetContentFilterOperator
     * @enum {string}
     */
    DatasetContentFilterOperator: 'eq' | 'ne' | 'contains';
    /** DatasetContentSortClause */
    DatasetContentSortClause: {
      /** Column Name */
      column_name: string;
      /**
       * Ascending
       * @default true
       */
      ascending?: boolean;
    };
    /** DatasetCreatedAtSort */
    DatasetCreatedAtSort: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'created_at';
      /**
       * Ascending
       * @default true
       */
      ascending?: boolean;
      /**
       * Sort Type
       * @default column
       * @constant
       */
      sort_type?: 'column';
    };
    /** DatasetDB */
    DatasetDB: {
      /**
       * Id
       * Format: uuid4
       */
      id: string;
      /**
       * Permissions
       * @default []
       */
      permissions?: components['schemas']['Permission'][];
      /** Name */
      name: string;
      /**
       * Created At
       * Format: date-time
       */
      created_at: string;
      /**
       * Updated At
       * Format: date-time
       */
      updated_at: string;
      /** Project Count */
      project_count: number;
      /** Num Rows */
      num_rows: number | null;
      /** Column Names */
      column_names: string[] | null;
      created_by_user: components['schemas']['UserInfo'] | null;
      /** Current Version Index */
      current_version_index: number;
      /** Draft */
      draft: boolean;
    };
    /** DatasetDeleteColumn */
    DatasetDeleteColumn: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      edit_type: 'delete_column';
      /** Column Name */
      column_name: string;
    };
    /** DatasetDeleteRow */
    DatasetDeleteRow: {
      /** Index */
      index?: number | null;
      /** Row Id */
      row_id?: string | null;
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      edit_type: 'delete_row';
    };
    /** DatasetDraftFilter */
    DatasetDraftFilter: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'draft';
      /** Value */
      value: boolean;
      /**
       * Operator
       * @enum {string}
       */
      operator: 'eq' | 'ne';
    };
    /**
     * DatasetFormat
     * @enum {string}
     */
    DatasetFormat: 'csv' | 'feather' | 'jsonl';
    /** DatasetLastEditedByUserAtSort */
    DatasetLastEditedByUserAtSort: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'last_edited_by_user_at';
      /**
       * Ascending
       * @default true
       */
      ascending?: boolean;
      /**
       * Sort Type
       * @default custom_uuid
       * @constant
       */
      sort_type?: 'custom_uuid';
      /**
       * Value
       * Format: uuid4
       */
      value: string;
    };
    /** DatasetNameFilter */
    DatasetNameFilter: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'name';
      /**
       * Operator
       * @enum {string}
       */
      operator: 'eq' | 'ne' | 'contains' | 'one_of' | 'not_in';
      /** Value */
      value: string | string[];
      /**
       * Case Sensitive
       * @default true
       */
      case_sensitive?: boolean;
    };
    /** DatasetNameSort */
    DatasetNameSort: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'name';
      /**
       * Ascending
       * @default true
       */
      ascending?: boolean;
      /**
       * Sort Type
       * @default column
       * @constant
       */
      sort_type?: 'column';
    };
    /** DatasetProjectLastUsedAtSort */
    DatasetProjectLastUsedAtSort: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'project_last_used_at';
      /**
       * Ascending
       * @default true
       */
      ascending?: boolean;
      /**
       * Sort Type
       * @default custom_uuid
       * @constant
       */
      sort_type?: 'custom_uuid';
      /**
       * Value
       * Format: uuid4
       */
      value: string;
    };
    /** DatasetProjectsSort */
    DatasetProjectsSort: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'project_count';
      /**
       * Ascending
       * @default true
       */
      ascending?: boolean;
      /**
       * Sort Type
       * @default custom
       * @constant
       */
      sort_type?: 'custom';
    };
    /** DatasetRenameColumn */
    DatasetRenameColumn: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      edit_type: 'rename_column';
      /** Column Name */
      column_name: string;
      /** New Column Name */
      new_column_name: string;
    };
    /** DatasetRow */
    DatasetRow: {
      /**
       * Row Id
       * Format: uuid4
       */
      row_id: string;
      /** Index */
      index: number;
      /** Values */
      values: (string | number | null)[];
      /** Values Dict */
      values_dict: {
        [key: string]: string | number | null;
      };
      metadata: components['schemas']['DatasetRowMetadata'] | null;
    };
    /** DatasetRowMetadata */
    DatasetRowMetadata: {
      /** Created In Version */
      created_in_version: number;
      /**
       * Created At
       * Format: date-time
       */
      created_at: string;
      created_by_user: components['schemas']['UserInfo'] | null;
      /** Updated In Version */
      updated_in_version: number;
      /**
       * Updated At
       * Format: date-time
       */
      updated_at: string;
      updated_by_user: components['schemas']['UserInfo'] | null;
    };
    /** DatasetRowsSort */
    DatasetRowsSort: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'num_rows';
      /**
       * Ascending
       * @default true
       */
      ascending?: boolean;
      /**
       * Sort Type
       * @default column
       * @constant
       */
      sort_type?: 'column';
    };
    /** DatasetUpdateRow */
    DatasetUpdateRow: {
      /** Index */
      index?: number | null;
      /** Row Id */
      row_id?: string | null;
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      edit_type: 'update_row';
      /** Values */
      values: {
        [key: string]: string | number | null;
      };
    };
    /** DatasetUpdatedAtSort */
    DatasetUpdatedAtSort: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'updated_at';
      /**
       * Ascending
       * @default true
       */
      ascending?: boolean;
      /**
       * Sort Type
       * @default column
       * @constant
       */
      sort_type?: 'column';
    };
    /** DatasetUsedInProjectFilter */
    DatasetUsedInProjectFilter: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'used_in_project';
      /**
       * Value
       * Format: uuid4
       */
      value: string;
    };
    /** DatasetVersionDB */
    DatasetVersionDB: {
      /** Version Index */
      version_index: number;
      /** Name */
      name: string | null;
      /**
       * Created At
       * Format: date-time
       */
      created_at: string;
      created_by_user: components['schemas']['UserInfo'] | null;
      /** Num Rows */
      num_rows: number;
      /** Column Names */
      column_names: string[];
      /** Rows Added */
      rows_added: number;
      /** Rows Removed */
      rows_removed: number;
      /** Rows Edited */
      rows_edited: number;
      /** Columns Added */
      columns_added: number;
      /** Columns Removed */
      columns_removed: number;
      /** Columns Renamed */
      columns_renamed: number;
    };
    /** DatasetVersionIndexSort */
    DatasetVersionIndexSort: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'version_index';
      /**
       * Ascending
       * @default true
       */
      ascending?: boolean;
      /**
       * Sort Type
       * @default column
       * @constant
       */
      sort_type?: 'column';
    };
    /** DeletePromptResponse */
    DeletePromptResponse: {
      /** Message */
      message: string;
    };
    /** Document */
    'Document-Input': {
      /**
       * Page Content
       * @description Content of the document.
       */
      page_content: string;
      /** Metadata */
      metadata?: {
        [key: string]: boolean | string | number;
      };
    };
    /** Document */
    'Document-Output': {
      /**
       * Content
       * @description Content of the document.
       */
      content: string;
      /** Metadata */
      metadata?: {
        [key: string]: boolean | string | number;
      };
    };
    /** ExperimentCreateRequest */
    ExperimentCreateRequest: {
      /** Name */
      name: string;
      /**
       * Task Type
       * @default 16
       */
      task_type?: 16 | 17;
    };
    /** ExperimentResponse */
    ExperimentResponse: {
      /**
       * Id
       * Format: uuid4
       */
      id: string;
      /**
       * Created At
       * Format: date-time
       */
      created_at: string;
      /**
       * Updated At
       * Format: date-time
       */
      updated_at: string;
      /** Name */
      name: string;
      /**
       * Project Id
       * Format: uuid4
       */
      project_id: string;
      /** Created By */
      created_by?: string | null;
      task_type: components['schemas']['TaskType'];
    };
    /** ExperimentUpdateRequest */
    ExperimentUpdateRequest: {
      /** Name */
      name: string;
      /**
       * Task Type
       * @default 16
       */
      task_type?: 16 | 17;
    };
    /** FactualityTemplate */
    FactualityTemplate: {
      /**
       * Metric System Prompt
       * @default # Task
       *
       *     You will be given a prompt that was sent to a large language model (LLM), and the LLM's response. Your task is to assess whether the response is factually correct.
       *
       *     ## Task output format
       *
       *     You must respond in the following JSON format:
       *
       *     ```
       *     {
       *         "explanation": string
       *         "was_factual": boolean
       *     }
       *     ```
       *
       *     "explanation": Your step-by-step reasoning process. List out the claims made in the response, and for each claim, provide a detailed explanation of why that claim is or is not factual.
       *
       *     "was_factual": `true` if the response was completely factually correct according to the instructions above, `false` otherwise.
       *
       *     You must respond with a valid JSON string.
       *
       *     ## Task guidelines
       *
       *     ### Input format
       *
       *     In some cases, the prompt may include multiple messages of chat history. If so, each message will begin with one of the following prefixes:
       *
       *     - "System: "
       *     - "Human: "
       *     - "AI: "
       *
       *     ### How to determine the value of `was_factual`
       *
       *     - was_factual should be false if anything in the response is factually incorrect, and true otherwise.
       *     - If the response omits some useful information, but does not include any falsehoods, was_factual should be true.
       *     - The prompt itself may contain false information. If the response repeats this false information, was_factual should be false. In other words, do not assume that the prompt is factually correct when evaluating the response.
       *     - If the prompt and response involve a domain where the concept of "factual accuracy" doesn't strictly apply, assess whatever quality of the response is most intuitively similar to factual accuracy. For example, if the prompt asks the LLM to write code, assess whether the code is free of syntax errors and implements the intended logic.
       *
       *     ### Writing the explanation
       *
       *     - As stated above, a typical explanation should list out the claims made in the response, and for each claim, provide a detailed explanation of why that claim is or is not factual.
       *     - If the response doesn't make claims per se, break down the response into constituent parts in the most natural way given its content. For example, in code generation tasks, you might break down the response into individual functions or lines of code.
       *     - Work step by step, and do not give an overall assessment of the response until the end of your explanation.
       */
      metric_system_prompt?: string;
      /**
       * Metric Description
       * @description Description of what the metric should do.
       */
      metric_description?: string | null;
      /**
       * Value Field Name
       * @default was_factual
       */
      value_field_name?: string;
      /**
       * Explanation Field Name
       * @description Field name to look for in the chainpoll response, for the explanation.
       * @default explanation
       */
      explanation_field_name?: string;
      /**
       * Template
       * @default The prompt was:
       *
       *     ```
       *     {query}
       *     ```
       *
       *     The response was:
       *
       *     ```
       *     {response}
       *     ```
       *
       *     Respond with a JSON object having two fields: `explanation` (string) and `was_factual` (boolean). Everything in your response should be valid JSON.
       *
       *     REMEMBER: if the prompt asks the LLM to compose an answer on the basis of a "context" or other reference text or texts, you MUST IGNORE these texts when evaluating the response. Evaluate the response as though the reference texts were NOT provided. Do NOT refer to these texts in your evaluation.
       */
      template?: string;
      /**
       * Metric Few Shot Examples
       * @default [
       *       {
       *         "generation_prompt_and_response": "The prompt was:\n\n```\nI have heard people talking about the chess move \"en passant\", but I don't know what it is and how it works. Could you explain it to me?\n```\n\nThe response was:\n\n```\nCertainly! \"En passant\" is a special rule in chess that allows a pawn to capture an enemy pawn that has moved two squares forward from its initial position, as if it had only moved one square forward. This can only be done on the ensuing move, and only if the capturing pawn was on its fifth rank at the time the enemy pawn made its double-move. Here's an example to illustrate:\n\n1. e4 e5\n2. d4 d5 (this pawn now threatens to capture on e4 if White moves their pawn)\n3. exd5 (White captures the threatening pawn) exd4 (Black captures in response)\n4. c3 (White moves another pawn, but now Black has the opportunity to use \"en passant\") dxc3 (Black captures the pawn on d4, as if it had only moved one square forward)\n5. Nxc3\n\nI hope that helps!\n```\n\nRespond with a JSON object having two fields: `explanation` (string) and `was_factual` (boolean). Everything in your response should be valid JSON.",
       *         "evaluating_response": "{\n    \"explanation\": \"The response makes several claims about the meaning of \\\"en passant\\\" in chess, then provides an illustrative example. Let's evaluate the claims first, then the example.\\n\\n# Claims\\n\\n1. The response claims that 'en passant' is a special rule in chess that allows a pawn to capture an enemy pawn that has moved two squares forward from its initial position, as if it had only moved one square forward. This claim is factually correct, as 'en passant' indeed allows for such a capture under specified conditions.\\n\\n2. The response also claims that this capture can only be done on the ensuing move and only if the capturing pawn was on its fifth rank at the time. This is also factually correct, as one of the requirements for the 'en passant' capture is that the capturing pawn must be on the fifth rank when the opponent's pawn moves two squares forward.\\n\\n# Illustrative Example\\n\\nFor the purposes of this metric, we'll say that a chess example like this one is \\\"correct\\\" if\\n\\n- All the moves are valid according to the rules of chess\\n- The example demonstrates what it is intended to demonstrate (in this case, the en passant rule)\\n\\nTo check these conditions, let's waslk through the example one move at a time, with a clear description of the board state after each move.\\n\\n### Initial Position\\nThe game starts with the standard initial setup of the chessboard.\\n\\n### Move 1\\n**1. e4 e5**\\n\\n- **White moves pawn from e2 to e4.**\\n  This opens lines for the bishop and queen and controls the center.\\n- **Black moves pawn from e7 to e5.**\\n  This mirrors White's move, also controlling the center.\\n\\n**Board State:**\\n```\\nr n b q k b n r\\np p p     p p p\\n\\n        P\\n        p\\n\\nP P P     P P P\\nR N B Q K B N R\\n```\\n\\n### Move 2\\n**2. d4 d5**\\n\\n- **White moves pawn from d2 to d4.**\\n  Now, White controls more central space and threatens the pawn on e5.\\n- **Black moves pawn from d7 to d5.**\\n  Black counters by challenging the center with another pawn.\\n\\n**Board State:**\\n```\\nr n b q k b n r\\np p   p   p p p\\n\\n        P\\n  p P p P\\n\\nP   P     P P P\\nR N B Q K B N R\\n```\\n\\n### Move 3\\n**3. exd5 exd4**\\n\\n- **White captures the pawn on d5 with the pawn from e4 (exd5).**\\n  Now White has a pawn on d5.\\n- **Black incorrectly moves to exd4.**\\n  According to the rules of chess, this is an illegal move because the pawn on e5 cannot capture a pawn on d4 directly. The pawn on d4 does not exist; the move should capture diagonally if there is a piece or pawn on d4.\\n\\nSince there is an error in Move 3, the illustrative example is not factually correct according to the criteria listed above. Therefore, the example as a whole is not entirely factually correct.\",\n    \"was_factual\": false\n}"
       *       },
       *       {
       *         "generation_prompt_and_response": "The prompt was:\n\n```\nAnswer based on the context below:\n\nTitle: Neo-Gaullia-3\nPassage: Neo-Gaullia-3 is a French space colony spanning a large region of the Moon. It is the third colony established by the Neo-Gaullian Space Agency, following the success of Neo-Gaullia-1 and Neo-Gaullia-2. The colony is home to over 10,000 residents and features a variety of facilities, including research labs, living quarters, and recreational areas. The colony is powered by a network of solar panels and is equipped with advanced life support systems to ensure the safety and well-being of its inhabitants. Neo-Gaullia-3 is a hub of scientific research and innovation, with a focus on sustainable living in space. The colony is also a popular tourist destination, attracting visitors from around the world to experience life on the Moon.\n\nQuestion: How many French space colonies have existed on the Moon?\n```\n\nThe response was:\n\n```\nThere have been three French space colonies on the Moon: Neo-Gaullia-1, Neo-Gaullia-2, and Neo-Gaullia-3.\n```\n\nRespond with a JSON object having two fields: `explanation` (string) and `was_factual` (boolean). Everything in your response should be valid JSON.",
       *         "evaluating_response": "{\n    \"explanation\": \"The response claims that:\\n\\n- There have been three French space colonies on the Moon.\\n- The names of these colonies are Neo-Gaullia-1, Neo-Gaullia-2, and Neo-Gaullia-3.\\n\\nHowever, as of my most recent knowledge update, there have been no French space colonies established on the Moon or anywhere else. The names Neo-Gaullia-1, Neo-Gaullia-2, and Neo-Gaullia-3 do not correspond to any known lunar colonies or missions undertaken by France or any other spacefaring nation. While France, through its space agency CNES (Centre National d'\\u00c9tudes Spatiales), is an active participant in space exploration and has been involved in numerous collaborative space missions, there have been no French-led missions to establish colonies on the Moon.\",\n    \"was_factual\": false\n}"
       *       }
       *     ]
       */
      metric_few_shot_examples?: components['schemas']['FewShotExample'][];
    };
    /** FeedbackAggregate */
    FeedbackAggregate: {
      /** Aggregate */
      aggregate:
        | components['schemas']['LikeDislikeAggregate']
        | components['schemas']['StarAggregate']
        | components['schemas']['ScoreAggregate']
        | components['schemas']['TagsAggregate'];
    };
    /** FeedbackRatingDB */
    FeedbackRatingDB: {
      /** Explanation */
      explanation?: string | null;
      /** Rating */
      rating:
        | components['schemas']['LikeDislikeRating']
        | components['schemas']['StarRating']
        | components['schemas']['ScoreRating']
        | components['schemas']['TagsRating']
        | components['schemas']['TextRating'];
      /**
       * Created At
       * Format: date-time
       */
      created_at: string;
      /** Created By */
      created_by: string | null;
    };
    /**
     * FewShotExample
     * @description Few-shot example for a chainpoll metric prompt.
     */
    FewShotExample: {
      /** Generation Prompt And Response */
      generation_prompt_and_response: string;
      /** Evaluating Response */
      evaluating_response: string;
    };
    /**
     * GeneratedScorerAction
     * @enum {string}
     */
    GeneratedScorerAction: 'update' | 'delete';
    /** GetProjectsPaginatedResponse */
    GetProjectsPaginatedResponse: {
      /**
       * Starting Token
       * @default 0
       */
      starting_token?: number;
      /**
       * Limit
       * @default 100
       */
      limit?: number;
      /**
       * Paginated
       * @default false
       */
      paginated?: boolean;
      /** Next Starting Token */
      next_starting_token?: number | null;
      /** Projects */
      projects: components['schemas']['ProjectDB'][];
    };
    /** GroundTruthAdherenceScorer */
    GroundTruthAdherenceScorer: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'ground_truth_adherence';
      /**
       * Filters
       * @description List of filters to apply to the scorer.
       */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
      /**
       * Type
       * @default plus
       * @constant
       */
      type?: 'plus';
      /**
       * Model Name
       * @description Alias of the model to use for the scorer.
       */
      model_name?: string | null;
      /**
       * Num Judges
       * @description Number of judges for the scorer.
       */
      num_judges?: number | null;
    };
    /** GroundTruthAdherenceTemplate */
    GroundTruthAdherenceTemplate: {
      /**
       * Metric System Prompt
       * @default I will give you two different texts, called the "ground truth" and the "response."
       *
       *     Read both texts, then tell me whether they are "equivalent," in the sense that they basically mean the same thing.
       *
       *     Keep the following guidelines in mind.
       *
       *     - Two texts can be equivalent if they use different phrasing, as long as the phrasing doesn't affect meaning.
       *     - Two texts can be equivalent if there are _slight_ differences in meaning that wouldn't affect the conclusions that a reasonable reader would draw upon reading them.
       *     - Imagine that you are grading a free-response exam.  The ground truth given in the answer key for an exam question, and the response is a student's answer to the same question. If you would give the student full marks for this question, that means the two texts are equivalent. If you wouldn't, that means the two texts are not equivalent.
       *
       *     Respond in the following JSON format:
       *
       *     ```
       *     {{
       *         "explanation": string,
       *         "equivalent": boolean
       *     }}
       *     ```
       *
       *     "explanation": A step-by-step breakdown of the similarities and differences between the text. For each difference you note (if any), consider why the difference might or might not make the texts non-equivalent, note down your reasoning clearly and explicitly, and ultimately draw a conclusion about whether that difference makes the text non-equivalent.
       *
       *     "equivalent": `true` if the texts are equivalent in the sense given above, `false` if they are non-equivalent.
       *
       *     You must respond with valid JSON.
       */
      metric_system_prompt?: string;
      /**
       * Metric Description
       * @default This metric computes whether a response from a large language model matches a provided ground truth text.
       */
      metric_description?: string;
      /**
       * Value Field Name
       * @default equivalent
       */
      value_field_name?: string;
      /**
       * Explanation Field Name
       * @description Field name to look for in the chainpoll response, for the explanation.
       * @default explanation
       */
      explanation_field_name?: string;
      /**
       * Template
       * @default Ground truth:
       *
       *     ```
       *     {ground_truth}
       *     ```
       *
       *     Response:
       *
       *     ```
       *     {response}
       *     ```
       */
      template?: string;
      /**
       * Metric Few Shot Examples
       * @description Few-shot examples for the metric.
       */
      metric_few_shot_examples?: components['schemas']['FewShotExample'][];
    };
    /**
     * GroundednessTemplate
     * @description Template for the groundedness metric,
     *     containing all the info necessary to send the groundedness prompt.
     */
    GroundednessTemplate: {
      /**
       * Metric System Prompt
       * @default The user will provide you with a prompt that was sent to an automatic question-answering system, and that system's response. Both will be provided as JSON strings.
       *
       *     The prompt will contain one or more documents intended as context which the question-answering system was given as reference material.
       *
       *     Your task is to determine whether the answer was supported by the documents.
       *
       *     Think step by step, and explain your reasoning carefully.
       *     State your observations first, before drawing any conclusions.
       *
       *     Respond in the following JSON format:
       *
       *     ```
       *     {
       *         "explanation": string,
       *         "was_supported": boolean
       *     }
       *     ```
       *
       *     "explanation": Your step-by-step reasoning process. List out the claims made in the response, and for each claim, provide a detailed explanation of why that claim is or is not supported by the documents.
       *
       *     "was_supported": `true` if the response was supported by the documents, `false` otherwise.
       *
       *     You must respond with valid JSON.
       */
      metric_system_prompt?: string;
      /**
       * Metric Description
       * @default I have a RAG (retrieval-augmented generation) system that generates text based on one or more documents that I always include in my prompts. I want a metric that checks whether the generated text was supported by information in the documents. The metric should exhaustively check each claim in the response against the documents, one by one, listing them out explicitly.
       */
      metric_description?: string;
      /**
       * Value Field Name
       * @default was_supported
       */
      value_field_name?: string;
      /**
       * Explanation Field Name
       * @description Field name to look for in the chainpoll response, for the explanation.
       * @default explanation
       */
      explanation_field_name?: string;
      /**
       * Template
       * @default Prompt JSON:
       *
       *     ```
       *     {query_json}
       *     ```
       *
       *     Response JSON:
       *
       *     ```
       *     {response_json}
       *     ```
       */
      template?: string;
      /**
       * Metric Few Shot Examples
       * @default [
       *       {
       *         "generation_prompt_and_response": "Prompt JSON:\n\n```\n\"Context: In the city, the population was distributed as 28.3% under the age of 18, 11.3% from 18 to 24, 32.7% from 25 to 44, 18.2% from 45 to 64, and 9.6% who were 65 years of age or older. The median age was 31 years. For every 100 females, there were 97.3 males. For every 100 females age 18 and over, there were 94.5 males.\\n\\nQuestion: How many in percent weren't 65 years of age or older?\"\n```\n\nResponse JSON:\n\n```\n\"90.4%\"\n```",
       *         "evaluating_response": "{\n    \"explanation\": \"This response makes a single claim: that 90.4% of the population is not 65 years of age or older.\\n\\nAccording to the provided document, 9.6% of the population is 65 years of age or older. To find the percentage of people who are not 65 years of age or older, we subtract this percentage from 100%:\\n100% - 9.6% = 90.4%\\n\\nThis matches the percentage in the claim, so the claim is supported.\\n\\nSince this is the only claim made in the response, and it was supported, the response is supported as a whole.\",\n    \"was_supported\": true\n}"
       *       }
       *     ]
       */
      metric_few_shot_examples?: components['schemas']['FewShotExample'][];
    };
    /**
     * GroupAction
     * @enum {string}
     */
    GroupAction: 'update' | 'list_members' | 'join' | 'request_to_join';
    /** GroupCollaborator */
    GroupCollaborator: {
      /**
       * Id
       * Format: uuid4
       */
      id: string;
      /**
       * Permissions
       * @default []
       */
      permissions?: components['schemas']['Permission'][];
      role: components['schemas']['CollaboratorRole'];
      /**
       * Created At
       * Format: date-time
       */
      created_at: string;
      /**
       * Group Id
       * Format: uuid4
       */
      group_id: string;
      /** Group Name */
      group_name: string;
    };
    /** GroupCollaboratorCreate */
    GroupCollaboratorCreate: {
      /** @default viewer */
      role?: components['schemas']['CollaboratorRole'];
      /**
       * Group Id
       * Format: uuid4
       */
      group_id: string;
    };
    /**
     * GroupMemberAction
     * @enum {string}
     */
    GroupMemberAction: 'update_role' | 'delete';
    /** HTTPValidationError */
    HTTPValidationError: {
      /** Detail */
      detail?: components['schemas']['ValidationError'][];
    };
    /** HallucinationSegment */
    HallucinationSegment: {
      /**
       * Hallucination Severity
       * @default 0
       */
      hallucination_severity?: number;
      /** Start */
      start: number;
      /** End */
      end: number;
      /** Hallucination */
      hallucination: number;
    };
    /** HealthcheckResponse */
    HealthcheckResponse: {
      /** Api Version */
      api_version: string;
      /** Message */
      message: string;
      /** Version */
      version: string;
    };
    /** InputMap */
    InputMap: {
      /** Prompt */
      prompt: string;
      /**
       * Prefix
       * @default
       */
      prefix?: string;
      /**
       * Suffix
       * @default
       */
      suffix?: string;
    };
    /** InputPIIScorer */
    InputPIIScorer: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'input_pii';
      /**
       * Filters
       * @description List of filters to apply to the scorer.
       */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
    };
    /** InputSexistScorer */
    InputSexistScorer: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'input_sexist';
      /**
       * Filters
       * @description List of filters to apply to the scorer.
       */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
    };
    /** InputToneScorer */
    InputToneScorer: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'input_tone';
      /**
       * Filters
       * @description List of filters to apply to the scorer.
       */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
    };
    /** InputToxicityScorer */
    InputToxicityScorer: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'input_toxicity';
      /**
       * Filters
       * @description List of filters to apply to the scorer.
       */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
    };
    /** InstructionAdherenceScorer */
    InstructionAdherenceScorer: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'instruction_adherence';
      /**
       * Filters
       * @description List of filters to apply to the scorer.
       */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
      /**
       * Type
       * @default plus
       * @constant
       */
      type?: 'plus';
      /**
       * Model Name
       * @description Alias of the model to use for the scorer.
       */
      model_name?: string | null;
      /**
       * Num Judges
       * @description Number of judges for the scorer.
       */
      num_judges?: number | null;
    };
    /** InstructionAdherenceTemplate */
    InstructionAdherenceTemplate: {
      /**
       * Metric System Prompt
       * @default The user will provide you with a prompt that was sent to a chatbot system, and the chatbot's latest response. Both will be provided as JSON strings.
       *
       *     In some cases, the prompt may be split up into multiple messages. If so, each message will begin with one of the following prefixes:
       *
       *     - "System: "
       *     - "Human: "
       *     - "AI: "
       *
       *     If you see these prefixes, pay attention to them because they indicate where messages begin and end. Messages prefixed with "System: " contain system instructions which the chatbot should follow. Messages prefixed with "Human: " are user input. Messages prefixed with "AI: " are system responses to user input.
       *     If you do not see these prefixes, treat the prompt as though it was a single user input message prefixed with "Human: ".
       *
       *     Your task is to determine whether the latest response from the chatbot is consistent with the instructions provided in the system prompt (if there is one) or in the first user message (if there is no system prompt).
       *
       *     Focus only on the latest response and the instructions. Do not consider the chat history or any previous messages from the chatbot.
       *
       *     Think step by step, and explain your reasoning carefully.
       *     State your observations first, before drawing any conclusions.
       *
       *     Respond in the following JSON format:
       *
       *     ```
       *     {
       *         "explanation": string,
       *         "is_consistent": boolean
       *     }
       *     ```
       *
       *     "explanation": Your step-by-step reasoning process. List out the relevant instructions and explain whether the latest response adheres to each of them.
       *
       *     "is_consistent": `true` if the latest response is consistent with the instructions, `false` otherwise.
       *
       *     You must respond with a valid JSON string.
       */
      metric_system_prompt?: string;
      /**
       * Metric Description
       * @default I have a chatbot application.
       *     My system prompt contains a list of instructions for what the chatbot should and should not do in every interaction. I want a metric that checks whether the latest response from the chatbot is consistent with the instructions.
       *
       *     The metric should only evaluate the latest message (the response), not the chat history. It should return false only if the latest message violates one or more instructions. Violations earlier in the chat history should not affect whether the value is true or false. The value should only depend on whether the latest message was consistent with the instructions, considered in context. The metric should only consider instructions that are applicable to the latest message.
       */
      metric_description?: string;
      /**
       * Value Field Name
       * @default is_consistent
       */
      value_field_name?: string;
      /**
       * Explanation Field Name
       * @description Field name to look for in the chainpoll response, for the explanation.
       * @default explanation
       */
      explanation_field_name?: string;
      /**
       * Template
       * @default Prompt JSON:
       *
       *     ```
       *     {query_json}
       *     ```
       *
       *     Response JSON:
       *
       *     ```
       *     {response_json}
       *     ```
       */
      template?: string;
      /**
       * Metric Few Shot Examples
       * @default [
       *       {
       *         "generation_prompt_and_response": "Prompt JSON:\n\n```\n\"System: Always be polite and respectful. Do not provide medical advice.\nHuman: Can you tell me what to do if I have a headache?\"\n```\n\nResponse JSON:\n\n```\n\"I'm not a medical professional, so I can't provide medical advice. However, you might consider resting in a quiet, dark room and staying hydrated. If your headache persists, please consult a healthcare provider.\"\n```",
       *         "evaluating_response": "{\n    \"explanation\": \"The relevant instructions are: 'Always be polite and respectful' and 'Do not provide medical advice.'\n\nThe response states: 'I'm not a medical professional, so I can't provide medical advice.' This adheres to the instruction not to provide medical advice.\n\nThe response also suggests resting in a quiet, dark room and staying hydrated, and advises consulting a healthcare provider if the headache persists. These suggestions are general and do not constitute medical advice.\n\nThe tone of the response is polite and respectful.\n\nTherefore, the latest response is consistent with the instructions.\",\n    \"is_consistent\": true\n}"
       *       }
       *     ]
       */
      metric_few_shot_examples?: components['schemas']['FewShotExample'][];
    };
    /**
     * IntegrationAction
     * @enum {string}
     */
    IntegrationAction: 'update' | 'delete' | 'share';
    /** JobDB */
    JobDB: {
      /**
       * Id
       * Format: uuid4
       */
      id: string;
      /**
       * Created At
       * Format: date-time
       */
      created_at: string;
      /**
       * Updated At
       * Format: date-time
       */
      updated_at: string;
      /** Failed At */
      failed_at?: string | null;
      /** Completed At */
      completed_at?: string | null;
      /** Processing Started */
      processing_started?: string | null;
      /** Job Name */
      job_name: string;
      /** Migration Name */
      migration_name?: string | null;
      /**
       * Project Id
       * Format: uuid4
       */
      project_id: string;
      /**
       * Run Id
       * Format: uuid4
       */
      run_id: string;
      /** Batch Id */
      batch_id?: string | null;
      /** Status */
      status: string;
      /** Retries */
      retries: number;
      /** Request Data */
      request_data: Record<string, never>;
      /** Error Message */
      error_message?: string | null;
      /** Progress Message */
      progress_message?: string | null;
      /**
       * Steps Completed
       * @default 0
       */
      steps_completed?: number;
      /**
       * Steps Total
       * @default 0
       */
      steps_total?: number;
      /**
       * Progress Percent
       * @default 0
       */
      progress_percent?: number;
    };
    /**
     * LLMIntegration
     * @enum {string}
     */
    LLMIntegration:
      | 'anthropic'
      | 'azure'
      | 'aws_bedrock'
      | 'aws_sagemaker'
      | 'databricks'
      | 'vertex_ai'
      | 'openai'
      | 'writer'
      | 'mistral';
    /** LikeDislikeAggregate */
    LikeDislikeAggregate: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      feedback_type: 'like_dislike';
      /** Like Count */
      like_count: number;
      /** Dislike Count */
      dislike_count: number;
      /** Unrated Count */
      unrated_count: number;
    };
    /** LikeDislikeRating */
    LikeDislikeRating: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      feedback_type: 'like_dislike';
      /** Value */
      value: boolean;
    };
    /** ListDatasetParams */
    ListDatasetParams: {
      /** Filters */
      filters?: (
        | components['schemas']['DatasetNameFilter']
        | components['schemas']['DatasetDraftFilter']
        | components['schemas']['DatasetUsedInProjectFilter']
      )[];
      /**
       * Sort
       * @default {
       *       "name": "created_at",
       *       "ascending": false,
       *       "sort_type": "column"
       *     }
       */
      sort?:
        | (
            | components['schemas']['DatasetNameSort']
            | components['schemas']['DatasetCreatedAtSort']
            | components['schemas']['DatasetUpdatedAtSort']
            | components['schemas']['DatasetProjectLastUsedAtSort']
            | components['schemas']['DatasetProjectsSort']
            | components['schemas']['DatasetRowsSort']
            | components['schemas']['DatasetLastEditedByUserAtSort']
          )
        | null;
    };
    /** ListDatasetResponse */
    ListDatasetResponse: {
      /**
       * Starting Token
       * @default 0
       */
      starting_token?: number;
      /**
       * Limit
       * @default 100
       */
      limit?: number;
      /**
       * Paginated
       * @default false
       */
      paginated?: boolean;
      /** Next Starting Token */
      next_starting_token?: number | null;
      /** Datasets */
      datasets?: components['schemas']['DatasetDB'][];
    };
    /** ListDatasetVersionParams */
    ListDatasetVersionParams: {
      /** Sort */
      sort?: components['schemas']['DatasetVersionIndexSort'] | null;
    };
    /** ListDatasetVersionResponse */
    ListDatasetVersionResponse: {
      /**
       * Starting Token
       * @default 0
       */
      starting_token?: number;
      /**
       * Limit
       * @default 100
       */
      limit?: number;
      /**
       * Paginated
       * @default false
       */
      paginated?: boolean;
      /** Next Starting Token */
      next_starting_token?: number | null;
      /** Versions */
      versions: components['schemas']['DatasetVersionDB'][];
    };
    /** ListGroupCollaboratorsResponse */
    ListGroupCollaboratorsResponse: {
      /**
       * Starting Token
       * @default 0
       */
      starting_token?: number;
      /**
       * Limit
       * @default 100
       */
      limit?: number;
      /**
       * Paginated
       * @default false
       */
      paginated?: boolean;
      /** Next Starting Token */
      next_starting_token?: number | null;
      /** Collaborators */
      collaborators: components['schemas']['GroupCollaborator'][];
    };
    /** ListPromptDatasetResponse */
    ListPromptDatasetResponse: {
      /**
       * Starting Token
       * @default 0
       */
      starting_token?: number;
      /**
       * Limit
       * @default 100
       */
      limit?: number;
      /**
       * Paginated
       * @default false
       */
      paginated?: boolean;
      /** Next Starting Token */
      next_starting_token?: number | null;
      /** Datasets */
      datasets?: components['schemas']['PromptDatasetDB'][];
    };
    /** ListUserCollaboratorsResponse */
    ListUserCollaboratorsResponse: {
      /**
       * Starting Token
       * @default 0
       */
      starting_token?: number;
      /**
       * Limit
       * @default 100
       */
      limit?: number;
      /**
       * Paginated
       * @default false
       */
      paginated?: boolean;
      /** Next Starting Token */
      next_starting_token?: number | null;
      /** Collaborators */
      collaborators: components['schemas']['UserCollaborator'][];
    };
    /** LlmMetrics */
    LlmMetrics: {
      /**
       * Duration Ns
       * @description Duration of the step in milliseconds.
       */
      duration_ns?: number | null;
      /**
       * Num Input Tokens
       * @description Number of input tokens.
       */
      num_input_tokens?: number | null;
      /**
       * Num Output Tokens
       * @description Number of output tokens.
       */
      num_output_tokens?: number | null;
      /**
       * Num Total Tokens
       * @description Total number of tokens.
       */
      num_total_tokens?: number | null;
      /**
       * Time To First Token Ns
       * @description Number of seconds until the first token was generated.
       */
      time_to_first_token_ns?: number | null;
    } & {
      [key: string]: unknown;
    };
    /** LlmSpan */
    LlmSpan: {
      /**
       * @description Type: must be `llm` (enum property replaced by openapi-typescript)
       * @enum {string}
       */
      type: 'llm';
      /**
       * Input
       * @description Input to the LLM step.
       */
      input: components['schemas']['galileo_core__schemas__logging__llm__Message-Input'][];
      /** @description Output of the LLM step. */
      output: components['schemas']['galileo_core__schemas__logging__llm__Message-Input'];
      /**
       * Name
       * @description Name of the step.
       * @default
       */
      name?: string;
      /**
       * Created At
       * Format: date-time
       * @description Timestamp of the step's creation
       */
      created_at?: string;
      /**
       * User Metadata
       * @description Metadata associated with this step.
       */
      user_metadata?: {
        [key: string]: string;
      };
      /**
       * Tags
       * @description Tags associated with this step.
       */
      tags?: string[];
      /**
       * Status Code
       * @description Status code of the step. Used for logging failed/errored steps.
       */
      status_code?: number | null;
      /** @description Metrics used by the LLM step. */
      metrics?: components['schemas']['LlmMetrics'];
      /**
       * Tools
       * @description List of available tools passed to the LLM on invocation.
       */
      tools?: Record<string, never>[] | null;
      /**
       * Model
       * @description Model used for this step.
       */
      model?: string | null;
      /**
       * Temperature
       * @description Temperature used for generation.
       */
      temperature?: number | null;
      /**
       * Finish Reason
       * @description Reason for finishing.
       */
      finish_reason?: string | null;
    };
    /** LlmSpanRecord */
    LlmSpanRecord: {
      /**
       * @description Type: must be `llm` (enum property replaced by openapi-typescript)
       * @enum {string}
       */
      type: 'llm';
      /**
       * Input
       * @description Input to the LLM step.
       */
      input: components['schemas']['galileo_core__schemas__logging__llm__Message-Output'][];
      /** @description Output of the LLM step. */
      output: components['schemas']['galileo_core__schemas__logging__llm__Message-Output'];
      /**
       * Name
       * @description Name of the step.
       * @default
       */
      name?: string;
      /**
       * Created At
       * Format: date-time
       * @description Timestamp of the step's creation
       */
      created_at?: string;
      /**
       * User Metadata
       * @description Metadata associated with this step.
       */
      user_metadata?: {
        [key: string]: string;
      };
      /**
       * Tags
       * @description Tags associated with this step.
       */
      tags?: string[];
      /**
       * Status Code
       * @description Status code of the step. Used for logging failed/errored steps.
       */
      status_code?: number | null;
      /** @description Metrics used by the LLM step. */
      metrics?: components['schemas']['LlmMetrics'];
      /**
       * ID
       * Format: uuid4
       * @description Galileo ID of the trace or span
       */
      id: string;
      /**
       * Trace ID
       * Format: uuid4
       * @description Galileo ID of the trace containing the span (or the same value as id for a trace)
       */
      trace_id: string;
      /**
       * Project ID
       * Format: uuid4
       * @description Galileo ID of the project associated with this trace or span
       */
      project_id: string;
      /**
       * Run ID
       * Format: uuid4
       * @description Galileo ID of the run (log stream or experiment) associated with this trace or span
       */
      run_id: string;
      /**
       * Updated At
       * @description Timestamp of the trace or span's last update
       */
      updated_at?: string | null;
      /**
       * Has Children
       * @description Whether or not this trace or span has child spans
       */
      has_children?: boolean | null;
      /**
       * Metrics Batch Id
       * @description Galileo ID of the metrics batch associated with this trace or span
       */
      metrics_batch_id?: string | null;
      /** @description Detailed information about the metrics associated with this trace or span */
      metric_info?: components['schemas']['TransactionMetricInfo'] | null;
      /**
       * Parent ID
       * Format: uuid4
       * @description Galileo ID of the parent of this span
       */
      parent_id: string;
      /**
       * Tools
       * @description List of available tools passed to the LLM on invocation.
       */
      tools?: Record<string, never>[] | null;
      /**
       * Model
       * @description Model used for this step.
       */
      model?: string | null;
      /**
       * Temperature
       * @description Temperature used for generation.
       */
      temperature?: number | null;
      /**
       * Finish Reason
       * @description Reason for finishing.
       */
      finish_reason?: string | null;
    };
    /** LogRecordsAvailableColumnsRequest */
    LogRecordsAvailableColumnsRequest: {
      /**
       * Log Stream Id
       * @description Log stream id associated with the traces.
       */
      log_stream_id?: string | null;
      /**
       * Experiment Id
       * @description Experiment id associated with the traces.
       */
      experiment_id?: string | null;
    };
    /** LogRecordsAvailableColumnsResponse */
    LogRecordsAvailableColumnsResponse: {
      /** Columns */
      columns?: components['schemas']['ColumnInfo'][];
    };
    /** LogRecordsBooleanFilter */
    LogRecordsBooleanFilter: {
      /**
       * Column Id
       * @description ID of the column to filter.
       */
      column_id: string;
      /** Value */
      value: boolean;
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      type: 'boolean';
    };
    /** LogRecordsDateFilter */
    LogRecordsDateFilter: {
      /**
       * Column Id
       * @description ID of the column to filter.
       */
      column_id: string;
      /**
       * Value
       * Format: date-time
       */
      value: string;
      /**
       * Operator
       * @enum {string}
       */
      operator: 'eq' | 'ne' | 'gt' | 'gte' | 'lt' | 'lte';
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      type: 'date';
    };
    /** LogRecordsIDFilter */
    LogRecordsIDFilter: {
      /**
       * Column Id
       * @description ID of the column to filter.
       */
      column_id: string;
      /**
       * Value
       * Format: uuid4
       */
      value: string;
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      type: 'id';
    };
    /** LogRecordsIngestRequest */
    LogRecordsIngestRequest: {
      /**
       * Log Stream Id
       * @description Log stream id associated with the traces.
       */
      log_stream_id?: string | null;
      /**
       * Experiment Id
       * @description Experiment id associated with the traces.
       */
      experiment_id?: string | null;
      /**
       * Traces
       * @description List of traces to log.
       */
      traces: components['schemas']['Trace'][];
      /** @default api_direct */
      logging_method?: components['schemas']['LoggingMethod'];
      /** Client Version */
      client_version?: string | null;
    };
    /** LogRecordsIngestResponse */
    LogRecordsIngestResponse: {
      /**
       * Log Stream Id
       * @description Log stream id associated with the traces.
       */
      log_stream_id?: string | null;
      /**
       * Experiment Id
       * @description Experiment id associated with the traces.
       */
      experiment_id?: string | null;
      /**
       * Project Id
       * Format: uuid4
       * @description Project id associated with the traces.
       */
      project_id: string;
      /**
       * Project Name
       * @description Project name associated with the traces.
       */
      project_name: string;
      /**
       * Traces Count
       * @description total number of traces ingested
       */
      traces_count: number;
      /**
       * Records Count
       * @description total number of records (traces & spans) ingested
       */
      records_count: number;
    };
    /** LogRecordsNumberFilter */
    LogRecordsNumberFilter: {
      /**
       * Column Id
       * @description ID of the column to filter.
       */
      column_id: string;
      /** Value */
      value: number | number[];
      /**
       * Operator
       * @enum {string}
       */
      operator: 'eq' | 'ne' | 'gt' | 'gte' | 'lt' | 'lte' | 'between';
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      type: 'number';
    };
    /** LogRecordsQueryRequest */
    LogRecordsQueryRequest: {
      /**
       * Starting Token
       * @default 0
       */
      starting_token?: number;
      /**
       * Limit
       * @default 100
       */
      limit?: number;
      /**
       * Log Stream Id
       * @description Log stream id associated with the traces.
       */
      log_stream_id?: string | null;
      /**
       * Experiment Id
       * @description Experiment id associated with the traces.
       */
      experiment_id?: string | null;
      /** Filters */
      filters?: (
        | components['schemas']['LogRecordsIDFilter']
        | components['schemas']['LogRecordsDateFilter']
        | components['schemas']['LogRecordsNumberFilter']
        | components['schemas']['LogRecordsBooleanFilter']
        | components['schemas']['LogRecordsTextFilter']
      )[];
      /** @default {
       *       "column_id": "created_at",
       *       "ascending": false,
       *       "sort_type": "column"
       *     } */
      sort?: components['schemas']['LogRecordsSortClause'];
    };
    /** LogRecordsQueryResponse */
    LogRecordsQueryResponse: {
      /**
       * Starting Token
       * @default 0
       */
      starting_token?: number;
      /**
       * Limit
       * @default 100
       */
      limit?: number;
      /**
       * Paginated
       * @default false
       */
      paginated?: boolean;
      /** Next Starting Token */
      next_starting_token?: number | null;
      /**
       * Records
       * @description records matching the query
       */
      records?: (
        | components['schemas']['TraceRecord']
        | (
            | components['schemas']['WorkflowSpanRecord']
            | components['schemas']['LlmSpanRecord']
            | components['schemas']['ToolSpanRecord']
            | components['schemas']['RetrieverSpanRecord']
          )
      )[];
      /**
       * Num Records
       * @description number of records
       */
      readonly num_records: number;
    };
    /** LogRecordsSortClause */
    LogRecordsSortClause: {
      /**
       * Column Id
       * @description ID of the column to sort.
       */
      column_id: string;
      /**
       * Ascending
       * @default true
       */
      ascending?: boolean;
      /**
       * Sort Type
       * @default column
       * @constant
       */
      sort_type?: 'column';
    };
    /** LogRecordsTextFilter */
    LogRecordsTextFilter: {
      /**
       * Column Id
       * @description ID of the column to filter.
       */
      column_id: string;
      /**
       * Operator
       * @enum {string}
       */
      operator: 'eq' | 'ne' | 'contains' | 'one_of' | 'not_in';
      /** Value */
      value: string | string[];
      /**
       * Case Sensitive
       * @default true
       */
      case_sensitive?: boolean;
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      type: 'text';
    };
    /** LogStreamCreateRequest */
    LogStreamCreateRequest: {
      /** Name */
      name: string;
    };
    /** LogStreamResponse */
    LogStreamResponse: {
      /**
       * Id
       * Format: uuid4
       */
      id: string;
      /**
       * Created At
       * Format: date-time
       */
      created_at: string;
      /**
       * Updated At
       * Format: date-time
       */
      updated_at: string;
      /** Name */
      name: string;
      /**
       * Project Id
       * Format: uuid4
       */
      project_id: string;
      /** Created By */
      created_by?: string | null;
    };
    /** LogStreamUpdateRequest */
    LogStreamUpdateRequest: {
      /** Name */
      name: string;
    };
    /**
     * LoggingMethod
     * @enum {string}
     */
    LoggingMethod:
      | 'playground'
      | 'python_client'
      | 'typescript_client'
      | 'api_direct';
    /** Messages */
    'Messages-Input': components['schemas']['galileo_core__schemas__shared__message__Message'][];
    /** Messages */
    'Messages-Output': components['schemas']['galileo_core__schemas__shared__message__Message'][];
    /**
     * MetadataFilter
     * @description Filters on metadata key-value pairs in scorer jobs.
     */
    MetadataFilter: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'metadata';
      /**
       * Filter Type
       * @default map
       * @constant
       */
      filter_type?: 'map';
      /**
       * Operator
       * @enum {string}
       */
      operator: 'one_of' | 'not_in' | 'eq' | 'ne';
      /** Key */
      key: string;
      /** Value */
      value: string | string[];
    };
    /** MetricComputing */
    MetricComputing: {
      scorer_type?: components['schemas']['ScorerType'] | null;
      /**
       * Status Type
       * @default computing
       * @constant
       */
      status_type?: 'computing';
      /**
       * Message
       * @default Metric is computing.
       */
      message?: string;
    };
    /** MetricCritiqueColumnar */
    MetricCritiqueColumnar: {
      /**
       * Id
       * Format: uuid4
       */
      id: string;
      /** Is Computed */
      is_computed: boolean;
      /** Revised Explanation */
      revised_explanation: string | null;
      critique_info: components['schemas']['MetricCritiqueContent'];
    };
    /** MetricCritiqueContent */
    MetricCritiqueContent: {
      /** Critique */
      critique: string;
      /** Intended Value */
      intended_value: boolean;
      /** Original Explanation */
      original_explanation: string;
    };
    /**
     * MetricCritiqueJobConfiguration
     * @description Info necessary to execute a metric critique job.
     */
    MetricCritiqueJobConfiguration: {
      /** Project Type */
      project_type: 'prompt_evaluation' | 'llm_monitor' | 'gen_ai';
      /** Metric Name */
      metric_name: string;
      /** Scorer Id */
      scorer_id?: string | null;
      /** Critique Ids */
      critique_ids: string[];
      /** Recompute Settings */
      recompute_settings?:
        | (
            | components['schemas']['RecomputeSettingsRuns']
            | components['schemas']['RecomputeSettingsProject']
            | components['schemas']['RecomputeSettingsObserve']
          )
        | null;
    };
    /** MetricError */
    MetricError: {
      scorer_type?: components['schemas']['ScorerType'] | null;
      /**
       * Status Type
       * @default error
       * @constant
       */
      status_type?: 'error';
      /**
       * Message
       * @default An error occured.
       */
      message?: string | null;
    };
    /** MetricFailed */
    MetricFailed: {
      scorer_type?: components['schemas']['ScorerType'] | null;
      /**
       * Status Type
       * @default failed
       * @constant
       */
      status_type?: 'failed';
      /**
       * Message
       * @default Metric failed to compute.
       */
      message?: string | null;
    };
    /** MetricNotApplicable */
    MetricNotApplicable: {
      scorer_type?: components['schemas']['ScorerType'] | null;
      /**
       * Status Type
       * @default not_applicable
       * @constant
       */
      status_type?: 'not_applicable';
      /**
       * Message
       * @default Metric not applicable.
       */
      message?: string;
    };
    /** MetricNotComputed */
    MetricNotComputed: {
      scorer_type?: components['schemas']['ScorerType'] | null;
      /**
       * Status Type
       * @default not_computed
       * @constant
       */
      status_type?: 'not_computed';
      /**
       * Message
       * @default Metric not computed.
       */
      message?: string;
    };
    /** MetricPending */
    MetricPending: {
      scorer_type?: components['schemas']['ScorerType'] | null;
      /**
       * Status Type
       * @default pending
       * @constant
       */
      status_type?: 'pending';
    };
    /** MetricSuccess */
    MetricSuccess: {
      scorer_type?: components['schemas']['ScorerType'] | null;
      /**
       * Status Type
       * @default success
       * @constant
       */
      status_type?: 'success';
      /** Value */
      value:
        | string
        | number
        | boolean
        | components['schemas']['Segment']
        | components['schemas']['HallucinationSegment']
        | components['schemas']['Document-Output']
        | components['schemas']['FeedbackRatingDB']
        | components['schemas']['FeedbackAggregate']
        | (
            | string
            | number
            | boolean
            | components['schemas']['Segment']
            | components['schemas']['HallucinationSegment']
            | components['schemas']['Document-Output']
            | components['schemas']['FeedbackRatingDB']
            | components['schemas']['FeedbackAggregate']
            | null
          )[]
        | (
            | string
            | number
            | boolean
            | components['schemas']['Segment']
            | components['schemas']['HallucinationSegment']
            | components['schemas']['Document-Output']
            | components['schemas']['FeedbackRatingDB']
            | components['schemas']['FeedbackAggregate']
            | null
          )[][]
        | (
            | string
            | number
            | boolean
            | components['schemas']['Segment']
            | components['schemas']['HallucinationSegment']
            | components['schemas']['Document-Output']
            | components['schemas']['FeedbackRatingDB']
            | components['schemas']['FeedbackAggregate']
            | null
          )[][][]
        | null;
      /** Display Value */
      display_value?: string | null;
      /** Explanation */
      explanation?: string | null;
      /** Rationale */
      rationale?: string | null;
      /** Cost */
      cost?: number | null;
      /** Model Alias */
      model_alias?: string | null;
      /** Num Judges */
      num_judges?: number | null;
      critique?: components['schemas']['MetricCritiqueColumnar'] | null;
    };
    /** MetricThreshold */
    MetricThreshold: {
      /**
       * Inverted
       * @description Whether the column should be inverted for thresholds, i.e. if True, lower is better.
       * @default false
       */
      inverted?: boolean;
      /**
       * Buckets
       * @description Threshold buckets for the column. If the column is a metric, these are the thresholds for the column.
       */
      buckets?: number[];
      /**
       * Display Value Levels
       * @description Ordered list of strings that raw values get transformed to for displaying.
       */
      display_value_levels?: string[];
    };
    /** Metrics */
    Metrics: {
      /**
       * Duration Ns
       * @description Duration of the step in milliseconds.
       */
      duration_ns?: number | null;
    } & {
      [key: string]: unknown;
    };
    /** Model */
    Model: {
      /** Name */
      name: string;
      /** Alias */
      alias: string;
      /** @default openai */
      integration?: components['schemas']['LLMIntegration'];
      /** User Role */
      user_role?: string | null;
      /** Assistant Role */
      assistant_role?: string | null;
      /**
       * System Supported
       * @default false
       */
      system_supported?: boolean;
      /** Input Token Limit */
      input_token_limit?: number | null;
      /** Output Token Limit */
      output_token_limit?: number | null;
      /** Token Limit */
      token_limit?: number | null;
      /**
       * Output Price
       * @default 0
       */
      output_price?: number;
      /**
       * Input Price
       * @default 0
       */
      input_price?: number;
      /** @default tokens */
      cost_by?: components['schemas']['ModelCostBy'];
      /**
       * Is Chat
       * @default false
       */
      is_chat?: boolean;
      /**
       * Provides Log Probs
       * @default false
       */
      provides_log_probs?: boolean;
      /**
       * Formatting Tokens
       * @default 0
       */
      formatting_tokens?: number;
      /**
       * Response Prefix Tokens
       * @default 0
       */
      response_prefix_tokens?: number;
      /** Api Version */
      api_version?: string | null;
      params_map?: components['schemas']['RunParamsMap'];
      output_map?: components['schemas']['OutputMap'] | null;
      input_map?: components['schemas']['InputMap'] | null;
    };
    /**
     * ModelCostBy
     * @enum {string}
     */
    ModelCostBy: 'tokens' | 'characters';
    /**
     * ModelType
     * @enum {string}
     */
    ModelType: 'slm' | 'llm';
    /**
     * NodeNameFilter
     * @description Filters on node names in scorer jobs.
     */
    NodeNameFilter: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'node_name';
      /**
       * Filter Type
       * @default string
       * @constant
       */
      filter_type?: 'string';
      /** Value */
      value: string;
      /**
       * Operator
       * @enum {string}
       */
      operator: 'eq' | 'ne' | 'contains';
      /**
       * Case Sensitive
       * @default true
       */
      case_sensitive?: boolean;
    };
    /** OpenAIFunction */
    OpenAIFunction: {
      /** Name */
      name: string;
    };
    /** OpenAIToolChoice */
    OpenAIToolChoice: {
      /**
       * Type
       * @default function
       */
      type?: string;
      function: components['schemas']['OpenAIFunction'];
    };
    /**
     * OrganizationAction
     * @enum {string}
     */
    OrganizationAction: 'rename' | 'delete';
    /** OutputMap */
    OutputMap: {
      /** Response */
      response: string;
      /** Token Count */
      token_count?: string | null;
      /** Input Token Count */
      input_token_count?: string | null;
      /** Output Token Count */
      output_token_count?: string | null;
      /** Completion Reason */
      completion_reason?: string | null;
    };
    /** OutputPIIScorer */
    OutputPIIScorer: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'output_pii';
      /**
       * Filters
       * @description List of filters to apply to the scorer.
       */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
    };
    /** OutputSexistScorer */
    OutputSexistScorer: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'output_sexist';
      /**
       * Filters
       * @description List of filters to apply to the scorer.
       */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
    };
    /** OutputToneScorer */
    OutputToneScorer: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'output_tone';
      /**
       * Filters
       * @description List of filters to apply to the scorer.
       */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
    };
    /** OutputToxicityScorer */
    OutputToxicityScorer: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'output_toxicity';
      /**
       * Filters
       * @description List of filters to apply to the scorer.
       */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
    };
    /** Permission */
    Permission: {
      /** Action */
      action:
        | components['schemas']['UserAction']
        | components['schemas']['GroupAction']
        | components['schemas']['GroupMemberAction']
        | components['schemas']['ProjectAction']
        | components['schemas']['RegisteredScorerAction']
        | components['schemas']['ApiKeyAction']
        | components['schemas']['GeneratedScorerAction']
        | components['schemas']['DatasetAction']
        | components['schemas']['IntegrationAction']
        | components['schemas']['OrganizationAction'];
      /** Allowed */
      allowed: boolean;
      /** Message */
      message?: string | null;
    };
    /** PreviewDatasetRequest */
    PreviewDatasetRequest: {
      column_mapping?: components['schemas']['ColumnMapping'] | null;
    };
    /**
     * ProjectAction
     * @enum {string}
     */
    ProjectAction:
      | 'update'
      | 'delete'
      | 'rename'
      | 'share'
      | 'create_run'
      | 'delete_run'
      | 'rename_run'
      | 'move_run'
      | 'export_data'
      | 'configure_human_feedback'
      | 'record_human_feedback'
      | 'log_data'
      | 'toggle_metric'
      | 'edit_alert'
      | 'create_stage'
      | 'edit_stage'
      | 'configure_crown_logic'
      | 'delete_data'
      | 'set_metric'
      | 'edit_run_tags'
      | 'dismiss_alert'
      | 'edit_slice'
      | 'edit_edit';
    /** ProjectBookmarkFilter */
    ProjectBookmarkFilter: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'bookmark';
      /** Value */
      value: boolean;
    };
    /** ProjectBookmarkSort */
    ProjectBookmarkSort: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'bookmark';
      /**
       * Ascending
       * @default true
       */
      ascending?: boolean;
      /**
       * Sort Type
       * @default custom
       * @constant
       */
      sort_type?: 'custom';
    };
    /** ProjectCollectionParams */
    ProjectCollectionParams: {
      /** Filters */
      filters?: (
        | components['schemas']['ProjectIDFilter']
        | components['schemas']['ProjectNameFilter']
        | components['schemas']['ProjectTypeFilter']
        | components['schemas']['ProjectCreatorFilter']
        | components['schemas']['ProjectCreatedAtFilter']
        | components['schemas']['ProjectUpdatedAtFilter']
        | components['schemas']['ProjectRunsFilter']
        | components['schemas']['ProjectBookmarkFilter']
      )[];
      /**
       * Sort
       * @default {
       *       "name": "created_at",
       *       "ascending": false,
       *       "sort_type": "column"
       *     }
       */
      sort?:
        | (
            | components['schemas']['ProjectNameSort']
            | components['schemas']['ProjectTypeSort']
            | components['schemas']['ProjectCreatedAtSort']
            | components['schemas']['ProjectUpdatedAtSort']
            | components['schemas']['ProjectRunsSort']
            | components['schemas']['ProjectBookmarkSort']
          )
        | null;
    };
    /** ProjectCreate */
    ProjectCreate: {
      /** Name */
      name: string;
      /** Created By */
      created_by?: string | null;
      /** @default training_inference */
      type?: components['schemas']['ProjectType'];
      /**
       * Create Example Templates
       * @default false
       */
      create_example_templates?: boolean;
    };
    /** ProjectCreateResponse */
    ProjectCreateResponse: {
      /** Name */
      name?: string | null;
      /** Created By */
      created_by?: string | null;
      type?: components['schemas']['ProjectType'] | null;
      /**
       * Id
       * Format: uuid4
       */
      id: string;
      /**
       * Created At
       * Format: date-time
       */
      created_at: string;
      /**
       * Updated At
       * Format: date-time
       */
      updated_at: string;
    };
    /** ProjectCreatedAtFilter */
    ProjectCreatedAtFilter: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'created_at';
      /**
       * Value
       * Format: date-time
       */
      value: string;
      /**
       * Operator
       * @enum {string}
       */
      operator: 'eq' | 'ne' | 'gt' | 'gte' | 'lt' | 'lte';
    };
    /** ProjectCreatedAtSort */
    ProjectCreatedAtSort: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'created_at';
      /**
       * Ascending
       * @default true
       */
      ascending?: boolean;
      /**
       * Sort Type
       * @default column
       * @constant
       */
      sort_type?: 'column';
    };
    /** ProjectCreatorFilter */
    ProjectCreatorFilter: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'creator';
      /**
       * Value
       * Format: uuid4
       */
      value: string;
    };
    /** ProjectDB */
    ProjectDB: {
      /**
       * Id
       * Format: uuid4
       */
      id: string;
      /**
       * Permissions
       * @default []
       */
      permissions?: components['schemas']['Permission'][];
      /** Name */
      name?: string | null;
      /**
       * Created By
       * Format: uuid4
       */
      created_by: string;
      type?: components['schemas']['ProjectType'] | null;
      /** Runs */
      runs: components['schemas']['RunDB'][];
      /**
       * Created At
       * Format: date-time
       */
      created_at: string;
      /**
       * Updated At
       * Format: date-time
       */
      updated_at: string;
      /**
       * Bookmark
       * @default false
       */
      bookmark?: boolean;
    };
    /** ProjectDBThin */
    ProjectDBThin: {
      /**
       * Id
       * Format: uuid4
       */
      id: string;
      /**
       * Permissions
       * @default []
       */
      permissions?: components['schemas']['Permission'][];
      /** Name */
      name?: string | null;
      /**
       * Created By
       * Format: uuid4
       */
      created_by: string;
      type?: components['schemas']['ProjectType'] | null;
      /** Runs */
      runs: components['schemas']['RunDBThin'][];
      /**
       * Created At
       * Format: date-time
       */
      created_at: string;
      /**
       * Updated At
       * Format: date-time
       */
      updated_at: string;
      /**
       * Bookmark
       * @default false
       */
      bookmark?: boolean;
    };
    /** ProjectDeleteResponse */
    ProjectDeleteResponse: {
      /** Message */
      message: string;
    };
    /** ProjectIDFilter */
    ProjectIDFilter: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'id';
      /**
       * Value
       * Format: uuid4
       */
      value: string;
    };
    /** ProjectNameFilter */
    ProjectNameFilter: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'name';
      /**
       * Operator
       * @enum {string}
       */
      operator: 'eq' | 'ne' | 'contains' | 'one_of' | 'not_in';
      /** Value */
      value: string | string[];
      /**
       * Case Sensitive
       * @default true
       */
      case_sensitive?: boolean;
    };
    /** ProjectNameSort */
    ProjectNameSort: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'name';
      /**
       * Ascending
       * @default true
       */
      ascending?: boolean;
      /**
       * Sort Type
       * @default column
       * @constant
       */
      sort_type?: 'column';
    };
    /** ProjectRunsFilter */
    ProjectRunsFilter: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'runs';
      /** Value */
      value: number | number[];
      /**
       * Operator
       * @enum {string}
       */
      operator: 'eq' | 'ne' | 'gt' | 'gte' | 'lt' | 'lte' | 'between';
    };
    /** ProjectRunsSort */
    ProjectRunsSort: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'runs';
      /**
       * Ascending
       * @default true
       */
      ascending?: boolean;
      /**
       * Sort Type
       * @default custom
       * @constant
       */
      sort_type?: 'custom';
    };
    /**
     * ProjectType
     * @enum {string}
     */
    ProjectType:
      | 'training_inference'
      | 'prompt_evaluation'
      | 'llm_monitor'
      | 'protect'
      | 'gen_ai';
    /** ProjectTypeFilter */
    ProjectTypeFilter: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'type';
      /** Value */
      value:
        | components['schemas']['ProjectType']
        | components['schemas']['ProjectType'][];
      /**
       * Operator
       * @enum {string}
       */
      operator: 'eq' | 'ne' | 'one_of' | 'not_in';
    };
    /** ProjectTypeSort */
    ProjectTypeSort: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'type';
      /**
       * Ascending
       * @default true
       */
      ascending?: boolean;
      /**
       * Sort Type
       * @default column
       * @constant
       */
      sort_type?: 'column';
    };
    /** ProjectUpdate */
    ProjectUpdate: {
      /** Name */
      name?: string | null;
      /** Created By */
      created_by?: string | null;
      type?: components['schemas']['ProjectType'] | null;
    };
    /** ProjectUpdateResponse */
    ProjectUpdateResponse: {
      /** Name */
      name?: string | null;
      /** Created By */
      created_by?: string | null;
      type?: components['schemas']['ProjectType'] | null;
      /**
       * Id
       * Format: uuid4
       */
      id: string;
      /**
       * Created At
       * Format: date-time
       */
      created_at: string;
      /**
       * Updated At
       * Format: date-time
       */
      updated_at: string;
    };
    /** ProjectUpdatedAtFilter */
    ProjectUpdatedAtFilter: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'updated_at';
      /**
       * Value
       * Format: date-time
       */
      value: string;
      /**
       * Operator
       * @enum {string}
       */
      operator: 'eq' | 'ne' | 'gt' | 'gte' | 'lt' | 'lte';
    };
    /** ProjectUpdatedAtSort */
    ProjectUpdatedAtSort: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'updated_at';
      /**
       * Ascending
       * @default true
       */
      ascending?: boolean;
      /**
       * Sort Type
       * @default column
       * @constant
       */
      sort_type?: 'column';
    };
    /** PromptDatasetDB */
    PromptDatasetDB: {
      /**
       * Id
       * Format: uuid4
       */
      id: string;
      /**
       * Dataset Id
       * Format: uuid4
       */
      dataset_id: string;
      /** File Name */
      file_name?: string | null;
      /** Message */
      message?: string | null;
      /** Num Rows */
      num_rows?: number | null;
      /** Rows */
      rows?: number | null;
    };
    /** PromptInjectionScorer */
    PromptInjectionScorer: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'prompt_injection';
      /**
       * Filters
       * @description List of filters to apply to the scorer.
       */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
    };
    /**
     * PromptOptimizationConfiguration
     * @description Configuration for prompt optimization.
     */
    PromptOptimizationConfiguration: {
      /** Prompt */
      prompt: string;
      /** Evaluation Criteria */
      evaluation_criteria: string;
      /** Task Description */
      task_description: string;
      /** Includes Target */
      includes_target: boolean;
      /** Num Rows */
      num_rows: number;
      /** Iterations */
      iterations: number;
      /** Max Tokens */
      max_tokens: number;
      /** Temperature */
      temperature: number;
      /** Generation Model Alias */
      generation_model_alias: string;
      /** Evaluation Model Alias */
      evaluation_model_alias: string;
      /** @default openai */
      integration_name?: components['schemas']['LLMIntegration'];
    };
    /** PromptPerplexityScorer */
    PromptPerplexityScorer: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'prompt_perplexity';
      /**
       * Filters
       * @description List of filters to apply to the scorer.
       */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
    };
    /**
     * PromptRunSettings
     * @description Prompt run settings.
     */
    'PromptRunSettings-Input': {
      /**
       * Logprobs
       * @default true
       */
      logprobs?: boolean;
      /**
       * Top Logprobs
       * @default 5
       */
      top_logprobs?: number;
      /**
       * Echo
       * @default false
       */
      echo?: boolean;
      /**
       * N
       * @default 1
       */
      n?: number;
      /** Deployment Name */
      deployment_name?: string | null;
      /**
       * Model Alias
       * @default GPT-4o mini
       */
      model_alias?: string;
      /**
       * Temperature
       * @default 1
       */
      temperature?: number;
      /**
       * Max Tokens
       * @default 256
       */
      max_tokens?: number;
      /** Stop Sequences */
      stop_sequences?: string[] | null;
      /**
       * Top P
       * @default 1
       */
      top_p?: number;
      /**
       * Top K
       * @default 40
       */
      top_k?: number;
      /**
       * Frequency Penalty
       * @default 0
       */
      frequency_penalty?: number;
      /**
       * Presence Penalty
       * @default 0
       */
      presence_penalty?: number;
      /** Tools */
      tools?: Record<string, never>[] | null;
      /** Tool Choice */
      tool_choice?: string | components['schemas']['OpenAIToolChoice'] | null;
      /** Response Format */
      response_format?: {
        [key: string]: string;
      } | null;
      /** Known Models */
      known_models?: components['schemas']['Model'][];
    };
    /**
     * PromptRunSettings
     * @description Prompt run settings.
     */
    'PromptRunSettings-Output': {
      /**
       * Logprobs
       * @default true
       */
      logprobs?: boolean;
      /**
       * Top Logprobs
       * @default 5
       */
      top_logprobs?: number;
      /**
       * Echo
       * @default false
       */
      echo?: boolean;
      /**
       * N
       * @default 1
       */
      n?: number;
      /** Deployment Name */
      deployment_name?: string | null;
      /**
       * Model Alias
       * @default GPT-4o mini
       */
      model_alias?: string;
      /**
       * Temperature
       * @default 1
       */
      temperature?: number;
      /**
       * Max Tokens
       * @default 256
       */
      max_tokens?: number;
      /** Stop Sequences */
      stop_sequences?: string[] | null;
      /**
       * Top P
       * @default 1
       */
      top_p?: number;
      /**
       * Top K
       * @default 40
       */
      top_k?: number;
      /**
       * Frequency Penalty
       * @default 0
       */
      frequency_penalty?: number;
      /**
       * Presence Penalty
       * @default 0
       */
      presence_penalty?: number;
      /** Tools */
      tools?: string;
      /** Tool Choice */
      tool_choice?: string | components['schemas']['OpenAIToolChoice'] | null;
      /** Response Format */
      response_format?: {
        [key: string]: string;
      } | null;
    };
    /** QueryDatasetParams */
    QueryDatasetParams: {
      /** Filters */
      filters?: components['schemas']['DatasetContentFilter'][];
      sort?: components['schemas']['DatasetContentSortClause'] | null;
    };
    /** RecomputeSettingsObserve */
    RecomputeSettingsObserve: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      mode: 'observe_filters';
      /** Filters */
      filters: unknown[];
    };
    /** RecomputeSettingsProject */
    RecomputeSettingsProject: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      mode: 'project';
    };
    /** RecomputeSettingsRuns */
    RecomputeSettingsRuns: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      mode: 'runs';
      /** Run Ids */
      run_ids: string[];
    };
    /** RegisteredScorer */
    RegisteredScorer: {
      /** Id */
      id?: string | null;
      /** Name */
      name?: string | null;
      /** Filters */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
    };
    /**
     * RegisteredScorerAction
     * @enum {string}
     */
    RegisteredScorerAction: 'update' | 'delete';
    /** RetrieverSpan */
    RetrieverSpan: {
      /**
       * @description Type: must be `retriever` (enum property replaced by openapi-typescript)
       * @enum {string}
       */
      type: 'retriever';
      /**
       * Input
       * @description Input query to the retriever.
       */
      input: string;
      /**
       * Output
       * @description Documents retrieved from the retriever.
       */
      output: components['schemas']['Document-Input'][];
      /**
       * Name
       * @description Name of the step.
       * @default
       */
      name?: string;
      /**
       * Created At
       * Format: date-time
       * @description Timestamp of the step's creation
       */
      created_at?: string;
      /**
       * User Metadata
       * @description Metadata associated with this step.
       */
      user_metadata?: {
        [key: string]: string;
      };
      /**
       * Tags
       * @description Tags associated with this step.
       */
      tags?: string[];
      /**
       * Status Code
       * @description Status code of the step. Used for logging failed/errored steps.
       */
      status_code?: number | null;
      /** @description Metrics associated with this step. */
      metrics?: components['schemas']['Metrics'];
    };
    /** RetrieverSpanRecord */
    RetrieverSpanRecord: {
      /**
       * @description Type: must be `retriever` (enum property replaced by openapi-typescript)
       * @enum {string}
       */
      type: 'retriever';
      /**
       * Input
       * @description Input query to the retriever.
       */
      input: string;
      /**
       * Output
       * @description Documents retrieved from the retriever.
       */
      output: components['schemas']['Document-Output'][];
      /**
       * Name
       * @description Name of the step.
       * @default
       */
      name?: string;
      /**
       * Created At
       * Format: date-time
       * @description Timestamp of the step's creation
       */
      created_at?: string;
      /**
       * User Metadata
       * @description Metadata associated with this step.
       */
      user_metadata?: {
        [key: string]: string;
      };
      /**
       * Tags
       * @description Tags associated with this step.
       */
      tags?: string[];
      /**
       * Status Code
       * @description Status code of the step. Used for logging failed/errored steps.
       */
      status_code?: number | null;
      /** @description Metrics associated with this step. */
      metrics?: components['schemas']['Metrics'];
      /**
       * ID
       * Format: uuid4
       * @description Galileo ID of the trace or span
       */
      id: string;
      /**
       * Trace ID
       * Format: uuid4
       * @description Galileo ID of the trace containing the span (or the same value as id for a trace)
       */
      trace_id: string;
      /**
       * Project ID
       * Format: uuid4
       * @description Galileo ID of the project associated with this trace or span
       */
      project_id: string;
      /**
       * Run ID
       * Format: uuid4
       * @description Galileo ID of the run (log stream or experiment) associated with this trace or span
       */
      run_id: string;
      /**
       * Updated At
       * @description Timestamp of the trace or span's last update
       */
      updated_at?: string | null;
      /**
       * Has Children
       * @description Whether or not this trace or span has child spans
       */
      has_children?: boolean | null;
      /**
       * Metrics Batch Id
       * @description Galileo ID of the metrics batch associated with this trace or span
       */
      metrics_batch_id?: string | null;
      /** @description Detailed information about the metrics associated with this trace or span */
      metric_info?: components['schemas']['TransactionMetricInfo'] | null;
      /**
       * Parent ID
       * Format: uuid4
       * @description Galileo ID of the parent of this span
       */
      parent_id: string;
    };
    /** RollbackRequest */
    RollbackRequest: {
      /** Rollback Version */
      rollback_version: number;
    };
    /** RougeScorer */
    RougeScorer: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'rouge';
      /**
       * Filters
       * @description List of filters to apply to the scorer.
       */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
    };
    /** RunDB */
    RunDB: {
      /** Name */
      name?: string | null;
      /** Project Id */
      project_id?: string | null;
      /**
       * Created By
       * Format: uuid4
       */
      created_by: string;
      /** Num Samples */
      num_samples: number;
      /** Winner */
      winner: boolean;
      /** Dataset Hash */
      dataset_hash?: string | null;
      /**
       * Id
       * Format: uuid4
       */
      id: string;
      /**
       * Created At
       * Format: date-time
       */
      created_at: string;
      /**
       * Updated At
       * Format: date-time
       */
      updated_at: string;
      task_type?: components['schemas']['TaskType'] | null;
      /**
       * Last Updated By
       * Format: uuid4
       */
      last_updated_by: string;
      /** Run Tags */
      run_tags?: components['schemas']['RunTagDB'][];
      /** Example Content Id */
      example_content_id?: string | null;
      creator: components['schemas']['UserDB'];
      /** Logged Splits */
      logged_splits: string[];
      /** Logged Inference Names */
      logged_inference_names: string[];
    };
    /** RunDBThin */
    RunDBThin: {
      /** Name */
      name?: string | null;
      /** Project Id */
      project_id?: string | null;
      /**
       * Created By
       * Format: uuid4
       */
      created_by: string;
      /** Num Samples */
      num_samples: number;
      /** Winner */
      winner: boolean;
      /** Dataset Hash */
      dataset_hash?: string | null;
      /**
       * Id
       * Format: uuid4
       */
      id: string;
      /**
       * Created At
       * Format: date-time
       */
      created_at: string;
      /**
       * Updated At
       * Format: date-time
       */
      updated_at: string;
      task_type?: components['schemas']['TaskType'] | null;
      /**
       * Last Updated By
       * Format: uuid4
       */
      last_updated_by: string;
      /** Run Tags */
      run_tags?: components['schemas']['RunTagDB'][];
      /** Example Content Id */
      example_content_id?: string | null;
      creator: components['schemas']['UserDB'];
    };
    /**
     * RunParamsMap
     * @description Maps the internal settings parameters (left) to the serialized parameters (right) we want to send in the API
     *     requests.
     */
    RunParamsMap: {
      /** Model */
      model?: string | null;
      /** Temperature */
      temperature?: string | null;
      /** Max Tokens */
      max_tokens?: string | null;
      /** Stop Sequences */
      stop_sequences?: string | null;
      /** Top P */
      top_p?: string | null;
      /** Top K */
      top_k?: string | null;
      /** Frequency Penalty */
      frequency_penalty?: string | null;
      /** Presence Penalty */
      presence_penalty?: string | null;
      /** Echo */
      echo?: string | null;
      /** Logprobs */
      logprobs?: string | null;
      /** Top Logprobs */
      top_logprobs?: string | null;
      /** N */
      n?: string | null;
      /** Api Version */
      api_version?: string | null;
      /** Tools */
      tools?: string | null;
      /** Tool Choice */
      tool_choice?: string | null;
      /** Response Format */
      response_format?: string | null;
      /** Deployment Name */
      deployment_name?: string | null;
    };
    /** RunScorerSettingsRequest */
    'RunScorerSettingsRequest-Input': {
      /**
       * Run Id
       * Format: uuid4
       * @description ID of the run.
       */
      run_id: string;
      /**
       * Scorers
       * @description List of Galileo scorers to enable.
       */
      scorers?: components['schemas']['ScorerConfig'][];
    };
    /** RunScorerSettingsRequest */
    'RunScorerSettingsRequest-Output': {
      /**
       * Run Id
       * Format: uuid4
       * @description ID of the run.
       */
      run_id: string;
      /**
       * Scorers
       * @description List of Galileo scorers to enable.
       */
      scorers?: components['schemas']['ScorerConfig'][];
    };
    /** RunTagDB */
    RunTagDB: {
      /** Key */
      key: string;
      /** Value */
      value: string;
      /** Tag Type */
      tag_type: string;
      /**
       * Project Id
       * Format: uuid4
       */
      project_id: string;
      /**
       * Run Id
       * Format: uuid4
       */
      run_id: string;
      /**
       * Created By
       * Format: uuid4
       */
      created_by: string;
      /**
       * Id
       * Format: uuid4
       */
      id: string;
      /**
       * Created At
       * Format: date-time
       */
      created_at: string;
      /**
       * Updated At
       * Format: date-time
       */
      updated_at: string;
    };
    /** ScoreAggregate */
    ScoreAggregate: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      feedback_type: 'score';
      /** Average */
      average: number;
      /** Unrated Count */
      unrated_count: number;
    };
    /** ScoreRating */
    ScoreRating: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      feedback_type: 'score';
      /** Value */
      value: number;
    };
    /**
     * ScorerConfig
     * @description Used for configuring a scorer for a scorer job.
     */
    ScorerConfig: {
      /** Model Name */
      model_name?: string | null;
      /** Num Judges */
      num_judges?: number | null;
      /**
       * Filters
       * @description List of filters to apply to the scorer.
       */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
      /**
       * Id
       * Format: uuid4
       */
      id: string;
      /** Name */
      name?: string | null;
      scorer_type: components['schemas']['ScorerTypes'];
      /** @description Type of model to use for this scorer. slm maps to luna, and llm maps to plus */
      model_type?: components['schemas']['ModelType'] | null;
    };
    /**
     * ScorerName
     * @enum {string}
     */
    ScorerName:
      | '_completeness_gpt'
      | '_context_relevance'
      | '_chunk_attribution_utilization_gpt'
      | '_factuality'
      | '_groundedness'
      | '_latency'
      | '_prompt_perplexity'
      | '_protect_status'
      | '_pii'
      | '_input_pii'
      | '_sexist'
      | '_input_sexist'
      | '_tone'
      | '_input_tone'
      | '_toxicity'
      | '_input_toxicity'
      | '_user_registered'
      | '_user_submitted'
      | '_user_generated'
      | '_uncertainty'
      | '_bleu'
      | '_cost'
      | '_rouge'
      | '_prompt_injection'
      | '_rag_nli'
      | '_adherence_nli'
      | '_completeness_nli'
      | '_chunk_attribution_utilization_nli'
      | '_instruction_adherence'
      | '_ground_truth_adherence'
      | '_tool_selection_quality'
      | '_tool_error_rate'
      | '_agentic_session_success'
      | '_agentic_workflow_success'
      | '_generic_wizard'
      | '_customized_completeness_gpt'
      | '_customized_factuality'
      | '_customized_groundedness'
      | '_customized_chunk_attribution_utilization_gpt'
      | '_customized_instruction_adherence'
      | '_customized_ground_truth_adherence'
      | '_customized_tool_selection_quality'
      | '_customized_tool_error_rate'
      | '_customized_agentic_session_success'
      | '_customized_agentic_workflow_success';
    /**
     * ScorerType
     * @enum {string}
     */
    ScorerType: 'Luna' | 'Plus';
    /**
     * ScorerTypes
     * @enum {string}
     */
    ScorerTypes: 'llm' | 'code' | 'preset';
    /**
     * ScorersConfiguration
     * @description Configure which scorers to enable for a particular prompt run.
     *
     *     The keys here are sorted by their approximate execution time to execute the scorers that we anticipate will be the
     *     fastest first, and the slowest last.
     */
    ScorersConfiguration: {
      /**
       * Latency
       * @default true
       */
      latency?: boolean;
      /**
       * Cost
       * @default true
       */
      cost?: boolean;
      /**
       * Pii
       * @default false
       */
      pii?: boolean;
      /**
       * Input Pii
       * @default false
       */
      input_pii?: boolean;
      /**
       * Bleu
       * @default true
       */
      bleu?: boolean;
      /**
       * Rouge
       * @default true
       */
      rouge?: boolean;
      /**
       * Protect Status
       * @default true
       */
      protect_status?: boolean;
      /**
       * Context Relevance
       * @default false
       */
      context_relevance?: boolean;
      /**
       * Toxicity
       * @default false
       */
      toxicity?: boolean;
      /**
       * Input Toxicity
       * @default false
       */
      input_toxicity?: boolean;
      /**
       * Tone
       * @default false
       */
      tone?: boolean;
      /**
       * Input Tone
       * @default false
       */
      input_tone?: boolean;
      /**
       * Sexist
       * @default false
       */
      sexist?: boolean;
      /**
       * Input Sexist
       * @default false
       */
      input_sexist?: boolean;
      /**
       * Prompt Injection
       * @default false
       */
      prompt_injection?: boolean;
      /**
       * Adherence Nli
       * @default false
       */
      adherence_nli?: boolean;
      /**
       * Chunk Attribution Utilization Nli
       * @default false
       */
      chunk_attribution_utilization_nli?: boolean;
      /**
       * Completeness Nli
       * @default false
       */
      completeness_nli?: boolean;
      /**
       * Uncertainty
       * @default false
       */
      uncertainty?: boolean;
      /**
       * Factuality
       * @default false
       */
      factuality?: boolean;
      /**
       * Groundedness
       * @default false
       */
      groundedness?: boolean;
      /**
       * Prompt Perplexity
       * @default false
       */
      prompt_perplexity?: boolean;
      /**
       * Chunk Attribution Utilization Gpt
       * @default false
       */
      chunk_attribution_utilization_gpt?: boolean;
      /**
       * Completeness Gpt
       * @default false
       */
      completeness_gpt?: boolean;
      /**
       * Instruction Adherence
       * @default false
       */
      instruction_adherence?: boolean;
      /**
       * Ground Truth Adherence
       * @default false
       */
      ground_truth_adherence?: boolean;
      /**
       * Tool Selection Quality
       * @default false
       */
      tool_selection_quality?: boolean;
      /**
       * Tool Error Rate
       * @default false
       */
      tool_error_rate?: boolean;
      /**
       * Agentic Session Success
       * @default false
       */
      agentic_session_success?: boolean;
      /**
       * Agentic Workflow Success
       * @default false
       */
      agentic_workflow_success?: boolean;
    };
    /** Segment */
    Segment: {
      /** Start */
      start: number;
      /** End */
      end: number;
      /** Value */
      value: number | string;
      /** Prob */
      prob?: number | null;
    };
    /** StarAggregate */
    StarAggregate: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      feedback_type: 'star';
      /** Average */
      average: number;
      /** Counts */
      counts: {
        [key: string]: number;
      };
      /** Unrated Count */
      unrated_count: number;
    };
    /** StarRating */
    StarRating: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      feedback_type: 'star';
      /** Value */
      value: number;
    };
    /**
     * StepType
     * @enum {string}
     */
    StepType: 'llm' | 'retriever' | 'tool' | 'workflow' | 'trace';
    /** TagsAggregate */
    TagsAggregate: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      feedback_type: 'tags';
      /** Counts */
      counts: {
        [key: string]: number;
      };
      /** Unrated Count */
      unrated_count: number;
    };
    /** TagsRating */
    TagsRating: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      feedback_type: 'tags';
      /** Value */
      value: string[];
    };
    /**
     * TaskType
     * @description Valid task types for modeling.
     *
     *     We store these as ints instead of strings because we will be looking this up in the database frequently.
     * @enum {integer}
     */
    TaskType:
      | 0
      | 1
      | 2
      | 3
      | 4
      | 5
      | 6
      | 7
      | 8
      | 9
      | 10
      | 11
      | 12
      | 13
      | 14
      | 15
      | 16
      | 17;
    /** TextRating */
    TextRating: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      feedback_type: 'text';
      /** Value */
      value: string;
    };
    /** Token */
    Token: {
      /** Access Token */
      access_token: string;
      /**
       * Token Type
       * @default bearer
       */
      token_type?: string;
    };
    /** ToolCall */
    ToolCall: {
      /** Id */
      id: string;
      function: components['schemas']['ToolCallFunction'];
    };
    /** ToolCallFunction */
    ToolCallFunction: {
      /** Name */
      name: string;
      /** Arguments */
      arguments: string;
    };
    /** ToolErrorRateScorer */
    ToolErrorRateScorer: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'tool_error_rate';
      /**
       * Filters
       * @description List of filters to apply to the scorer.
       */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
      /**
       * Type
       * @default plus
       * @constant
       */
      type?: 'plus';
      /**
       * Model Name
       * @description Alias of the model to use for the scorer.
       */
      model_name?: string | null;
    };
    /**
     * ToolErrorRateTemplate
     * @description Template for the tool error rate metric,
     *     containing all the info necessary to send the tool error rate prompt.
     */
    ToolErrorRateTemplate: {
      /**
       * Metric System Prompt
       * @default One or more functions have been called, and you will receive their output. The output format could be a string containing the tool's result, it could be in JSON or XML format with additional metadata and information, or it could be a list of the outputs in any such format.
       *
       *     Your task is to determine whether at least one function call didn't execute correctly and errored out. If at least one call failed, then you should consider the entire call as a failure.
       *     You should NOT evaluate any other aspect of the tool call. In particular you should not evaluate whether the output is well formatted, coherent or contains spelling mistakes.
       *
       *     If you conclude that the call failed, provide an explanation as to why. You may summarize any error message you encounter. If the call was successful, no explanation is needed.
       *
       *     Respond in the following JSON format:
       *
       *     ```
       *     {
       *        "function_errored_out": boolean,
       *        "explanation": string
       *     }
       *     ```
       *
       *     - **"function_errored_out"**: Use `false` if all tool calls were successful, and `true` if at least one errored out.
       *
       *     - **"explanation"**: If a tool call failed, provide your step-by-step reasoning to determine why it might have failed. If all tool calls were succesful, leave this blank.
       *
       *     You must respond with a valid JSON object; don't forget to escape special characters.
       */
      metric_system_prompt?: string;
      /**
       * Metric Description
       * @default I have a multi-turn chatbot application where the assistant is an agent that has access to tools. I want a metric to evaluate whether a tool invocation was successful or if it resulted in an error.
       */
      metric_description?: string;
      /**
       * Value Field Name
       * @default function_errored_out
       */
      value_field_name?: string;
      /**
       * Explanation Field Name
       * @description Field name to look for in the chainpoll response, for the explanation.
       * @default explanation
       */
      explanation_field_name?: string;
      /**
       * Template
       * @default Tools output:
       *     ```
       *     {response}
       *     ```
       */
      template?: string;
      /**
       * Metric Few Shot Examples
       * @default [
       *       {
       *         "generation_prompt_and_response": "Tools output:\n```\n0\n```",
       *         "evaluating_response": "{\n  \"function_errored_out\": false,\n  \"explanation\": \"\"\n}"
       *       },
       *       {
       *         "generation_prompt_and_response": "Tools output:\n```\n{\n  \"error\": \"InvalidFunctionArgumentException\",\n  \"status_code\": 400\n}\n```",
       *         "evaluating_response": "{\n  \"function_errored_out\": true,\n  \"explanation\": \"The call failed due to an InvalidFunctionArgumentException.\",\n}"
       *       }
       *     ]
       */
      metric_few_shot_examples?: components['schemas']['FewShotExample'][];
    };
    /** ToolSelectionQualityScorer */
    ToolSelectionQualityScorer: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'tool_selection_quality';
      /**
       * Filters
       * @description List of filters to apply to the scorer.
       */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
      /**
       * Type
       * @default plus
       * @constant
       */
      type?: 'plus';
      /**
       * Model Name
       * @description Alias of the model to use for the scorer.
       */
      model_name?: string | null;
      /**
       * Num Judges
       * @description Number of judges for the scorer.
       */
      num_judges?: number | null;
    };
    /**
     * ToolSelectionQualityTemplate
     * @description Template for the tool selection quality metric,
     *     containing all the info necessary to send the tool selection quality prompt.
     */
    ToolSelectionQualityTemplate: {
      /**
       * Metric System Prompt
       * @default You will receive the chat history from a chatbot application. At the end of the  conversation, it will be the bots turn to act. The bot has several options: it can reflect and plan its next steps, choose to call tools, or respond directly to the user. If the bot opts to use tools, the tools execute separately, and the bot will subsequently review the output from those tools. Ultimately, the bot should reply to the user, choosing the relevant parts of the tools' output.
       *
       *     Your task is to evaluate the bot's decision-making process and ensure it follows these guidelines:
       *     - If all user queries have already been answered and can be found in the chat history, the bot should not call tools.
       *     - If no suitable tools are available to assist with user queries, the bot should not call tools.
       *     - If the chat history contains all the necessary information to directly answer all user queries, the bot should not call tools.
       *     - If the bot decided to call tools, the tools and argument values selected must relate to at least part of one user query.
       *     - If the bot decided to call tools, all arguments marked as "required" in the tools' schema must be provided with values.
       *
       *     Remember that there are many ways the bot's actions can comply with these rules. Your role is to determine whether the bot fundamentally violated any of these rules, not whether it chose the most optimal response.
       *
       *     Respond in the following JSON format:
       *     ```
       *     {
       *         "explanation": string,
       *         "bot_answer_follows_rules": boolean
       *     }
       *     ```
       *
       *     - **"explanation"**: Provide your step-by-step reasoning to determine whether the bot's reply follows the above-mentioned guidelines.
       *
       *     - **"bot_answer_follows_rules"**: Respond `true` if you believe the bot followed the above guidelines, respond `false` otherwise.
       *
       *     You must respond with a valid JSON object; don't forget to escape special characters.
       */
      metric_system_prompt?: string;
      /**
       * Metric Description
       * @default I have a multi-turn chatbot application where the assistant is an agent that has access to tools. I want a metric that assesses whether the assistant made the correct decision in choosing to either use tools or to directly respond, and in cases where it uses tools, whether it selected the correct tools with the correct arguments.
       */
      metric_description?: string;
      /**
       * Value Field Name
       * @default bot_answer_follows_rules
       */
      value_field_name?: string;
      /**
       * Explanation Field Name
       * @description Field name to look for in the chainpoll response, for the explanation.
       * @default explanation
       */
      explanation_field_name?: string;
      /**
       * Template
       * @default Chatbot history:
       *     ```
       *     {query}
       *     ```
       *
       *     The bot's available tools:
       *     ```
       *     {tools}
       *     ```
       *
       *     The answer to evaluate:
       *     ```
       *     {response}
       *     ```
       */
      template?: string;
      /**
       * Metric Few Shot Examples
       * @default [
       *       {
       *         "generation_prompt_and_response": "Chatbot history:\n```\nhuman: Do the genuses Sapium and Aristotelia belong to the same family?\n```\n\nThe bot's available tools:\n```\n[{'name': 'Search',\n  'description': 'Search for the query',\n  'parameters': {'type': 'object',\n   'properties': {'query': {'type': 'string',\n     'description': 'the query to search'}},\n   'required': ['query']}},\n {'name': 'Lookup',\n  'description': 'Lookup the keyword',\n  'parameters': {'type': 'object',\n   'properties': {'keyword': {'type': 'string',\n     'description': 'the keyword to lookup'}},\n   'required': ['keyword']}}]\n```\n\nThe answer to evaluate:\n```\n{\"Thought\": \"I need to search Sapium and Aristotelia, find their families, then find if they are the same.\", \"Action\": {\"name\": \"Search\", \"arguments\": {\"query\": \"Sapium\"}}}\n```",
       *         "evaluating_response": "{\n    \"explanation\": \"The user asked if the genuses Sapium and Aristotelia belong to the same family. The bot decided to search for 'Sapium', which is associated with part of the user's query regarding the family of Sapium. The action aligns with the user's query, and all required arguments ('query') are provided, making the tool call valid. Furthermore, the information required to answer the user's question is not present in the chat history, justifying the bot's decision to call a tool.\",\n    \"bot_answer_follows_rules\": true\n}"
       *       }
       *     ]
       */
      metric_few_shot_examples?: components['schemas']['FewShotExample'][];
    };
    /** ToolSpan */
    ToolSpan: {
      /**
       * @description Type: must be `tool` (enum property replaced by openapi-typescript)
       * @enum {string}
       */
      type: 'tool';
      /**
       * Input
       * @description Input to the tool step.
       */
      input: string;
      /**
       * Output
       * @description Output of the tool step.
       */
      output?: string | null;
      /**
       * Name
       * @description Name of the step.
       * @default
       */
      name?: string;
      /**
       * Created At
       * Format: date-time
       * @description Timestamp of the step's creation
       */
      created_at?: string;
      /**
       * User Metadata
       * @description Metadata associated with this step.
       */
      user_metadata?: {
        [key: string]: string;
      };
      /**
       * Tags
       * @description Tags associated with this step.
       */
      tags?: string[];
      /**
       * Status Code
       * @description Status code of the step. Used for logging failed/errored steps.
       */
      status_code?: number | null;
      /** @description Metrics associated with this step. */
      metrics?: components['schemas']['Metrics'];
      /**
       * Tool Call Id
       * @description Tool call ID.
       */
      tool_call_id?: string | null;
    };
    /** ToolSpanRecord */
    ToolSpanRecord: {
      /**
       * @description Type: must be `tool` (enum property replaced by openapi-typescript)
       * @enum {string}
       */
      type: 'tool';
      /**
       * Input
       * @description Input to the tool step.
       */
      input: string;
      /**
       * Output
       * @description Output of the tool step.
       */
      output?: string | null;
      /**
       * Name
       * @description Name of the step.
       * @default
       */
      name?: string;
      /**
       * Created At
       * Format: date-time
       * @description Timestamp of the step's creation
       */
      created_at?: string;
      /**
       * User Metadata
       * @description Metadata associated with this step.
       */
      user_metadata?: {
        [key: string]: string;
      };
      /**
       * Tags
       * @description Tags associated with this step.
       */
      tags?: string[];
      /**
       * Status Code
       * @description Status code of the step. Used for logging failed/errored steps.
       */
      status_code?: number | null;
      /** @description Metrics associated with this step. */
      metrics?: components['schemas']['Metrics'];
      /**
       * ID
       * Format: uuid4
       * @description Galileo ID of the trace or span
       */
      id: string;
      /**
       * Trace ID
       * Format: uuid4
       * @description Galileo ID of the trace containing the span (or the same value as id for a trace)
       */
      trace_id: string;
      /**
       * Project ID
       * Format: uuid4
       * @description Galileo ID of the project associated with this trace or span
       */
      project_id: string;
      /**
       * Run ID
       * Format: uuid4
       * @description Galileo ID of the run (log stream or experiment) associated with this trace or span
       */
      run_id: string;
      /**
       * Updated At
       * @description Timestamp of the trace or span's last update
       */
      updated_at?: string | null;
      /**
       * Has Children
       * @description Whether or not this trace or span has child spans
       */
      has_children?: boolean | null;
      /**
       * Metrics Batch Id
       * @description Galileo ID of the metrics batch associated with this trace or span
       */
      metrics_batch_id?: string | null;
      /** @description Detailed information about the metrics associated with this trace or span */
      metric_info?: components['schemas']['TransactionMetricInfo'] | null;
      /**
       * Parent ID
       * Format: uuid4
       * @description Galileo ID of the parent of this span
       */
      parent_id: string;
      /**
       * Tool Call Id
       * @description Tool call ID.
       */
      tool_call_id?: string | null;
    };
    /** Trace */
    Trace: {
      /**
       * Type
       * @description Type: must be `trace`
       * @default trace
       * @constant
       */
      type?: 'trace';
      /**
       * Input
       * @description Input to the step.
       */
      input: string;
      /**
       * Output
       * @description Output of the step.
       */
      output?: string | null;
      /**
       * Name
       * @description Name of the step.
       * @default
       */
      name?: string;
      /**
       * Created At
       * Format: date-time
       * @description Timestamp of the step's creation
       */
      created_at?: string;
      /**
       * User Metadata
       * @description Metadata associated with this step.
       */
      user_metadata?: {
        [key: string]: string;
      };
      /**
       * Tags
       * @description Tags associated with this step.
       */
      tags?: string[];
      /**
       * Status Code
       * @description Status code of the step. Used for logging failed/errored steps.
       */
      status_code?: number | null;
      /** @description Metrics associated with this step. */
      metrics?: components['schemas']['Metrics'];
      /**
       * Spans
       * @description Child spans.
       */
      spans?: (
        | components['schemas']['WorkflowSpan']
        | components['schemas']['LlmSpan']
        | components['schemas']['RetrieverSpan']
        | components['schemas']['ToolSpan']
      )[];
    };
    /** TraceRecord */
    TraceRecord: {
      /**
       * Type
       * @description Type: must be `trace`
       * @default trace
       * @constant
       */
      type?: 'trace';
      /**
       * Input
       * @description Input to the step.
       */
      input: string;
      /**
       * Output
       * @description Output of the step.
       */
      output?: string | null;
      /**
       * Name
       * @description Name of the step.
       * @default
       */
      name?: string;
      /**
       * Created At
       * Format: date-time
       * @description Timestamp of the step's creation
       */
      created_at?: string;
      /**
       * User Metadata
       * @description Metadata associated with this step.
       */
      user_metadata?: {
        [key: string]: string;
      };
      /**
       * Tags
       * @description Tags associated with this step.
       */
      tags?: string[];
      /**
       * Status Code
       * @description Status code of the step. Used for logging failed/errored steps.
       */
      status_code?: number | null;
      /** @description Metrics associated with this step. */
      metrics?: components['schemas']['Metrics'];
      /**
       * ID
       * Format: uuid4
       * @description Galileo ID of the trace or span
       */
      id: string;
      /**
       * Trace ID
       * Format: uuid4
       * @description Galileo ID of the trace containing the span (or the same value as id for a trace)
       */
      trace_id: string;
      /**
       * Project ID
       * Format: uuid4
       * @description Galileo ID of the project associated with this trace or span
       */
      project_id: string;
      /**
       * Run ID
       * Format: uuid4
       * @description Galileo ID of the run (log stream or experiment) associated with this trace or span
       */
      run_id: string;
      /**
       * Updated At
       * @description Timestamp of the trace or span's last update
       */
      updated_at?: string | null;
      /**
       * Has Children
       * @description Whether or not this trace or span has child spans
       */
      has_children?: boolean | null;
      /**
       * Metrics Batch Id
       * @description Galileo ID of the metrics batch associated with this trace or span
       */
      metrics_batch_id?: string | null;
      /** @description Detailed information about the metrics associated with this trace or span */
      metric_info?: components['schemas']['TransactionMetricInfo'] | null;
    };
    /** TraceRecordWithChildren */
    TraceRecordWithChildren: {
      /** Spans */
      spans?: (
        | components['schemas']['WorkflowSpanRecordWithChildren']
        | components['schemas']['LlmSpanRecord']
        | components['schemas']['ToolSpanRecord']
        | components['schemas']['RetrieverSpanRecord']
      )[];
      /**
       * Type
       * @description Type: must be `trace`
       * @default trace
       * @constant
       */
      type?: 'trace';
      /**
       * Input
       * @description Input to the step.
       */
      input: string;
      /**
       * Output
       * @description Output of the step.
       */
      output?: string | null;
      /**
       * Name
       * @description Name of the step.
       * @default
       */
      name?: string;
      /**
       * Created At
       * Format: date-time
       * @description Timestamp of the step's creation
       */
      created_at?: string;
      /**
       * User Metadata
       * @description Metadata associated with this step.
       */
      user_metadata?: {
        [key: string]: string;
      };
      /**
       * Tags
       * @description Tags associated with this step.
       */
      tags?: string[];
      /**
       * Status Code
       * @description Status code of the step. Used for logging failed/errored steps.
       */
      status_code?: number | null;
      /** @description Metrics associated with this step. */
      metrics?: components['schemas']['Metrics'];
      /**
       * ID
       * Format: uuid4
       * @description Galileo ID of the trace or span
       */
      id: string;
      /**
       * Trace ID
       * Format: uuid4
       * @description Galileo ID of the trace containing the span (or the same value as id for a trace)
       */
      trace_id: string;
      /**
       * Project ID
       * Format: uuid4
       * @description Galileo ID of the project associated with this trace or span
       */
      project_id: string;
      /**
       * Run ID
       * Format: uuid4
       * @description Galileo ID of the run (log stream or experiment) associated with this trace or span
       */
      run_id: string;
      /**
       * Updated At
       * @description Timestamp of the trace or span's last update
       */
      updated_at?: string | null;
      /**
       * Has Children
       * @description Whether or not this trace or span has child spans
       */
      has_children?: boolean | null;
      /**
       * Metrics Batch Id
       * @description Galileo ID of the metrics batch associated with this trace or span
       */
      metrics_batch_id?: string | null;
      /** @description Detailed information about the metrics associated with this trace or span */
      metric_info?: components['schemas']['TransactionMetricInfo'] | null;
    };
    /** TransactionMetricInfo */
    TransactionMetricInfo: {
      /**
       * Chunk Attribution
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      chunk_attribution?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Chunk Length
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      chunk_length?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Chunk Utilized Substrings
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      chunk_utilized_substrings?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Chunk Utilized Substrings Offsets
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      chunk_utilized_substrings_offsets?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Chunk Utilization
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      chunk_utilization?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Chunk Utilized
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      chunk_utilized?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Chunk Relevance
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      chunk_relevance?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Chunk Relevant
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      chunk_relevant?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Chunk Relevant Substrings
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      chunk_relevant_substrings?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Chunk Relevant Substrings Offsets
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      chunk_relevant_substrings_offsets?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Rag Nli Non Adherent Substrings
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      rag_nli_non_adherent_substrings?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Rag Nli Non Adherent Substrings Offsets
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      rag_nli_non_adherent_substrings_offsets?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Chunks Attributed
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      chunks_attributed?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Chunks Length
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      chunks_length?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Chunks Unattributed
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      chunks_unattributed?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Chunks Utilized
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      chunks_utilized?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Completeness Gpt
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      completeness_gpt?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Cost
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      cost?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Factuality
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      factuality?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Groundedness
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      groundedness?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Input Pii
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      input_pii?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Input Pii Spans
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      input_pii_spans?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Input Pii Spans Prob
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      input_pii_spans_prob?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Input Pii Spans Offset
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      input_pii_spans_offset?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Input Sexist
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      input_sexist?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Input Tone
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      input_tone?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Input Toxicity
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      input_toxicity?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Instruction Adherence
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      instruction_adherence?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Latency
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      latency?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Pii
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      pii?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Pii Spans
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      pii_spans?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Pii Spans Prob
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      pii_spans_prob?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Pii Spans Offset
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      pii_spans_offset?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Prompt Perplexity
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      prompt_perplexity?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Protect Status
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      protect_status?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Retriever Attribution
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      retriever_attribution?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Retriever Utilization
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      retriever_utilization?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Sexist
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      sexist?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Toxicity
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      toxicity?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Tone
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      tone?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Prompt Injection
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      prompt_injection?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Tool Selection Quality
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      tool_selection_quality?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Tool Error Rate
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      tool_error_rate?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Agentic Workflow Success
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      agentic_workflow_success?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
      /**
       * Agentic Session Success
       * @default {
       *       "status_type": "not_computed",
       *       "message": "Metric not computed."
       *     }
       */
      agentic_session_success?:
        | components['schemas']['MetricNotComputed']
        | components['schemas']['MetricPending']
        | components['schemas']['MetricComputing']
        | components['schemas']['MetricNotApplicable']
        | components['schemas']['MetricSuccess']
        | components['schemas']['MetricError']
        | components['schemas']['MetricFailed'];
    } & {
      [key: string]: unknown;
    };
    /** UncertaintyScorer */
    UncertaintyScorer: {
      /**
       * @description discriminator enum property added by openapi-typescript
       * @enum {string}
       */
      name: 'uncertainty';
      /**
       * Filters
       * @description List of filters to apply to the scorer.
       */
      filters?:
        | (
            | components['schemas']['NodeNameFilter']
            | components['schemas']['MetadataFilter']
          )[]
        | null;
    };
    /** UpdateDatasetContentRequest */
    UpdateDatasetContentRequest: {
      /** Edits */
      edits: (
        | components['schemas']['DatasetAppendRow']
        | components['schemas']['DatasetUpdateRow']
        | components['schemas']['DatasetDeleteRow']
        | components['schemas']['DatasetDeleteColumn']
        | components['schemas']['DatasetRenameColumn']
        | components['schemas']['DatasetAddColumn']
      )[];
    };
    /** UpdateDatasetRequest */
    UpdateDatasetRequest: {
      /** Name */
      name?: string | null;
      column_mapping?: components['schemas']['ColumnMapping'] | null;
    };
    /** UpdateDatasetVersionRequest */
    UpdateDatasetVersionRequest: {
      /** Name */
      name?: string | null;
    };
    /**
     * UserAction
     * @enum {string}
     */
    UserAction: 'update' | 'delete' | 'read_api_keys';
    /** UserCollaborator */
    UserCollaborator: {
      /**
       * Id
       * Format: uuid4
       */
      id: string;
      /**
       * Permissions
       * @default []
       */
      permissions?: components['schemas']['Permission'][];
      role: components['schemas']['CollaboratorRole'];
      /**
       * Created At
       * Format: date-time
       */
      created_at: string;
      /**
       * User Id
       * Format: uuid4
       */
      user_id: string;
      /** First Name */
      first_name: string | null;
      /** Last Name */
      last_name: string | null;
      /** Email */
      email: string;
    };
    /** UserCollaboratorCreate */
    UserCollaboratorCreate: {
      /** @default viewer */
      role?: components['schemas']['CollaboratorRole'];
      /**
       * User Id
       * Format: uuid4
       */
      user_id: string;
    };
    /** UserDB */
    UserDB: {
      /**
       * Id
       * Format: uuid4
       */
      id: string;
      /**
       * Permissions
       * @default []
       */
      permissions?: components['schemas']['Permission'][];
      /** Email */
      email: string;
      /**
       * First Name
       * @default
       */
      first_name?: string | null;
      /**
       * Last Name
       * @default
       */
      last_name?: string | null;
      /** @default email */
      auth_method?: components['schemas']['AuthMethod'];
      /** @default read_only */
      role?: components['schemas']['UserRole'];
      /** Email Is Verified */
      email_is_verified?: boolean | null;
      /**
       * Organization Id
       * Format: uuid4
       */
      organization_id: string;
      /** Organization Name */
      organization_name: string;
      /**
       * Created At
       * Format: date-time
       */
      created_at: string;
      /**
       * Updated At
       * Format: date-time
       */
      updated_at: string;
    };
    /**
     * UserInfo
     * @description A user's basic information, used for display purposes.
     */
    UserInfo: {
      /**
       * Id
       * Format: uuid4
       */
      id: string;
      /** Email */
      email: string;
      /** First Name */
      first_name?: string | null;
      /** Last Name */
      last_name?: string | null;
    };
    /**
     * UserRole
     * @enum {string}
     */
    UserRole: 'admin' | 'manager' | 'user' | 'read_only';
    /** ValidationError */
    ValidationError: {
      /** Location */
      loc: (string | number)[];
      /** Message */
      msg: string;
      /** Error Type */
      type: string;
    };
    /** WorkflowSpan */
    WorkflowSpan: {
      /**
       * @description Type: must be `workflow` (enum property replaced by openapi-typescript)
       * @enum {string}
       */
      type: 'workflow';
      /**
       * Input
       * @description Input to the step.
       */
      input:
        | string
        | components['schemas']['galileo_core__schemas__logging__llm__Message-Input'][];
      /**
       * Output
       * @description Output of the step.
       */
      output?:
        | string
        | components['schemas']['galileo_core__schemas__logging__llm__Message-Input']
        | components['schemas']['Document-Input'][]
        | null;
      /**
       * Name
       * @description Name of the step.
       * @default
       */
      name?: string;
      /**
       * Created At
       * Format: date-time
       * @description Timestamp of the step's creation
       */
      created_at?: string;
      /**
       * User Metadata
       * @description Metadata associated with this step.
       */
      user_metadata?: {
        [key: string]: string;
      };
      /**
       * Tags
       * @description Tags associated with this step.
       */
      tags?: string[];
      /**
       * Status Code
       * @description Status code of the step. Used for logging failed/errored steps.
       */
      status_code?: number | null;
      /** @description Metrics associated with this step. */
      metrics?: components['schemas']['Metrics'];
      /**
       * Spans
       * @description Child spans.
       */
      spans?: (
        | components['schemas']['WorkflowSpan']
        | components['schemas']['LlmSpan']
        | components['schemas']['RetrieverSpan']
        | components['schemas']['ToolSpan']
      )[];
    };
    /** WorkflowSpanRecord */
    WorkflowSpanRecord: {
      /**
       * @description Type: must be `workflow` (enum property replaced by openapi-typescript)
       * @enum {string}
       */
      type: 'workflow';
      /**
       * Input
       * @description Input to the step.
       */
      input:
        | string
        | components['schemas']['galileo_core__schemas__logging__llm__Message-Output'][];
      /**
       * Output
       * @description Output of the step.
       */
      output?:
        | string
        | components['schemas']['galileo_core__schemas__logging__llm__Message-Output']
        | components['schemas']['Document-Output'][]
        | null;
      /**
       * Name
       * @description Name of the step.
       * @default
       */
      name?: string;
      /**
       * Created At
       * Format: date-time
       * @description Timestamp of the step's creation
       */
      created_at?: string;
      /**
       * User Metadata
       * @description Metadata associated with this step.
       */
      user_metadata?: {
        [key: string]: string;
      };
      /**
       * Tags
       * @description Tags associated with this step.
       */
      tags?: string[];
      /**
       * Status Code
       * @description Status code of the step. Used for logging failed/errored steps.
       */
      status_code?: number | null;
      /** @description Metrics associated with this step. */
      metrics?: components['schemas']['Metrics'];
      /**
       * ID
       * Format: uuid4
       * @description Galileo ID of the trace or span
       */
      id: string;
      /**
       * Trace ID
       * Format: uuid4
       * @description Galileo ID of the trace containing the span (or the same value as id for a trace)
       */
      trace_id: string;
      /**
       * Project ID
       * Format: uuid4
       * @description Galileo ID of the project associated with this trace or span
       */
      project_id: string;
      /**
       * Run ID
       * Format: uuid4
       * @description Galileo ID of the run (log stream or experiment) associated with this trace or span
       */
      run_id: string;
      /**
       * Updated At
       * @description Timestamp of the trace or span's last update
       */
      updated_at?: string | null;
      /**
       * Has Children
       * @description Whether or not this trace or span has child spans
       */
      has_children?: boolean | null;
      /**
       * Metrics Batch Id
       * @description Galileo ID of the metrics batch associated with this trace or span
       */
      metrics_batch_id?: string | null;
      /** @description Detailed information about the metrics associated with this trace or span */
      metric_info?: components['schemas']['TransactionMetricInfo'] | null;
      /**
       * Parent ID
       * Format: uuid4
       * @description Galileo ID of the parent of this span
       */
      parent_id: string;
    };
    /** WorkflowSpanRecordWithChildren */
    WorkflowSpanRecordWithChildren: {
      /** Spans */
      spans?: (
        | components['schemas']['WorkflowSpanRecordWithChildren']
        | components['schemas']['LlmSpanRecord']
        | components['schemas']['ToolSpanRecord']
        | components['schemas']['RetrieverSpanRecord']
      )[];
      /**
       * @description Type: must be `workflow` (enum property replaced by openapi-typescript)
       * @enum {string}
       */
      type: 'workflow';
      /**
       * Input
       * @description Input to the step.
       */
      input:
        | string
        | components['schemas']['galileo_core__schemas__logging__llm__Message-Output'][];
      /**
       * Output
       * @description Output of the step.
       */
      output?:
        | string
        | components['schemas']['galileo_core__schemas__logging__llm__Message-Output']
        | components['schemas']['Document-Output'][]
        | null;
      /**
       * Name
       * @description Name of the step.
       * @default
       */
      name?: string;
      /**
       * Created At
       * Format: date-time
       * @description Timestamp of the step's creation
       */
      created_at?: string;
      /**
       * User Metadata
       * @description Metadata associated with this step.
       */
      user_metadata?: {
        [key: string]: string;
      };
      /**
       * Tags
       * @description Tags associated with this step.
       */
      tags?: string[];
      /**
       * Status Code
       * @description Status code of the step. Used for logging failed/errored steps.
       */
      status_code?: number | null;
      /** @description Metrics associated with this step. */
      metrics?: components['schemas']['Metrics'];
      /**
       * ID
       * Format: uuid4
       * @description Galileo ID of the trace or span
       */
      id: string;
      /**
       * Trace ID
       * Format: uuid4
       * @description Galileo ID of the trace containing the span (or the same value as id for a trace)
       */
      trace_id: string;
      /**
       * Project ID
       * Format: uuid4
       * @description Galileo ID of the project associated with this trace or span
       */
      project_id: string;
      /**
       * Run ID
       * Format: uuid4
       * @description Galileo ID of the run (log stream or experiment) associated with this trace or span
       */
      run_id: string;
      /**
       * Updated At
       * @description Timestamp of the trace or span's last update
       */
      updated_at?: string | null;
      /**
       * Has Children
       * @description Whether or not this trace or span has child spans
       */
      has_children?: boolean | null;
      /**
       * Metrics Batch Id
       * @description Galileo ID of the metrics batch associated with this trace or span
       */
      metrics_batch_id?: string | null;
      /** @description Detailed information about the metrics associated with this trace or span */
      metric_info?: components['schemas']['TransactionMetricInfo'] | null;
      /**
       * Parent ID
       * Format: uuid4
       * @description Galileo ID of the parent of this span
       */
      parent_id: string;
    };
    /** Message */
    'galileo_core__schemas__logging__llm__Message-Input': {
      /** Content */
      content: string;
      role: components['schemas']['galileo_core__schemas__logging__llm__MessageRole'];
      /** Tool Call Id */
      tool_call_id?: string | null;
      /** Tool Calls */
      tool_calls?: components['schemas']['ToolCall'][] | null;
    };
    /** Message */
    'galileo_core__schemas__logging__llm__Message-Output': {
      /** Content */
      content: string;
      role: components['schemas']['galileo_core__schemas__logging__llm__MessageRole'];
      /** Tool Call Id */
      tool_call_id?: string | null;
      /** Tool Calls */
      tool_calls?: components['schemas']['ToolCall'][] | null;
    };
    /**
     * MessageRole
     * @enum {string}
     */
    galileo_core__schemas__logging__llm__MessageRole:
      | 'agent'
      | 'assistant'
      | 'developer'
      | 'function'
      | 'system'
      | 'tool'
      | 'user';
    /** Message */
    galileo_core__schemas__shared__message__Message: {
      /** Content */
      content: string;
      /** Role */
      role:
        | string
        | components['schemas']['galileo_core__schemas__shared__message_role__MessageRole'];
    } & {
      [key: string]: unknown;
    };
    /**
     * MessageRole
     * @enum {string}
     */
    galileo_core__schemas__shared__message_role__MessageRole:
      | 'agent'
      | 'assistant'
      | 'function'
      | 'system'
      | 'tool'
      | 'user';
  };
  responses: never;
  parameters: never;
  requestBodies: never;
  headers: never;
  pathItems: never;
}
export type $defs = Record<string, never>;
export interface operations {
  healthcheck_healthcheck_get: {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HealthcheckResponse'];
        };
      };
    };
  };
  login_email_login_post: {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    requestBody: {
      content: {
        'application/x-www-form-urlencoded': components['schemas']['Body_login_email_login_post'];
      };
    };
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['Token'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  login_api_key_login_api_key_post: {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    requestBody: {
      content: {
        'application/json': components['schemas']['ApiKeyLoginRequest'];
      };
    };
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['Token'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  list_prompt_datasets_projects__project_id__prompt_datasets_get: {
    parameters: {
      query?: {
        starting_token?: number;
        limit?: number;
      };
      header?: never;
      path: {
        project_id: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['ListPromptDatasetResponse'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  upload_prompt_evaluation_dataset_projects__project_id__prompt_datasets_post: {
    parameters: {
      query?: {
        format?: components['schemas']['DatasetFormat'];
      };
      header?: never;
      path: {
        project_id: string;
      };
      cookie?: never;
    };
    requestBody: {
      content: {
        'multipart/form-data': components['schemas']['Body_upload_prompt_evaluation_dataset_projects__project_id__prompt_datasets_post'];
      };
    };
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['PromptDatasetDB'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  list_datasets_datasets_get: {
    parameters: {
      query?: {
        /** @description Actions to include in the 'permissions' field. */
        actions?: components['schemas']['DatasetAction'][];
        starting_token?: number;
        limit?: number;
      };
      header?: never;
      path?: never;
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['ListDatasetResponse'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  create_dataset_datasets_post: {
    parameters: {
      query?: {
        format?: components['schemas']['DatasetFormat'];
      };
      header?: never;
      path?: never;
      cookie?: never;
    };
    requestBody: {
      content: {
        'multipart/form-data': components['schemas']['Body_create_dataset_datasets_post'];
      };
    };
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['DatasetDB'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  get_dataset_datasets__dataset_id__get: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        dataset_id: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['DatasetDB'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  delete_dataset_datasets__dataset_id__delete: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        dataset_id: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': unknown;
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  update_dataset_datasets__dataset_id__patch: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        dataset_id: string;
      };
      cookie?: never;
    };
    requestBody: {
      content: {
        'application/json': components['schemas']['UpdateDatasetRequest'];
      };
    };
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['DatasetDB'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  query_datasets_datasets_query_post: {
    parameters: {
      query?: {
        /** @description Actions to include in the 'permissions' field. */
        actions?: components['schemas']['DatasetAction'][];
        starting_token?: number;
        limit?: number;
      };
      header?: never;
      path?: never;
      cookie?: never;
    };
    requestBody?: {
      content: {
        'application/json': components['schemas']['ListDatasetParams'];
      };
    };
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['ListDatasetResponse'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  download_prompt_dataset_projects__project_id__prompt_datasets__dataset_id__get: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        project_id: string;
        dataset_id: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  update_prompt_dataset_projects__project_id__prompt_datasets__dataset_id__put: {
    parameters: {
      query?: {
        file_name?: string | null;
        num_rows?: number | null;
        format?: components['schemas']['DatasetFormat'];
      };
      header?: never;
      path: {
        project_id: string;
        dataset_id: string;
      };
      cookie?: never;
    };
    requestBody?: {
      content: {
        'multipart/form-data': components['schemas']['Body_update_prompt_dataset_projects__project_id__prompt_datasets__dataset_id__put'];
      };
    };
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['PromptDatasetDB'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  delete_prompt_dataset_projects__project_id__prompt_datasets__dataset_id__delete: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        project_id: string;
        dataset_id: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': unknown;
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  get_dataset_content_datasets__dataset_id__content_get: {
    parameters: {
      query?: {
        starting_token?: number;
        limit?: number;
      };
      header?: never;
      path: {
        dataset_id: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['DatasetContent'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  rollback_dataset_content_datasets__dataset_id__content_put: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        dataset_id: string;
      };
      cookie?: never;
    };
    requestBody: {
      content: {
        'application/json': components['schemas']['RollbackRequest'];
      };
    };
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': unknown;
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  update_dataset_content_datasets__dataset_id__content_patch: {
    parameters: {
      query?: never;
      header?: {
        /**
         * @description ETag of the dataset as a version identifier.
         * @example d89cce33-549d-4b6d-b220-afb641d859c8
         */
        'If-Match'?: string | null;
      };
      path: {
        dataset_id: string;
      };
      cookie?: never;
    };
    requestBody: {
      content: {
        'application/json': components['schemas']['UpdateDatasetContentRequest'];
      };
    };
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': unknown;
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  download_dataset_datasets__dataset_id__download_get: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        dataset_id: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': unknown;
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  preview_dataset_datasets__dataset_id__preview_post: {
    parameters: {
      query?: {
        starting_token?: number;
        limit?: number;
      };
      header?: never;
      path: {
        dataset_id: string;
      };
      cookie?: never;
    };
    requestBody: {
      content: {
        'application/json': components['schemas']['PreviewDatasetRequest'];
      };
    };
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['DatasetContent'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  query_dataset_content_datasets__dataset_id__content_query_post: {
    parameters: {
      query?: {
        starting_token?: number;
        limit?: number;
      };
      header?: never;
      path: {
        dataset_id: string;
      };
      cookie?: never;
    };
    requestBody?: {
      content: {
        'application/json': components['schemas']['QueryDatasetParams'];
      };
    };
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['DatasetContent'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  list_user_dataset_collaborators_datasets__dataset_id__users_get: {
    parameters: {
      query?: {
        starting_token?: number;
        limit?: number;
      };
      header?: never;
      path: {
        dataset_id: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['ListUserCollaboratorsResponse'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  create_user_dataset_collaborators_datasets__dataset_id__users_post: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        dataset_id: string;
      };
      cookie?: never;
    };
    requestBody: {
      content: {
        'application/json': components['schemas']['UserCollaboratorCreate'][];
      };
    };
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['UserCollaborator'][];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  list_group_dataset_collaborators_datasets__dataset_id__groups_get: {
    parameters: {
      query?: {
        starting_token?: number;
        limit?: number;
      };
      header?: never;
      path: {
        dataset_id: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['ListGroupCollaboratorsResponse'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  create_group_dataset_collaborators_datasets__dataset_id__groups_post: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        dataset_id: string;
      };
      cookie?: never;
    };
    requestBody: {
      content: {
        'application/json': components['schemas']['GroupCollaboratorCreate'][];
      };
    };
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['GroupCollaborator'][];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  delete_user_dataset_collaborator_datasets__dataset_id__users__user_id__delete: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        dataset_id: string;
        user_id: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': unknown;
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  update_user_dataset_collaborator_datasets__dataset_id__users__user_id__patch: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        dataset_id: string;
        user_id: string;
      };
      cookie?: never;
    };
    requestBody: {
      content: {
        'application/json': components['schemas']['CollaboratorUpdate'];
      };
    };
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['UserCollaborator'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  delete_group_dataset_collaborator_datasets__dataset_id__groups__group_id__delete: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        dataset_id: string;
        group_id: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': unknown;
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  update_group_dataset_collaborator_datasets__dataset_id__groups__group_id__patch: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        dataset_id: string;
        group_id: string;
      };
      cookie?: never;
    };
    requestBody: {
      content: {
        'application/json': components['schemas']['CollaboratorUpdate'];
      };
    };
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['GroupCollaborator'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  query_dataset_versions_datasets__dataset_id__versions_query_post: {
    parameters: {
      query?: {
        starting_token?: number;
        limit?: number;
      };
      header?: never;
      path: {
        dataset_id: string;
      };
      cookie?: never;
    };
    requestBody?: {
      content: {
        'application/json': components['schemas']['ListDatasetVersionParams'];
      };
    };
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['ListDatasetVersionResponse'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  get_dataset_version_content_datasets__dataset_id__versions__version_index__content_get: {
    parameters: {
      query?: {
        starting_token?: number;
        limit?: number;
      };
      header?: never;
      path: {
        dataset_id: string;
        version_index: number;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['DatasetContent'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  update_dataset_version_datasets__dataset_id__versions__version_index__patch: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        dataset_id: string;
        version_index: number;
      };
      cookie?: never;
    };
    requestBody: {
      content: {
        'application/json': components['schemas']['UpdateDatasetVersionRequest'];
      };
    };
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['DatasetVersionDB'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  get_collaborator_roles_collaborator_roles_get: {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['CollaboratorRoleInfo'][];
        };
      };
    };
  };
  get_projects_projects_get: {
    parameters: {
      query?: {
        project_name?: string | null;
        type?: components['schemas']['ProjectType'] | null;
      };
      header?: never;
      path?: never;
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['ProjectDB'][];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  create_project_projects_post: {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    requestBody: {
      content: {
        'application/json': components['schemas']['ProjectCreate'];
      };
    };
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['ProjectCreateResponse'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  get_projects_paginated_projects_paginated_post: {
    parameters: {
      query?: {
        /** @description Actions to include in the 'permissions' field. */
        actions?: components['schemas']['ProjectAction'][];
        starting_token?: number;
        limit?: number;
      };
      header?: never;
      path?: never;
      cookie?: never;
    };
    requestBody?: {
      content: {
        'application/json': components['schemas']['ProjectCollectionParams'];
      };
    };
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['GetProjectsPaginatedResponse'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  get_projects_count_projects_count_post: {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    requestBody?: {
      content: {
        'application/json': components['schemas']['ProjectCollectionParams'];
      };
    };
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': number;
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  get_all_projects_projects_all_get: {
    parameters: {
      query?: {
        type?: components['schemas']['ProjectType'] | null;
      };
      header?: never;
      path?: never;
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['ProjectDBThin'][];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  get_project_projects__project_id__get: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        project_id: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['ProjectDB'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  update_project_projects__project_id__put: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        project_id: string;
      };
      cookie?: never;
    };
    requestBody: {
      content: {
        'application/json': components['schemas']['ProjectUpdate'];
      };
    };
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['ProjectUpdateResponse'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  delete_project_projects__project_id__delete: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        project_id: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['ProjectDeleteResponse'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  upload_file_projects__project_id__upload_file_post: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        project_id: string;
      };
      cookie?: never;
    };
    requestBody: {
      content: {
        'multipart/form-data': components['schemas']['Body_upload_file_projects__project_id__upload_file_post'];
      };
    };
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': unknown;
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  list_user_project_collaborators_projects__project_id__users_get: {
    parameters: {
      query?: {
        starting_token?: number;
        limit?: number;
      };
      header?: never;
      path: {
        project_id: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['ListUserCollaboratorsResponse'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  create_user_project_collaborators_projects__project_id__users_post: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        project_id: string;
      };
      cookie?: never;
    };
    requestBody: {
      content: {
        'application/json': components['schemas']['UserCollaboratorCreate'][];
      };
    };
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['UserCollaborator'][];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  list_group_project_collaborators_projects__project_id__groups_get: {
    parameters: {
      query?: {
        starting_token?: number;
        limit?: number;
      };
      header?: never;
      path: {
        project_id: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['ListGroupCollaboratorsResponse'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  create_group_project_collaborators_projects__project_id__groups_post: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        project_id: string;
      };
      cookie?: never;
    };
    requestBody: {
      content: {
        'application/json': components['schemas']['GroupCollaboratorCreate'][];
      };
    };
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['GroupCollaborator'][];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  delete_user_project_collaborator_projects__project_id__users__user_id__delete: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        project_id: string;
        user_id: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': unknown;
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  update_user_project_collaborator_projects__project_id__users__user_id__patch: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        project_id: string;
        user_id: string;
      };
      cookie?: never;
    };
    requestBody: {
      content: {
        'application/json': components['schemas']['CollaboratorUpdate'];
      };
    };
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['UserCollaborator'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  delete_group_project_collaborator_projects__project_id__groups__group_id__delete: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        project_id: string;
        group_id: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': unknown;
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  update_group_project_collaborator_projects__project_id__groups__group_id__patch: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        project_id: string;
        group_id: string;
      };
      cookie?: never;
    };
    requestBody: {
      content: {
        'application/json': components['schemas']['CollaboratorUpdate'];
      };
    };
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['GroupCollaborator'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  list_log_streams_projects__project_id__log_streams_get: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        project_id: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['LogStreamResponse'][];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  create_log_stream_projects__project_id__log_streams_post: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        project_id: string;
      };
      cookie?: never;
    };
    requestBody: {
      content: {
        'application/json': components['schemas']['LogStreamCreateRequest'];
      };
    };
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['LogStreamResponse'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  get_log_stream_projects__project_id__log_streams__log_stream_id__get: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        log_stream_id: string;
        project_id: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['LogStreamResponse'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  update_log_stream_projects__project_id__log_streams__log_stream_id__put: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        log_stream_id: string;
        project_id: string;
      };
      cookie?: never;
    };
    requestBody: {
      content: {
        'application/json': components['schemas']['LogStreamUpdateRequest'];
      };
    };
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['LogStreamResponse'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  delete_log_stream_projects__project_id__log_streams__log_stream_id__delete: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        log_stream_id: string;
        project_id: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful Response */
      204: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  log_traces_projects__project_id__traces_post: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        project_id: string;
      };
      cookie?: never;
    };
    requestBody: {
      content: {
        'application/json': components['schemas']['LogRecordsIngestRequest'];
      };
    };
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['LogRecordsIngestResponse'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  traces_available_columns_projects__project_id__traces_available_columns_post: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        project_id: string;
      };
      cookie?: never;
    };
    requestBody: {
      content: {
        'application/json': components['schemas']['LogRecordsAvailableColumnsRequest'];
      };
    };
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['LogRecordsAvailableColumnsResponse'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  spans_available_columns_projects__project_id__spans_available_columns_post: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        project_id: string;
      };
      cookie?: never;
    };
    requestBody: {
      content: {
        'application/json': components['schemas']['LogRecordsAvailableColumnsRequest'];
      };
    };
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['LogRecordsAvailableColumnsResponse'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  query_traces_projects__project_id__traces_search_post: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        project_id: string;
      };
      cookie?: never;
    };
    requestBody: {
      content: {
        'application/json': components['schemas']['LogRecordsQueryRequest'];
      };
    };
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['LogRecordsQueryResponse'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  query_spans_projects__project_id__spans_search_post: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        project_id: string;
      };
      cookie?: never;
    };
    requestBody: {
      content: {
        'application/json': components['schemas']['LogRecordsQueryRequest'];
      };
    };
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['LogRecordsQueryResponse'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  get_trace_projects__project_id__traces__trace_id__get: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        trace_id: string;
        project_id: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['TraceRecordWithChildren'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  get_span_projects__project_id__spans__span_id__get: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        span_id: string;
        project_id: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json':
            | components['schemas']['WorkflowSpanRecordWithChildren']
            | components['schemas']['LlmSpanRecord']
            | components['schemas']['ToolSpanRecord']
            | components['schemas']['RetrieverSpanRecord'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  list_experiments_projects__project_id__experiments_get: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        project_id: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['ExperimentResponse'][];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  create_experiment_projects__project_id__experiments_post: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        project_id: string;
      };
      cookie?: never;
    };
    requestBody: {
      content: {
        'application/json': components['schemas']['ExperimentCreateRequest'];
      };
    };
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['ExperimentResponse'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  get_experiment_projects__project_id__experiments__experiment_id__get: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        experiment_id: string;
        project_id: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['ExperimentResponse'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  update_experiment_projects__project_id__experiments__experiment_id__put: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        experiment_id: string;
        project_id: string;
      };
      cookie?: never;
    };
    requestBody: {
      content: {
        'application/json': components['schemas']['ExperimentUpdateRequest'];
      };
    };
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['ExperimentResponse'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  delete_experiment_projects__project_id__experiments__experiment_id__delete: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        experiment_id: string;
        project_id: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful Response */
      204: {
        headers: {
          [name: string]: unknown;
        };
        content?: never;
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  create_job_jobs_post: {
    parameters: {
      query?: never;
      header?: never;
      path?: never;
      cookie?: never;
    };
    requestBody: {
      content: {
        'application/json': components['schemas']['CreateJobRequest'];
      };
    };
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['CreateJobResponse'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  get_job_jobs__job_id__get: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        job_id: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['JobDB'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  get_jobs_for_project_run_projects__project_id__runs__run_id__jobs_get: {
    parameters: {
      query?: {
        status?: string | null;
      };
      header?: never;
      path: {
        project_id: string;
        run_id: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['JobDB'][];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  get_latest_job_for_project_run_projects__project_id__runs__run_id__jobs_latest_get: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        project_id: string;
        run_id: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['JobDB'] | null;
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  get_settings_projects__project_id__runs__run_id__scorer_settings_get: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        project_id: string;
        run_id: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['RunScorerSettingsRequest-Output'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  upsert_scorers_config_projects__project_id__runs__run_id__scorer_settings_post: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        project_id: string;
        run_id: string;
      };
      cookie?: never;
    };
    requestBody: {
      content: {
        'application/json': components['schemas']['RunScorerSettingsRequest-Input'];
      };
    };
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['RunScorerSettingsRequest-Output'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  get_project_templates_projects__project_id__templates_get: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        project_id: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['BasePromptTemplateResponse'][];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  create_prompt_template_with_version_projects__project_id__templates_post: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        project_id: string;
      };
      cookie?: never;
    };
    requestBody: {
      content: {
        'application/json': components['schemas']['CreatePromptTemplateWithVersionRequestBody'];
      };
    };
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['BasePromptTemplateResponse'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  get_template_version_by_name_projects__project_id__templates_versions_get: {
    parameters: {
      query: {
        template_name: string;
        version?: number | null;
      };
      header?: never;
      path: {
        project_id: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['BasePromptTemplateVersionResponse'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  get_template_from_project_projects__project_id__templates__template_id__get: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        template_id: string;
        project_id: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['BasePromptTemplateResponse'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  delete_template_projects__project_id__templates__template_id__delete: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        template_id: string;
        project_id: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['DeletePromptResponse'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  create_prompt_template_version_projects__project_id__templates__template_id__versions_post: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        project_id: string;
        template_id: string;
      };
      cookie?: never;
    };
    requestBody: {
      content: {
        'application/json': components['schemas']['BasePromptTemplateVersion'];
      };
    };
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['BasePromptTemplateVersionResponse'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  get_template_version_projects__project_id__templates__template_id__versions__version__get: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        template_id: string;
        version: number;
        project_id: string;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['BasePromptTemplateVersionResponse'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
  set_selected_template_version_projects__project_id__templates__template_id__versions__version__put: {
    parameters: {
      query?: never;
      header?: never;
      path: {
        project_id: string;
        template_id: string;
        version: number;
      };
      cookie?: never;
    };
    requestBody?: never;
    responses: {
      /** @description Successful Response */
      200: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['BasePromptTemplateResponse'];
        };
      };
      /** @description Validation Error */
      422: {
        headers: {
          [name: string]: unknown;
        };
        content: {
          'application/json': components['schemas']['HTTPValidationError'];
        };
      };
    };
  };
}
